[{"category":null,"content":"Affichage des données Maintenant que j\u0026rsquo;ai importé les données (vu dans la première partie et la deuxième partie), je souhaite pouvoir les visualiser.\nPour afficher les données dans une page web toute propre, j\u0026rsquo;ai créé une petite application web et une API REST qui exécute les requêtes AQL.\nVoici les dossiers sources :\n GTFS-arangodb\u0026ndash;import sources pour l\u0026rsquo;import GTFS-arangodb\u0026ndash;mapviewer sources front GTFS-arangodb\u0026ndash;api sources de l\u0026rsquo;API  Pour rappel, le but est d\u0026rsquo;afficher les horaires pour un point d\u0026rsquo;arrêt à une date précise.\nJ\u0026rsquo;ai choisi d\u0026rsquo;utiliser une carte afin d\u0026rsquo;afficher les arrêts sur un rayon d\u0026rsquo;un kilomètre en partant du centre. La liste des arrêts trouvés s\u0026rsquo;affiche sur le côté et il est donc possible de voir les trajets.\nPour cela j\u0026rsquo;ai utilisé MapTiler comme source pour la carte. Pour afficher les tuiles de carte vectorielle et profiter de l\u0026rsquo;accélération materielle GPU j\u0026rsquo;ai utilisé maplibre-gl-js qui est un fork open-source de mapbox-gl-js avec react-map-gl pour l\u0026rsquo;intégration avec React.\nVoici un morceau du code pour la carte :\nVoici une capture d\u0026rsquo;écran du résultat :\nPour aller plus loin Maitenant que j\u0026rsquo;ai pu découvrir ce que sont les données GTFS et importer les données pour les exploiter. Il peut être intéressant d\u0026rsquo;essayer de créer un petit calculateur d\u0026rsquo;itinéraire à partir des noeuds et des liens grâce à la théorie des graphes.\n","date":"Apr 28, 2023","href":"https://blog.talanlabs.com/import-gtfs-arangodb-part-3/","kind":"page","labs":["Lab 9"],"tags":["arangojs","maptiler"],"title":"Importer des GTFS dans ArangoDB - Partie 3"},{"category":null,"content":"Introduction Dans la première partie, j\u0026rsquo;ai expliqué comment obtenir les données gtfs. Maintenant je vais importer les fichiers GTFS dans ArangoDB.\nLa base de données NoSQL ArangoDB ArangoDB est une base de données, open-source gratuite en version community, développée par l\u0026rsquo;entreprise du même nom. Elle est développée en C++ et javascript et propose une IHM web pour l\u0026rsquo;administration de la BDD. ArangoDB est une base de données hybride, NoSQL et Graphe. Elle a son propre language AQL (Arango Query Language), ressemblant au SQL, qui permet de faire des requêtes.\nInstaller ArangoDB J\u0026rsquo;ai installé la version community d\u0026rsquo;ArangoDB localement via un fichier docker compose.\nservices: arangodb: container_name: arangodb image: arangodb:3.9.3 ports: - \u0026#34;8529:8529\u0026#34; command: - \u0026#34;arangod\u0026#34; - \u0026#34;--cache.size\u0026#34; - \u0026#34;1048576\u0026#34; - \u0026#34;--rocksdb.block-cache-size\u0026#34; - \u0026#34;1048576\u0026#34; - \u0026#34;--rocksdb.enforce-block-cache-size-limit\u0026#34; volumes: - arangodbdata:/var/lib/arangodb3 environment: - ARANGO_NO_AUTH=1 volumes: arangodbdata: Ceci n\u0026rsquo;est pas obligatoire car il le calcule automatiquement suivant la taille totale de votre mémoire RAM. J\u0026rsquo;ai ajouté un réglage sur la taille du cache. Ceci n\u0026rsquo;est pas obligatoire il le calcule automatiquement suivant la taille totale de votre mémoire RAM\nFonctionnement ArangoDB contient des bases de données qui contiennent des collections. ArangoDB à une base _SYSTEM à partir de laquelle nous pouvons ajouter des bases de données ou bien accéder au dashboard.\nIl y a différents types de collections : le type Document ou le type Edges.\nDocuments Les documents contiennent des objets JSON. Ils sont considérés comme les sommets (vertex) de la bdd graph.\n{ \u0026#34;_id\u0026#34;: \u0026#34;stops/IDFM:monomodalStopPlace:47052\u0026#34;, // l\u0026#39;id de l\u0026#39;objet formé du nom de la collection et sa clef  \u0026#34;_rev\u0026#34;: \u0026#34;_e3we666---\u0026#34;, /* la révision est générée à la création du document et est mise à jour à la modification ou au remplacement. Il est en lecture seule pour l\u0026#39;utilisateur.*/ \u0026#34;_key\u0026#34;: \u0026#34;IDFM:monomodalStopPlace:47052\u0026#34;, // la clef de l\u0026#39;objet  ... // les données } Edges Les \u0026ldquo;edges\u0026rdquo; contiennent des objets JSON qui seront des arêtes.\nLes arêtes font partie de la définition des graphes. Ils décrivent quelles collections d\u0026rsquo;arêtes (edges) connectent quelles collections de sommets (vertex).\nLes collections d\u0026rsquo;arêtes auront également un index d\u0026rsquo;arêtes créé automatiquement, qui ne peut pas être modifié. Cet index permet un accès rapide aux documents via les attributs _from et _to.\n{ \u0026#34;_id\u0026#34;:\u0026#34;part_of_stop/251428286\u0026#34;, \u0026#34;_rev\u0026#34;:\u0026#34;_e3wf-uq--C\u0026#34;, \u0026#34;_key\u0026#34;:\u0026#34;251428286\u0026#34;, \u0026#34;_from\u0026#34;:\u0026#34;stops/IDFM:monomodalStopPlace:47052\u0026#34;, // sommet de départ  \u0026#34;_to\u0026#34;:\u0026#34;stops/IDFM:62951\u0026#34; // sommet d\u0026#39;arrivé } Les index Les documents peuvent avoir des index de types différents.\nLa description des index est ici : https://www.arangodb.com/docs/stable/http/indexes.html\n Geo-spatial index : un index créé à partir des valeurs des champs contenant une latitude et longitude (les nom des champs sont à spécifier lors de la création de l\u0026rsquo;index) Primary Index Edge Index Persistent Index Inverted Index TTL (time-to-live) index Multi-dimensional index Address of an Index  Parcourir le graphe (Traversals) FOR vertex, edge, path IN min..max ANY startVertex edgeCollection1, ..., edgeCollectionN Un parcours commence à un document spécifique (startVertex) et suit toutes les arêtes connectées à ce document. Pour tous les documents (sommets) qui sont ciblés par ces bords, il suivra à nouveau tous les bords qui leurs sont connectés et ainsi de suite. Il est possible de définir combien de ces itérations suivantes doivent être exécutées au minimum et maximum.\nImport Dans le but d\u0026rsquo;importer les données du GTFS dans la base de données j\u0026rsquo;ai créé un script qui permet d\u0026rsquo;importer automatiquement dans la base.\nVoici le modèle de donnée de type graphe que je souhaite :\nSchéma du graphe :\nDémarrer l\u0026rsquo;import Voici le projet sur gihub pour l\u0026rsquo;import :\nhttps://github.com/gregoire78/GTFS-arangodb--import\nnpm run build \u0026amp;\u0026amp; \\ npm start Cela peut prendre un certain temps, car il y a beaucoup de données à relier.\nAQL Une fois l\u0026rsquo;import fait on peut ouvrir la page pour faire des requêtes dans l\u0026rsquo;interface web d\u0026rsquo;ArangoDB (http://127.0.0.1:8529/_db/GTFS/)\nSélectionnez la base de données \u0026ldquo;GTFS\u0026rdquo; en haut à droite en cliquant sur \u0026ldquo;_SYSTEM\u0026rdquo;.\nIl faut aller ensuite dans \u0026ldquo;QUERIES\u0026rdquo; à gauche.\nIl y a des tutoriels AQL très bien sur le github.\nArrangoDB permet de faire des requêtes avec des données géographiques (coordonnées gps)\n// boucle sur la collection \u0026quot;stops\u0026quot; for s in stops // récupère la distance (en mètre) entre le point que l'on souhaite et les points dans la collection let d = distance(s.lat, s.lon, 48.8497477377042, 2.4180963749631648) // filtre les points d'arrêts dans un rayon de moins d'un kilomètre filter s.locationType == 1 and d \u0026lt; 1000 // ceci retourne le résultat return {name: s.name, distance: d} Le résultat de la requête est visible sous la forme d\u0026rsquo;une table :\nScript pour afficher les noeuds qui dépendent d\u0026rsquo;un arrêt et afficher le graphe on peut exécuter le script suivant :\nfor s in stops let d = distance(s.lat, s.lon, 48.8497477377042, 2.4180963749631648) filter d \u0026lt; 1000 limit 1000 for stop,e,b IN 1 inbound s._id part_of_stop return b Maintenant il est facile de jouer avec les données pour afficher de jolies graphes.\nVisualisation des données Dans la troisième partie, le but sera de montrer la récupération de données et afficher sur une carte les horaires des points d\u0026rsquo;arrêts.\n","date":"Apr 21, 2023","href":"https://blog.talanlabs.com/import-gtfs-arangodb-part-2/","kind":"page","labs":["Lab 9"],"tags":["arangodb","bdd"],"title":"Importer des GTFS dans ArangoDB - Partie 2"},{"category":null,"content":"Préambule Mon objectif est d\u0026rsquo;afficher des arrêts de bus ou de train avec les horaires disponibles à une date. J\u0026rsquo;utiliserai les fichiers GTFS d\u0026rsquo;île de France Mobilité disponibles en \u0026ldquo;open-data\u0026rdquo; dans une bdd ArangoDB.\nGTFS ? GTFS est un format de fichier contenant des données de transports collectifs.\nGTFS est un acronyme signifiant General Transit Feed Specification (spécification générale pour les dossier relatifs aux transports en commun).\nGTFS fut développé par Google et Trimet, un organisme de transports collectifs dans l\u0026rsquo;Oregon, et conçu par Tim et Bibiana McHugh pour permettre l\u0026rsquo;import des horaires avec des fichiers CSV dans Google Maps.\nUn dossier GTFS contient plusieurs fichiers ressemblant au format CSV (les valeurs des champs sont séparées par des virgules), avec une extension .txt, à l\u0026rsquo;intérieur d\u0026rsquo;un fichier ZIP de sorte à pouvoir transférer, télécharger et importer facilement les horaires, les tarifs les lignes de transports, le mode de transport, la société, les arrêts.\nAinsi il est possible d\u0026rsquo;importer un tel dossier dans une base de données graph pour pouvoir y faire des requêtes afin de récupérer les informations sur un voyage, un véhicule, une ligne ou un point d\u0026rsquo;arrêt.\nGTFS n\u0026rsquo;est pas une norme officielle en France. Ce format a été repris par la communauté open-source et est utilisé la plupart du temps pour les données en open data. Les formats d\u0026rsquo;échanges officiels en France sont NEPTUNE et NETEX (norme Européenne) pour les données théoriques et SIRI pour les données en temps réel. Pour en savoir plus, le site de CHOUETTE contient toutes infos sur ces différents formats.\nLe format du dossier de données Un dossier GTFS est découpé en plusieurs fichiers .txt.\nParmi les fichiers texte il y a ceux-ci :\n agency.txt: Cela va être une société de transport (RATP, SNCF, OPTILE \u0026hellip;) calendar.txt calendar_dates.txt routes.txt: Ce sont les itinéraires des lignes (RER A, Metro 6, \u0026hellip;). stop_extensions.txt stops.txt: Ce sont les arrêts. stop_times.txt: les horaires pour chaque station ou arrêt. transfers.txt: temps de transfert (à pieds) entre deux stop. trips.txt: Ce sont les trajets (ELBA, ZEUS, \u0026hellip;).  Il y a plusieurs autres fichiers qui sont optionnels et peuvent ne pas être présents.\nVous pouvez avoir un aperçu de ce que peuvent contenir les différents fichiers sur la page github de Google Transit.\nVoici quelques liens GTFS de métropoles françaises :\n Ile-de-France (réseau IDF Mobilités) : https://prim.iledefrance-mobilites.fr/fr/donnees-statiques/offre-horaires-tc-gtfs-idfm  Cette archive contient des fichier assez lourds. Le fichier stop_times.txt est ici le plus volumineux. Il contient plusieurs millions de lignes.\n(~ 9 millions de lignes pour l\u0026rsquo;ensemble des horaires pour les différents modes de transport de l\u0026rsquo;organisme IDF mobilités).\n  Rennes (réseau STAR) : https://data.explore.star.fr/explore/dataset/tco-busmetro-horaires-gtfs-versions-td/table/\n  Strasbourg (reseau CTS) : https://transport.data.gouv.fr/datasets/gtfs-strasbourg/\n  Vous pouvez retrouver ces dossiers sur transport.data.gouv.\nDonnées en temps réel Un dossier GTFS fournit des données qui sont dites statiques. C\u0026rsquo;est à dire que le dossier contient les horaires théoriques ayants cours pendant une période précise. Le dossier peut être mis à jours plus ou moins fréquemment suivant les organismes et ne s\u0026rsquo;adaptent pas aux changements horaires de dernière minute.\nIl existe, malheureusement, des aléas dans les transports en commun qui bousculent les horaires théoriques. GTFS propose donc un format pour publié les changements horaires en temps réel.\nLe dossier GTFS pour le temps réel GTFS Realtime, fait référence aux données présentes dans le dossier GTFS statique et utilise le protocol buffer language.\nAttention cependant tous ceux qui utilisent les dossier GTFS statiques n\u0026rsquo;utilisent pas obligatoirement GTFS realtime pour transmettre les données en temps réel.\nComment créer et tester les données Il existe de nombreux outils pour vérifier l’intégrité ou encore visualiser les données d\u0026rsquo;une archive GTFS.\nPar exemple gtfs-validator\nou encore ScheduleViewer\nPour avoir de la documentation en français sur les différentes normes de données dans les transports en commun voir le site de CHOUETTE sur l\u0026rsquo;échange de données dans les transports collectifs : http://www.chouette.mobi/\nLa documentation Google https://developers.google.com/transit/gtfs\nComment utiliser les données ? Et bien c\u0026rsquo;est ce que vont montrer les prochaines parties de l\u0026rsquo;article.\nDans la seconde partie, le but sera d\u0026rsquo;importer un dossier GTFS statique dans une base de données graph comme ArangoDB.\nDans la troisième partie, le but sera de montrer la récupération de données et l\u0026rsquo;affichage sur une carte les horaires des points d\u0026rsquo;arrêts.\n","date":"Apr 14, 2023","href":"https://blog.talanlabs.com/import-gtfs-arangodb-part-1/","kind":"page","labs":["Lab 9"],"tags":["gtfs","transports"],"title":"Importer des GTFS dans ArangoDB - Partie 1"},{"category":null,"content":"Dans le cadre d’un projet, j’ai eu besoin de développer une API rapidement. Je me suis alors posée la question de l’outil à utiliser. En faisant quelques recherches, je suis tombée plusieurs fois sur Flask. Ayant déjà fait un peu de python, je me suis lancée dans la découverte de ce framework…​\n A la découverte de Flask La première version de Flask date de 2010. Il s’agit d’un framework de développement web en python. Ce framework open-source est qualifié de micro-framework car il est très léger. Il est particulièrement adapté dans le cas de petites applications ou de PoC (Proof of Concept), ce qui correspond tout à fait à notre cas d’usage.\n Flask est utilisé par de nombreux développeurs. On peut retrouver son utilisation dans des sites tels que LinkedIn ou encore Pinterest.\n Pourquoi utiliser Flask ? Flask permet une prise en main simple et rapide, tout en permettant d’évoluer vers des applications plus complexes. En 5 lignes seulement, il est possible de créer une application web simplifiée.\n =\u0026gt;   Pré-requis : installation de python Commencez par vérifier que python est bien installé sur votre ordinateur.\n $ python3 --version   Si ce n’est pas le cas, installez-le :\n $ sudo apt install python3   Pour installer python sur d’autres OS que Linux : https://www.python.org/downloads/\n    Initier un nouveau projet Création du dossier Créons un dossier dans lequel mettre notre nouveau projet. Appelons le bookshop-api :\n $ mkdir bookshop-api    Versioning du code Afin de versionner notre api, nous allons initialiser un nouveau dépôt git :\n $ cd bookshop-api $ git init   N’oublions pas de créer un fichier .gitignore qui comportera les fichiers/dossiers qu’on ne veut pas versionner ni partager. Nous le remplirons par la suite.\n $ touch .gitignore      Mettre en place un environnement virtuel Il est recommandé de créer un environnement virtuel pour chaque projet sur lequel on travaille, dans lequel on installera localement les packages dont nous avons besoin. En effet, cela permet de travailler sur différents projets sans se soucier de la compatibilité entre les versions des librairies des différents projets.\n Pour créer un nouvel environnement virtuel, lancer à la racine du projet :\n $ cd bookshop-api $ python3 -m venv \u0026lt;name_of_virtual_env\u0026gt;   Appelons venv notre environnement virtuel. Maintenant que nous l’avons créé, il faut l’activer :\n $ source venv/bin/activate   L’environnement virtuel ne doit pas être versionné/partagé. Pour cela nous ajoutons la ligne suivante dans le fichier .gitigore :\n /venv/   Installation de Flask Entrons maintenant dans le vif du sujet : l’installation de Flask.\n Pour installer Flask dans l’environnement virtuel :\n $ pip install flask $ python -c \u0026#34;import flask; print(flask.__version__)\u0026#34; $ pip install -U flask-cors $ pip install python-dotenv   L’installation du package flask-cors permet la gestion du CORS. Pour en savoir plus sur ce package, rendez-vous ici. Pour en savoir plus sur le CORS, ou \u0026#34;Cross-origin resource sharing\u0026#34;, rendez-vous ici.\n Nous verrons plus bas l’utilisation du package python-dotenv.\n    Création d’un premier endpoint Créer un fichier main.py à la racine du projet, qui sera le point de départ de notre API.\n from flask import Flask app = Flask(__name__) @app.route(\u0026#39;/\u0026#39;) def hello(): return \u0026#39;Hello World!\u0026#39;   Comme vous pouvez le constater, il ne suffit vraiment que de 5 lignes pour créer un premier endpoint.\n   Lancement de l’application Variables d’environnement Définissons le nom de l’application ainsi que l’environnement de développement. La variable FLASK_APP permet d’indiquer à Flask où trouver l’application. Ici, le nom de l’application est main car le fichier à lancer est main.py. La variable FLASK_ENV indique quant à elle à Flask dans quel mode exécuter l’application. Ici, nous voulons l’exécuter en mode developpement, afin d’activer automatiquement le mode de debug. La valeur par défaut étant FLASK_ENV=production.\n Nous allons enregistrer ces variables d’environnement dans un fichier .env situé à la racine du projet :\n FLASK_APP=main FLASK_ENV=development   N’oubliez pas d’ajouter ce fichier .env dans votre .gitignore. Les variables d’environnement ne sont pas supposées être partagées, surtout dans le cas où il s’agit de variables secrètes.\n Le package python-dotenv est nécessaire pour que ce fichier contenant les variables d’environnement soit automatiquement lu au lancement de l’application. C’est pourquoi nous l’avons installé lors de la mise en place de l’application.\n    Lancement de l’API Pour démarrer le serveur Flask, lancer la commande suivante :\n $ flask run   La console affiche alors les lignes suivantes :\n * Serving Flask app \u0026#34;main\u0026#34; (lazy loading) * Environment: development * Debug mode: on * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit) * Restarting with stat * Debugger is active! * Debugger PIN: 813-894-335   En se rendant à l’adresse http://127.0.0.1:5000/, la phrase \u0026#34;Hello World\u0026#34; apparaît sur la page du navigateur.\n   Ajout de nouvelles routes Ce n’est pas tout d’afficher un \u0026#34;Hello World\u0026#34;, passons maintenant aux choses sérieuses : la création des routes de notre librairie en ligne.\n CRUD(S) endpoints Dans le cadre de ce projet, j’avais besoin de répondre aux besoins suivants :\n   Récupérer la liste de tous les livres ;\n  Récupérer un livre par son id ;\n  Créer un livre ;\n  Modifier un livre ;\n  Supprimer un livre.\n   Nous allons donc créer 5 endpoints correspondant aux 5 opérations CREATE, READ, UPDATE, DELETE, et SEARCH dans le fichier main.py:\n from flask import Flask, request, jsonify from flask_cors import CORS from utils.seeds import default_books from domain.Book import Book, from_json app = Flask(__name__) CORS(app, supports_credentials=True) @app.route(\u0026#39;/\u0026#39;) def hello(): return \u0026#39;Hello World!\u0026#39; @app.route(\u0026#39;/books\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_all_books(): return jsonify([book.to_json() for book in default_books]), 200 @app.route(\u0026#39;/books/\u0026lt;book_id\u0026gt;\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def get_one_book(book_id): for book in default_books: if str(book.id) == str(book_id): return book.to_json(), 200 return jsonify({\u0026#39;error\u0026#39;: \u0026#39;book not found\u0026#39;}), 404 @app.route(\u0026#39;/books\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def create_book(): data = request.get_json() book = from_json(data) book.id = len(default_books) + 1 default_books.append(book) return jsonify({\u0026#39;message\u0026#39;: \u0026#39;book successfully created\u0026#39;}), 200 @app.route(\u0026#39;/books/\u0026lt;book_id\u0026gt;\u0026#39;, methods=[\u0026#39;PUT\u0026#39;]) def update_book(book_id): data = request.get_json() updated_book = from_json(data) for book in default_books: if str(book.id) == str(book_id): index = default_books.index(book) default_books[index] = updated_book return jsonify({\u0026#39;message\u0026#39;: \u0026#34;book successfully updated\u0026#34;}), 200 return jsonify({\u0026#39;error\u0026#39;: \u0026#39;book not found\u0026#39;}), 404 @app.route(\u0026#39;/books/\u0026lt;book_id\u0026gt;\u0026#39;, methods=[\u0026#39;DELETE\u0026#39;]) def delete_book(book_id): for book in default_books: if str(book.id) == str(book_id): default_books.remove(book) return jsonify({\u0026#39;message\u0026#39;: \u0026#39;book successfully deleted\u0026#39;}), 200 return jsonify({\u0026#39;error\u0026#39;: \u0026#39;book not found\u0026#39;}), 404   La classe Book se trouve dans le fichier Book.py :\n import string class Book: def __init__(self, title: string, code_name: string, author: string, quantity: int, publish_date: string, id: int = 0) -\u0026gt; None: self.id = id self.title = title self.code_name = code_name self.author = author self.quantity = quantity self.publish_date = publish_date def to_json(self): return { \u0026#39;id\u0026#39;: self.id, \u0026#39;title\u0026#39;: self.title, \u0026#39;codeName\u0026#39;: self.code_name, \u0026#39;author\u0026#39;: self.author, \u0026#39;publishDate\u0026#39;: self.publish_date, \u0026#39;quantity\u0026#39;: self.quantity } def from_json(data): return Book( data[\u0026#39;title\u0026#39;], data[\u0026#39;codeName\u0026#39;], data[\u0026#39;author\u0026#39;], data[\u0026#39;quantity\u0026#39;], data[\u0026#39;publishDate\u0026#39;], data[\u0026#39;id\u0026#39;] )   La liste de livres default_books se trouve dans le fichier seeds.py\n from domain.Book import Book b1 = Book(\u0026#34;Juste un regard\u0026#34;, \u0026#34;juste_un_regard\u0026#34;, \u0026#34;Harlan Coben\u0026#34;, 8, 2010, 1) b2 = Book(\u0026#34;Ne le dis à personne\u0026#34;, \u0026#34;ne_le_dis_a_personne\u0026#34;, \u0026#34;Harlan Coben\u0026#34;, 1, 2005, 2) b3 = Book(\u0026#34;Dans les bois\u0026#34;, \u0026#34;dans_les_bois\u0026#34;, \u0026#34;Harlan Coben\u0026#34;, 5, 2020, 3) b4 = Book(\u0026#34;Balle de match\u0026#34;, \u0026#34;balle_de_match\u0026#34;, \u0026#34;Harlan Coben\u0026#34;, 3, 2007, 4) b5 = Book(\u0026#34;Disparu à jamais\u0026#34;, \u0026#34;disparu_a_jamais\u0026#34;, \u0026#34;Harlan Coben\u0026#34;, 3, 2003, 5) b6 = Book(\u0026#34;Promets moi\u0026#34;, \u0026#34;promets_moi\u0026#34;, \u0026#34;Harlan Coben\u0026#34;, 7, 1999, 6) default_books: [Book] = [b1, b2, b3, b4, b5, b6]    Appels à l’API Méthode GET - Récupérer tous les livres Requête curl :\n curl http://127.0.0.1:5000/books   Résultat :\n [ { \u0026#34;author\u0026#34;: \u0026#34;Harlan Coben\u0026#34;, \u0026#34;codeName\u0026#34;: \u0026#34;innocent\u0026#34;, \u0026#34;id\u0026#34;: 1, \u0026#34;publishDate\u0026#34;: 2005, \u0026#34;quantity\u0026#34;: 10, \u0026#34;title\u0026#34;: \u0026#34;Innocent\u0026#34; }, { \u0026#34;author\u0026#34;: \u0026#34;Harlan Coben\u0026#34;, \u0026#34;codeName\u0026#34;: \u0026#34;ne_le_dis_a_personne\u0026#34;, \u0026#34;id\u0026#34;: 2, \u0026#34;publishDate\u0026#34;: 2001, \u0026#34;quantity\u0026#34;: 7, \u0026#34;title\u0026#34;: \u0026#34;Ne le dis à personne\u0026#34; }, { \u0026#34;author\u0026#34;: \u0026#34;Harlan Coben\u0026#34;, \u0026#34;codeName\u0026#34;: \u0026#34;dans_les_bois\u0026#34;, \u0026#34;id\u0026#34;: 3, \u0026#34;publishDate\u0026#34;: 2007, \u0026#34;quantity\u0026#34;: 12, \u0026#34;title\u0026#34;: \u0026#34;Dans les bois\u0026#34; }, { \u0026#34;author\u0026#34;: \u0026#34;Harlan Coben\u0026#34;, \u0026#34;codeName\u0026#34;: \u0026#34;balle_de_match\u0026#34;, \u0026#34;id\u0026#34;: 4, \u0026#34;publishDate\u0026#34;: 2006, \u0026#34;quantity\u0026#34;: 3, \u0026#34;title\u0026#34;: \u0026#34;Balle de match\u0026#34; } ]    Méthode GET - Récupérer un livre Requête curl :\n curl http://127.0.0.1:5000/books/1   Résultat :\n { \u0026#34;author\u0026#34;: \u0026#34;Harlan Coben\u0026#34;, \u0026#34;codeName\u0026#34;: \u0026#34;innocent\u0026#34;, \u0026#34;id\u0026#34;: 1, \u0026#34;publishDate\u0026#34;: 2005, \u0026#34;quantity\u0026#34;: 10, \u0026#34;title\u0026#34;: \u0026#34;Innocent\u0026#34; }    Méthode POST - Créer un livre Requête curl :\n curl -X POST http://127.0.0.1:5000/books -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;author\u0026#34;: \u0026#34;Harlan Coben\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Juste un regard\u0026#34;, \u0026#34;publishDate\u0026#34;: 2004, \\ \u0026#34;quantity\u0026#34;: 12, \u0026#34;codeName\u0026#34;: \u0026#34;juste_un_regard\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;\u0026#34;}\u0026#39;   Résultat :\n { \u0026#34;message\u0026#34;: \u0026#34;book successfully created\u0026#34; }    Méthode PUT - Editer un livre Requête curl :\n curl -X PUT http://127.0.0.1:5000/books/5 -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;author\u0026#34;: \u0026#34;Harlan Coben\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Juste un regard\u0026#34;, \u0026#34;publishDate\u0026#34;: 2004, \u0026#34;quantity\u0026#34;: 20, \\ \u0026#34;codeName\u0026#34;: \u0026#34;juste_un_regard\u0026#34;, \u0026#34;id\u0026#34;: 5}\u0026#39;   Résultat :\n { \u0026#34;message\u0026#34;: \u0026#34;book successfully updated\u0026#34; }    Méthode DELETE - Supprimer un livre Requête curl :\n curl -X DELETE http://127.0.0.1:5000/books/46   Résultat :\n { \u0026#34;message\u0026#34;: \u0026#34;book successfully deleted\u0026#34; }    Livre non trouvé Requête curl :\n curl http://127.0.0.1:5000/books/157   Résultat :\n { \u0026#34;error\u0026#34;: \u0026#34;book not found\u0026#34; }       Conclusion Le choix de Flask était adapté pour mon cas d’usage. En effet, la prise en main a été très facile, et une API simplifiée a pu être créée en quelques lignes.\n Python est un langage très utilisé, et cela vaut le coup de s’y mettre pour utiliser Flask. En effet, la montée en compétence est très rapide.\n Flask étant un micro-framework, il faut tout développer soi-même tandis qu’avec un framework plus complet tel que Django, le développement de certaines fonctionnalité peut être facilité. De plus, la communauté de Flask n’est pas aussi grande que celle de Django.\n Cependant, il faut garder à l’esprit que l’utilisation de Flask est surtout recommandée pour de petites applications, pour lesquelles Flask est le choix idéal.\n Pour aller plus loin :\n   Flask permet de renvoyer des templates HTML directement. Ayant déjà un front et n’ayant que besoin d’une API, ce sujet n’a pas été abordé ici mais vous pourrez trouver de la documentation à ce sujet sur le site DigitalOcean.\n  La connexion avec une base de données n’a pas été traitée dans cet article, mais elle peut également se faire aisément avec Flask.\n     ","date":"Mar 13, 2023","href":"https://blog.talanlabs.com/2023-03-13-quickstart-flask-api/","kind":"page","labs":null,"tags":["api","python","flask"],"title":"Créer une API rapidement avec Flask"},{"category":null,"content":"Comme vous l\u0026rsquo;avez peut-être constaté, il existe plusieurs façons de manipuler des icônes dans une application WEB. Chaque façon de procéder ayant son lot d\u0026rsquo;avantages et d\u0026rsquo;inconvénients. Le but de cet article est de parler d\u0026rsquo;une partie d\u0026rsquo;entre elles, et ce par rapport à la mise en place d\u0026rsquo;un Design System et les besoins qui en découlent.\nQuels sont alors les enjeux de mettre à disposition des icônes dans un Design System ?\nTout d\u0026rsquo;abord, cela va permettre d\u0026rsquo;avoir un lot commun d\u0026rsquo;icônes pouvant être utilisées par toutes les applications utilisant ce Design System, renforçant ainsi son objectif d\u0026rsquo;homogénéité des outils développés. De plus, le travail des développeurs en sera facilité par cette centralisation des icônes. Enfin, la solution retenue pour leur utilisation doit prendre en compte plusieurs critères dont notamment le fait de pouvoir leur appliquer des couleurs spécifiques.\nVoyons maintenant différentes manières de manipuler des icônes.\nUtiliser des \u0026ldquo;inline\u0026rdquo; SVG \u0026ldquo;SVG (pour Scalable Vector Graphics en anglais, soit « graphiques vectoriels adaptables ») est un langage construit à partir de XML et qui permet de décrire des graphiques vectoriels en deux dimensions.\u0026rdquo; Documentation MDN sur les SVG.\nLes \u0026ldquo;inline\u0026rdquo; SVG vont permettre d\u0026rsquo;accéder au code des icônes sous forme de balises que l\u0026rsquo;on pourra insérer directement dans du HTML. Voici un exemple de code présent dans un SVG :\n\u0026lt;svg class=\u0026#34;svg-icon\u0026#34; viewBox=\u0026#34;0 0 20 20\u0026#34;\u0026gt; \u0026lt;path d=\u0026#34;M17.684,7.925l-5.131-0.67L10.329,...\u0026#34;\u0026gt;\u0026lt;/path\u0026gt; \u0026lt;/svg\u0026gt; Ce qui donnerait :\nAyant accès à ce code, nous pouvons modifier de manière très simple la couleur de l\u0026rsquo;icône. Par exemple en ajoutant un attribut fill sur la balise \u0026lt;path\u0026gt;\u0026lt;/path\u0026gt; :\n\u0026lt;svg class=\u0026#34;svg-icon\u0026#34; viewBox=\u0026#34;0 0 20 20\u0026#34;\u0026gt; \u0026lt;path fill=\u0026#34;red\u0026#34; d=\u0026#34;M17.684,7.925l-5.131-0.67L10.329,...\u0026#34;\u0026gt;\u0026lt;/path\u0026gt; \u0026lt;/svg\u0026gt; Voici le résultat de cette modification :\nQuant à la question de la qualité de l\u0026rsquo;icône suivant son agrandissement, les SVG étant créés de façon vectorielle, aucune perte de qualité n\u0026rsquo;aura lieu.\nDernier point important concernant les SVG \u0026ldquo;inline\u0026rdquo; : étant attachés au DOM en tant qu\u0026rsquo;éléments HTML, leur mise en cache dans le navigateur n\u0026rsquo;est pas possible. En tout cas ni nativement, ni facilement. Donc lorsque les icônes sont mises à disposition via un CDN (Content Delivery Network, autrement dit disponibles en ligne), elles doivent être téléchargées pour être utilisées. Ce téléchargement aura lieu pour chaque icône à chaque fois qu\u0026rsquo;elle sera affichée dans la page. Ce qui peut être problématique en terme de performance.\nUtiliser des balises \u0026lt;img /\u0026gt; L\u0026rsquo;un des moyens les plus communs d\u0026rsquo;afficher une icône dans une application est d\u0026rsquo;utiliser la balise \u0026lt;img /\u0026gt;, autrement dit de la considérer comme une image.\nCette façon de faire a l\u0026rsquo;avantage de pouvoir mettre l\u0026rsquo;icône en cache dans le navigateur, car elle sera considérée comme une ressource externe téléchargée via une URL.\nCependant, une limite se dévoile assez rapidement : l\u0026rsquo;impossibilité d\u0026rsquo;interagir avec l\u0026rsquo;icône afin de modifier, entre autres, sa couleur. Dans le cadre d\u0026rsquo;un Design System, cette limite devient une contrainte trop forte et nous oblige à trouver une autre solution.\nCréer une font d\u0026rsquo;icônes Une font d\u0026rsquo;icônes, ou police d\u0026rsquo;icônes, va nous permettre d\u0026rsquo;utiliser des icônes comme du texte dans du HTML. Cela implique que nous pouvons interagir avec l\u0026rsquo;icône, que ce soit pour lui appliquer une couleur, à l\u0026rsquo;aide de la propriété CSS color, ou pour lui modifier sa taille, à l\u0026rsquo;aide de la propriété CSS font-size (qui d\u0026rsquo;ailleurs n\u0026rsquo;inclura pas de perte de qualité comme dans le cas d\u0026rsquo;un simple texte).\nDe plus, tout comme les navigateurs le font pour télécharger des polices d\u0026rsquo;écriture (Open Sans par exemple), la font d\u0026rsquo;icônes sera mise en cache dans le navigateur, et ainsi n\u0026rsquo;importe quelle icône pourra être utilisée dans l\u0026rsquo;application, sans avoir à retélécharger quoi que ce soit.\nMais comment créer une font d\u0026rsquo;icônes ?\nPlusieurs librairies existent. Prenons le cas des librairies svgo et fantasticon.\nsvgo est une librairie permettant d\u0026rsquo;optimiser les fichiers SVG en supprimant le contenu superflu qu\u0026rsquo;ils peuvent contenir.\nfantasticon quant à elle est une librairie permettant de générer une font d\u0026rsquo;icônes à partir de fichiers SVG. On pourra trouver en sortie de ce traitement divers fichiers, notamment un index.css qui contiendra la déclaration de la nouvelle font à l\u0026rsquo;aide d\u0026rsquo;une font-face, ainsi que l\u0026rsquo;ensemble des classes CSS à appliquer sur une balise spécifique (communément la balise \u0026lt;i\u0026gt;\u0026lt;/i\u0026gt;) pour afficher telle ou telle icône. Voici un exemple de ce que l\u0026rsquo;on obtient dans ce fichier CSS :\n@font-face { font-family: \u0026#34;icons\u0026#34;; src: url(\u0026#34;path/icons.ttf?56trf6...\u0026#34;) format(\u0026#34;truetype\u0026#34;), url(\u0026#34;path/icons.woff?56trf6...\u0026#34;) format(\u0026#34;woff\u0026#34;), url(\u0026#34;path/icons.woff2?56trf6...\u0026#34;) format(\u0026#34;woff2\u0026#34;); } i[class^=\u0026#34;icon-\u0026#34;]:before, i[class*=\u0026#34; icon-\u0026#34;]:before { font-family: icons !important; font-style: normal; font-weight: normal !important; font-variant: normal; text-transform: none; line-height: 1; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } .icon-ic_my-great-icon:before { content: \u0026#34;\\f101\u0026#34;; } .icon-ic_another-icon:before { content: \u0026#34;\\f102\u0026#34;; } Nous avons donc en premier lieu la définition de la font-face contenant la nouvelle font-family \u0026ldquo;icons\u0026rdquo; ainsi que les URL vers les fichiers de font associés (.ttf, .woff, \u0026hellip;).\nVient ensuite la déclaration des classes CSS i[class^=\u0026quot;icon-\u0026quot;]:before et i[class*=\u0026quot; icon-\u0026quot;]:before afin d\u0026rsquo;appliquer des styles globaux sur les balises \u0026lt;i\u0026gt;\u0026lt;/i\u0026gt; ayant une classe CSS préfixée par icon-, notamment en appliquant la font-family \u0026ldquo;icons\u0026rdquo;.\nEnfin vient la déclaration de la classe CSS correspondant à chaque icône. On peut voir que la propriété CSS content est utilisée dans ces classes et qu\u0026rsquo;elle pointe vers un id correspondant en réalité à telle ou telle icône. Cette propriété permet donc de remplacer un élément HTML par un contenu spécifique, ici une icône. Elle est appliquée dans notre cas sur le pseudo élément ::before de la balise \u0026lt;i\u0026gt;\u0026lt;/i\u0026gt;.\nVoici le code HTML pour afficher l\u0026rsquo;icône ic_my-great-icon dans notre application :\n\u0026lt;i class=\u0026#34;icon icon-ic_my-great-icon\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; Conclusion Utiliser des SVG inline est une bonne solution pour afficher des icônes dans une application. Cependant, lorsque les icônes sont mises à disposition via un CDN, elles doivent être téléchargées. L\u0026rsquo;impossibilité de mettre ces icônes en cache dans le navigateur peut devenir un réel problème, pouvant causer des soucis de performance notamment.\nLa font d\u0026rsquo;icônes, comme les SVG inline, nous laisse la possibilité d\u0026rsquo;interagir avec ces icônes, que ce soit avec leur couleur ou leur taille principalement. Comme on le ferait simplement pour du texte dans notre HTML. De plus, cette solution répond à la problématique de la mise en cache dans le navigateur, du fait que la font sera téléchargée une fois par le navigateur et sera ensuite mise en cache, comme n\u0026rsquo;importe quelle autre font.\nPour ce qui est de l\u0026rsquo;accessibilité, utiliser des attributs ARIA (tels que aria-label) sur nos icônes nous permet de les rendre accessibles à tous, comme le ferait un attribut alt sur une balise \u0026lt;img/\u0026gt;.\n","date":"Feb 23, 2023","href":"https://blog.talanlabs.com/2022-12-15-icons-in-a-design-system/","kind":"page","labs":null,"tags":["design system","icons","library"],"title":"Générer l'iconographie de son Design System"},{"category":null,"content":"Le Data Thinking au coeur de l\u0026rsquo;innovation Les entreprises sont de plus en plus concurrentielles, de plus en plus agiles et réactives au marché. C’est pourquoi le Data Thinking permet à toute entreprise de pouvoir identifier rapidement des opportunités. Tout d\u0026rsquo;abord, qu\u0026rsquo;est ce que le Design Thinking [1] ? Le Design Thinking est une méthode apparue en 1980 à Stanford. La stratégie est de se focaliser sur l’humain, analyser son ressenti pour proposer des produits innovants et répondant à une vraie problématique utilisateur. Mais quelle est la différence entre le Design Thinking et le Data Thinking ? Le Design Thinking est une méthode mettant en avant la connaissance parfaite des consommateurs pour découvrir des problématiques utilisateurs. Le Data Thinking place la donnée au centre des réflexions permettant, dans un monde de plus en plus connecté, d’améliorer des produits existants et d’innover en utilisant les données pour proposer de nouveaux services/produits. Effectivement, l’innovation est le nerf de la guerre mais la Data est le fer de lance.\nLexique :  IA – Intelligence Artificielle ML - Machine Learning BI - Business Intelligence POC - Proof of Concept RGPD - Règlement Général de la Protection des données IoT - Internet of Things  Le Data Thinking: une variante au Design Thinking Tout comme le Design Thinking, le Data Thinking est itératif, ce qui permet de mieux s’adapter au besoin (cf. l\u0026rsquo;image ci-dessous) : Lors des ateliers, les données sont placées au centre des réflexions afin de mieux comprendre le parcours, le ressenti et les préférences des utilisateurs. Mais aussi, les données permettent d’analyser la faisabilité de nos solutions : « Avons-nous les données permettant de créer ce modèle ? », « Sommes-nous capables de nettoyer et utiliser ces données ? », « Pouvons-nous collecter des données supplémentaires ? » \u0026hellip;\nL\u0026rsquo;essor de la donnée crée de nouvelles façons de travailler Cover Photo by Carlos Muza on Unsplash\nPour aller plus loin sur l’impact de la donnée dans nos sociétés et entreprises, n’hésitez pas à lire cet article : 15 faits impressionnants sur le Big Data\n** 90% des données** dans le monde ont été créées les deux dernières années [2]. De plus, les entreprises utilisant à bon escient les données à leurs dispositions engendrent de plus gros bénéfices que les entreprises dites « traditionnelles » (n’utilisant pas le plein potentiel de leurs données). La donnée via les modèles de ML ou de Deeplearning permettent de prédire, piloter et globalement créer de nouveaux produits/services. Nous pouvons prendre l’exemple avec l’entreprise ByteDance [3]. Cette société plus connue sous le nom de TikTok est la licorne mondiale la plus valorisée (140 milliards de dollars) et elle est sur le créneau de l’Intelligence Artificielle avec notamment des algorithmes puissants permettant de recommander au mieux les vidéos aux utilisateurs.\nComment réussir un atelier de Data Thinking ? Photo by Jo Szczepanska on Unsplash\nRéunir les bons participants Lors d\u0026rsquo;un atelier de Data Thinking, il est primordial d\u0026rsquo;avoir 3 catégories d\u0026rsquo;acteurs:\n Des experts métiers : Leurs rôles sont de pouvoir identifier les besoins clients et les problématiques. Ils ont la connaissance du marché de façon très précise (concurrents principaux, répartition géographique…) ; Des experts techniques : Leur objectif est de pouvoir orienter les décisions sur les données nécessaires à l’élaboration des propositions de services. Ils jouent un rôle de conseil sur la faisabilité des solutions et sur la proposition des modèles à mettre en place. Ils sont capables d’identifier les données manquantes et la charge de pré-traitement nécessaire ; Des consultants : Ils sont présents afin d’animer et de s’assurer du bon déroulement de l’atelier. Les consultants ont idéalement des connaissances métier et technique afin d\u0026rsquo;être en mesure de challenger les participants, les besoins et les solutions. Les consultants ont comme charge la rédaction des comptes rendus et des rapports finaux sur les décisions prises.  Les étapes du Data Thinking : Source: Ressource Talan Labs\n  L’étape de « PREPARATION » a pour objectif de mettre à niveau chacun des participants sur les grands principes et concepts de l’IA et de la Data. En effet, les participants doivent pouvoir apporter chacun leurs idées en ayant des notions de bases sur les impacts de la donnée dans les organisations, les notions réglementaires comme la RGPD [4] [5]. Les participants doivent également avoir un objectif commun afin que chacun aille dans la même direction.\n  La partie « IDEATION » est l’étape où les participants vont devoir référencer les cas d’usages répondant aux besoins métiers tout en analysant la faisabilité technique des solutions proposées. Ces 2 vecteurs permettent de prioriser les cas d’usages les plus pertinents.\n  Enfin, il est primordial que les participants soient d’accord sur la roadmap afin de déclencher des actions sur les prochaines étapes (analyse budgétaire, lancement de POCs, création d’équipes, communications internes/externes, etc).\n  Les piliers du Data Thinking : Le Data Thinking repose sur 5 piliers distincts :\n Tout d’abord, la méthode a pour objectif de trouver des produits/services innovants. Nous partons des données pour imaginer l’ensemble des possibilités qu’elles nous offrent. Le second pilier correspond à la capacité à répondre à un besoin client. Effectivement, une fois l’exploration des données effectuée, nous pouvons proposer des cas d’usages validés aux équipes métiers qui ont la connaissance de leurs clients afin d’identifier la pertinence des propositions. La faisabilité technique est primordiale pour pouvoir imaginer que le fruit de vos ateliers d’idéation puisse un jour être déployé en production et être présent sur le marché. La faisabilité technique regroupe : la qualité de vos données, l’accès des données, les modèles pertinents… L’importance des intervenants permet d’avoir un atelier efficace mettant l’innovation, la faisabilité et la pertinence de vos cas d’usages en avant. Sans cela, la vision de votre marché et du potentiel de vos données risque d’être limitée. Enfin, l’objectif d’un atelier de cette typologie est de pouvoir passer rapidement de l’idéation au POC afin de pouvoir si besoin itérer avec les acteurs sur les points bloquants ou les alertes perçues.  Le Data Thinking comme choix à l\u0026rsquo;innovation par les entreprises : Le Data Thinking est une méthodologie reprenant les codes du Design Thinking. En effet, les deux méthodologies permettent d’innover et accélérer de manière plus efficace le processus d’idéation et de test. Beaucoup d’entreprises ont fait le choix de miser sur ces méthodes : Apple, Vodafone, Coca-Cola, General Electric… Ces choix sont de plus en plus communs parce que les entreprises sont très concurrentielles et axent leurs différenciations sur l’innovation. Le Data Thinking peut être utilisé sur une multitude de sujet : améliorer l’expérience utilisateur avec les recommandations, créer de nouveaux produits avec la reconnaissance d’image, améliorer votre connaissance client avec l’analyse textuelle par exemple.\nRessources supplémentaires :   Innovations :\n[1] - What is design thinking ?\n[2] - Les 15 faits impressionnants sur le Big Data\n[3] - Les 728 licornes mondiales en une infographie\n  Règlementaires :\n[4] - Comprendre le RGPD\n[5] - Mode d\u0026rsquo;emploi du RGPD\n  ","date":"Feb 22, 2023","href":"https://blog.talanlabs.com/data-thinking/","kind":"page","labs":null,"tags":["DataThinking","Data","Innovation"],"title":"Le Data Thinking au cœur de l'innovation"},{"category":null,"content":"disclaimer : The service described below is in public preview at the time of writing this blog!\nIntroduction As organisations quickly move towards the cloud approach to reap the benefits, it is quite important to keep the services protected from security threats leveraging the state-of-the-art technologies. Microsoft introduced a native security feature named Microsoft Defender for Cloud, which can be used as the Cloud Security Posture Management (CSPM) and Cloud Workload Protection Platform (CWPP) for Azure, on-premises, and multicloud resources like AWS and GCP.\nThe Microsoft Defender for Cloud offers multiple comprehensive defense services for the compute, data, and service layers of the environment:\n Microsoft Defender for Servers Microsoft Defender for Storage Microsoft Defender for SQL Microsoft Defender for Containers Microsoft Defender for App Service Microsoft Defender for Key Vault Microsoft Defender for Resource Manager Microsoft Defender for DNS Microsoft Defender for open-source relational databases Microsoft Defender for Azure Cosmos DB Defender Cloud Security Posture Management (CSPM) Defender for DevOps  This article talks about one of the Defender services - the Defender for Devops, which was recently announced by Microsoft. Let\u0026rsquo;s explore it!\nWhat is Microsoft Defender for Devops? Microsoft Defender for Devops bridge SecOps and DevOps with automated discovery across pipelines, starting with GitHub and Azure Devops, with more to come. This service is currently in public preview.\nThe principal capabilities of this service are:\n Unified visibility into DevOps security posture - It provides the unified visibility and control, with that security teams can help developers prioritize things that matter, and empower the developers to be in the driver\u0026rsquo;s seat. Strengthen cloud resource configurations throughout the development lifecycle. Prioritize remediation of critical issues in code by connecting application code insights with the runtime context.  It also lists out :\n Repositories from GitHub and Azure Devops. Pull request annotation status for the repositories. Exposed Secrets in repositories. Open-source dependency vulnerabilities. Code vulnerabilities and misconfigurations.  It uses Microsoft Security Devops(MSDO), which is a command line application that integrates static analysis tools into the development cycle. MSDO installs, configures and runs the latest versions of static analysis tools and it is data-driven with portable configurations that enable deterministic execution across multiple environments.\nBelow is the list of the open-source tools used by MSDO.\n   Name Language License     Bandit Python Apache License 2.0   BinSkim Binary\u0026ndash;Windows, ELF MIT License   ESlint JavaScript MIT License   Credscan Credential Scanner (CredScan) is a tool developed and  maintained by Microsoft to identify credential leaks such as those in source code and configuration files.  Common types: default passwords, SQL connection strings, Certificates with private keys Not Open Source   Template Analyzer ARM template, Bicep file MIT License   Terrascan Terraform (HCL2), Kubernetes (JSON/YAML), Helm v3, Kustomize,Dockerfiles, Cloud Formation Apache License 2.0   Trivy Container images, file systems, git repositories Apache License 2.0    Let\u0026rsquo;s configure it! Let\u0026rsquo;s take a look at how to configure Microsoft Devops defender for Azure Devops!\n  Login to your Azure portal and navigate to Microsoft Defender for Cloud.\n  Navigate to Environment Settings and select Azure Devops in Add Environment.\n  Fill in the connection name, Subscription and Resource group.\n  Select the Plan. Since this service is in public preview now, it is available as free of cost.\n  You have to authorize the connection to grant access to your Devops resources. Ensure you are selecting the correct Azure Devops profile while providing authorisation.\n  Once authorised, you can choose auto discovery or selected projects/repositories to apply the Defender for Devops service.\n  Install Extension There is an extension for Azure DevOps named Microsoft Security DevOps Azure DevOps Extension which contributes a build task to run the Microsoft Security DevOps CLI.\nLet\u0026rsquo;s see how to do this!\n  Log onto Azure Devops and select Manage Extensions from the Shopping Bag.\n  Search for the Microsoft Security Devops extension in Marketplace.\n  Select your Devops Organisation and install it.\n  Modify your pipeline   Add Microsoft Security Devops task in your yaml pipeline.\n  The inputs allowed for the MSDO task are given below :\nDisplay the results If you would like to get the analysis results displayed automatically under the Scans tab, you have to install the extension SARIF SAST Scans Tab on your Azure DevOps organization.\nLet\u0026rsquo;s look at a sample pipeline output which displays the Terrascan results under Scans tab. You can see the misconfigurations identified in the terraform Iac (Infrastructure as Code).\nNow let’s look at a sample build pipeline output which displays the Credscan results under Scans tab. It identified the passwords that I have hardcoded in the source code. That\u0026rsquo;s awesome!\nIf you click on the results, you will see more details on the error.\nConclusion Microsoft Defender for DevOps unifies DevOps Security Management across multi-pipeline and multi-cloud environments. This feature helps to identify all vulnerabilities in your Azure Devops and Github repositories in a single dashboard which is very impressive.\nSince this is in preview phase, we can expect many more updates here. If you are interested in learning more about this, check this out.\nHope you have learnt something new!\nCover Photo by Raphaël Cubertafon on Unsplash\n","date":"Feb 13, 2023","href":"https://blog.talanlabs.com/microsoft-defender-for-devops/","kind":"page","labs":null,"tags":["microsoft","security","IaC"],"title":"Microsoft Defender for Devops"},{"category":null,"content":"Valeur ajoutée du métavers dans différents secteurs porteurs Le métavers est un monde alimenté par les technologies mêlant réalité virtuelle, réalité augmentée1 et 3D et dans lequel l\u0026rsquo;individu peut, sous la forme d\u0026rsquo;un avatar, interagir avec des personnes, de manière continue, à tout moment et à tout endroit. Le métavers promet donc d\u0026rsquo;avoir un réel succès et de générer d’une part de la valeur financière pour les entreprises et d’autre part de la valeur expérientielle pour les utilisateurs.\nDifférents secteurs essaient aujourd’hui de l\u0026rsquo;incorporer dans leurs activités. Pour autant, il n’est pas sans dire que cet univers soulève tout de même certains questionnements. Les entreprises pourraient faire face à des problématiques de responsabilités juridiques puisque les règles dans cet univers ne sont pas encore précises et qu’il s’agit encore d’une zone grise. Elles devront également réfléchir à leur impact environnemental compte tenu de l’utilisation excessive d’énergie que suppose le recours au métavers. Les utilisateurs quant à eux pourraient être confrontés à un risque lié à l’isolement et à l’addiction qui entrainerait une dépendance et une difficulté à se déconnecter de cet univers et à vivre dans le monde réel.\nNous avons choisi de nous focaliser principalement sur trois secteurs qui nous semblent porteurs pour le métavers : le secteur bancaire, le secteur agroalimentaire et celui du luxe.\nLe secteur bancaire manifeste un intérêt pour le Métavers.\nLes banques cherchent à moderniser leur image et les services offerts à leur clientèle en étant ouvertes aux évolutions technologiques.\nCertaines ont établi des partenariats avec des plateformes sur le Métavers. La banque coréenne Kookmin Bank permet par exemple à ses clients d’échanger avec leur conseiller sur le métavers [1]. JP Morgan a développé sur la plateforme Decentraland, un espace où les clients peuvent créer un avatar et s\u0026rsquo;informer sur leur banque mais aussi une agence virtuelle qui à terme a pour objectif d’avoir les mêmes services que les agences traditionnelles.\nCes transactions seront ainsi consignées sur la blockchain de JP Morgan. [2]\n De même, la banque HSBC identifie les éventuelles opportunités mais s\u0026rsquo;y positionne doucement, notamment en achetant un terrain dans Sandbox [3]. Enfin, des projets de banques décentralisées ont également vu le jour grâce au Metavers, comme la Meta Bank DeFi co-fondée par May Mahboob.[4] Elle reposerait entièrement sur la cryptomonnaie. Son but serait d’initier les clients au Metavers et au concept de la virtualisation dans les services financiers. Mais Meta Bank DeFi souhaiterait aller encore plus loin en développant des espaces de jeux, d’art et des casinos. Reste à savoir si ce projet verra réellement le jour.\nLes secteurs alimentaire et nutritif semblent être les secteurs les plus touchés par le Métavers.\nEn effet, toutes les parties prenantes de ce secteur en bénéficieront. Les entreprises alimentaires gagneraient en matière d’optimisation de leur processus de supply chain. Plusieurs processus peuvent être réalisés simultanément et rapidement, comme la conception des boîtes alimentaires et le partage avec les fabricants. Ajoutons à cela que ces espaces virtuels appuieraient la traçabilité vis-à-vis des différents fabricants intervenant sur une marque alimentaire. Ces derniers auront accès à des représentations graphiques 3D de la conception des produits alimentaires, leur vente et leur distribution. Le Métavers appuiera également la traçabilité vis-à-vis des clients. Ils pourront ainsi suivre le déroulement du cycle, depuis la collecte des matières premières jusqu’à la distribution des produits alimentaires [5].\nConcernant les consommateurs, ils bénéficieront d’une expérience immersive. Ils pourront visualiser le produit virtuellement, à distance et plus simplement. Ils auront également l’occasion de se renseigner sur sa composition nutritive, sa conservation et sa date d’expiration. De plus, les commerçants pourraient voir diminuer le nombre de plaintes et de retours.\nLes restaurateurs, quant à eux, envisagent de nouveaux canaux de distribution plus digitaux. Cela passe par la mise en place d’accord de collaboration avec les acteurs du Métavers. La chaîne de restauration américaine Chipotle, par exemple, a ouvert un restaurant virtuel pour les joueurs de Roblox2 [6] sur le thème d’Halloween puis ont reçu un code promotionnel pour un achat gratuit dans le monde réel. Par ailleurs, les restaurateurs contrôleraient plus efficacement les réservations non respectées en demandant un acompte de réservation versé en crypto grâce à un système d’entiercement basé sur des contrats intelligents. D’autres parties prenantes pourraient également bénéficier indirectement du métavers en collaborant avec des acteurs plus avancés dans l’adoption du métavers. Walmart a, par exemple, créé un service de conseil en nutrition sur le Métavers. Le joueur créé son avatar et peut consulter un expert, par exemple un nutritionniste, pour recueillir des conseils sur son mode de vie. Ces mêmes experts pourraient alors élargir leur portefeuille clientèle.\nAu-delà du gain économique que pourrait représenter le métavers, son apport humanitaire peut être considéré. Ainsi, Unilever propose, par exemple, aux joueurs, une plateforme à travers laquelle ils peuvent faire un don de nourriture virtuelle. En contrepartie de cette donation, une réelle société alimentaire fait un don à des associations De même, Carrefour collecte l’argent résultant de la vente de NFBEE3 en faveur de l’association BeeFund de la Fondation de France. Cette dernière vise à préserver les abeilles.\nMais qu’en est-il du secteur du luxe ?\nGucci qui inaugure prochainement une exposition d’œuvres NFT pour célébrer l’histoire et le savoir-faire de la maison et qui créé également une académie dédiée au gaming[7]. Balenciaga et Prada qui développent des avatars sur l’Avatar Store de Meta, pour permettre aux utilisateurs de personnaliser leurs avatars avec leurs créations et leurs accessoires [8]\u0026hellip; Pourquoi le secteur du luxe est-il à l’assaut du métavers ? Quelles sont les opportunités que représentent le métavers pour les Maisons ? Le métavers permet au secteur du luxe de renforcer son image de marque auprès de ses clients en leur proposant des expériences immersives à tout moment. Par exemple, Ralph Lauren veut créer une boutique virtuelle où il est possible de faire des achats [9]. De plus, les Maisons collaborent avec d’autres marques pour attirer une nouvelle génération de consommateurs. La collaboration Louis Vuitton avec Suprême, une marque de vêtements streetwear a par exemple permis de toucher une clientèle plus jeune et d’augmenter les ventes. Pour autant, l’utilisation de cet outil reste toujours en adéquation avec leur stratégie de rareté et d’exclusivité. En effet, ils développent des NFT en quantité limitée, qui représentent une valeur ajoutée aux clients qui sont les seuls à les détenir. Le métavers est également utilisé pour renforcer leur engagement dans des causes sociales. La marque Clinique a par exemple lancé une campagne de maquillage NFT pour soutenir l’inclusivité dans le métavers à travers le slogan « Metavers more like us » [10]. Cette initiative est destinée aux femmes ayant différentes carnations et formes de visage. Pour chaque look NFT, il est possible d’avoir les références des produits utilisés pour que le client puisse les acheter en magasin.\n En outre, le métavers présente plusieurs atouts pour les acheteurs. En effet, il permet de garantir l’authenticité des articles. C’est pourquoi l’agence digitale Emakina met à disposition de ses clients ses expertises de conseil business et de design produits/services. Elle souhaite développer un partenariat avec Arianee, spécialiste dans la création de passeports numériques pour les marques de luxe [11]. L’objectif étant de suivre l’historique de réparations et de retouches de chaque produit grâce aux NFT. Enfin, les NFT ajoutent de la valeur au produit physique. la maison LVMH permet par exemple d’obtenir un jumeau numérique en NFT du produit acheté et grâce à ces NFT le client accède à des ventes et des défilés privés. Ces jetons non fongibles leur permettent de devenir des clients importants et d’avoir accès à des collections limitées. Ainsi, le secteur du luxe compte bien tirer profit des avantages du métavers pour fidéliser ses clients et renforcer son image de marque. Reste à savoir si les Maisons réussiront à conserver leur héritage tout en s’adaptant aux changements actuels. En guise de conclusion, nous avons pu voir en quoi le métavers contribue à la transformation d’entreprises issues de différents secteurs. L’idée de traçabilité est omniprésente dans tous les secteurs confondus. Pour le secteur bancaire, cette dernière permet une plus grande rapidité et fiabilité des transactions. Pour le secteur de l’agroalimentaire et du luxe, elle permet de renforcer la confiance des consommateurs. Le développement de l’expérience immersive quant à lui, montre que le produit en soi n’est plus le seul à avoir de l’importance parce que l’immersion dans ce monde est en elle-même une valeur ajoutée pour le client.\n1 réalité virtuelle, réalité augmentée : La réalité virtuelle permet de s\u0026rsquo;isoler du monde qui nous entoure alors que la réalité augmentée va apporter des éléments nouveaux et du relief au monde qui nous entoure.\n2 Roblox : Roblox est une plateforme gratuite qui permet à ses utilisateurs de développer leurs propres jeux, et de jouer aux créations d’autres joueurs, tout en interagissant entre eux via un système de chat. Sorti en 2005, Roblox s’adresse principalement aux enfants et aux adolescents.\n3 NFBEE : C’est la collection NFT lancée par Sandbox en collaboration avec les supermarchés Carrefour. Elle correspond à une famille adorable de petites abeilles.\nSources:\nCover image: https://trustmyscience.com/wp-content/uploads/2021/10/definition-Metavers-univers-virtuel-internet-futur-couv.jpg Article images: https://unsplash.com/photos/sRF-FoyPQss https://unsplash.com/photos/Q5sLnczWeWE\n[1] La banque sud-coréenne KB Bank entre dans le métavers - CoinNews\n[2] https://fintechmagazine.com/banking/jp-morgan-becomes-the-first-bank-to-launch-in-the-Metaverse\n[3] Le big data, mars 2022, \u0026ldquo;HSBC devient la 1ère banque du métavers en achetant un terrain Sandbox\u0026rdquo;. Disponible sur: https://www.lebigdata.fr/hsbc-Metavers-sandbox\n[4]May Mahboob: World’s First Decentralized 360 Solution bank in the photorealistic Metaverse – Crypto Pill (yourcryptopill.com)\n[5] Comment Le Metaverse Peut-il Aider L\u0026rsquo;industrie Alimentaire ? - Tech Tribune France\n[6] Danone et le Métavers, Qu’est ce qui nous attend ?\n[7] Journal du luxe, juin 2022, “Gucci inaugure une exposition d’oeuvres NFT.” Disponible sur: https://journalduluxe.fr/fr/mode/gucci-art-vault-exposition-nft#:~:text=Tr%C3%A8s%20investie%20dans%20l'exploration,exp%C3%A9rience%20immersive%20Vault%20Art%20Space.\n[8] Journal du luxe, juin 2022, “Balenciaga, Prada et Thom Browne débarquent sur l\u0026rsquo;Avatars Store de Meta.” Disponible sur: https://journalduluxe.fr/fr/business/balenciaga-prada-browne-meta-avatar\n[9] Forbes, décembre 2021, “Pourquoi Ralph Lauren veut vendre des vêtements virtuels.” Disponible sur: https://www.forbes.fr/societe/la-mode-dans-le-Metavers-pourquoi-ralph-lauren-veut-vendre-des-vetements-virtuels/\n[10] Clinique, juin 2022, “Metaverse like us.” Disponible sur: https://www.clinique.com/Metaverselikeus\n[11] Emakina x Arianee, mai 2022, “Offrez un passeport numérique à vos produits grâce aux NFTs”. Disponible sur: https://arianee.emakina.fr/\n","date":"Feb 10, 2023","href":"https://blog.talanlabs.com/valeur-ajoutee-metavers-secteurs-porteurs/","kind":"page","labs":null,"tags":["Metavers","Innovation","Luxe","Agroalimentaire","Banque"],"title":"Valeur ajoutée du métavers dans différents secteurs porteurs"},{"category":null,"content":"1.\tIntroduction Récemment, les différentes partie prenantes de l’écosystème IT ont commencé à voir les avantages du MLOps, définit comme étant un processus systémique pour la collaboration des équipes de développement et d’opération pour les solutions Machine Learning, dans l’exécution d’un pipeline de production efficace.\nPour cet effet, Talan, à travers son partenariat avec Amazon, s’engage à former ses data scientists et ML engineers sur les technologies AWS pour la conception, le développement et le déploiement des solutions Machine Learning.\nAinsi, je viens d’obtenir la certification AWS Machine Learning Speciality (MLS-C01) pour s’aligner avec les enjeux de l’écosystème et proposer des solutions de qualité à nos clients. Dans cet article de blog, je présente ma stratégie pour préparer cet examen ainsi que les concepts et les services à maitriser avant de payer les 300 dollars de l’examen.\n Mon certficat AWS Machine Learning Speciality  2.\tPrérequis indispensables Avant de commencer la préparation de la certification, il est important d’avoir une expérience de 2 à 5 ans dans la conception et le développement des solutions Machine Learning. Cette expérience devrait inclure la préparation des données, la compréhension des cas d’usage, le choix du modèle ML, l’apprentissage des modèles, l’optimisation des hyperparamètres, l’évaluation des performances et le déploiement des solutions.\nElle sera surement bénéfique afin de comprendre les enjeux et les problèmes résolus par les services AWS.\nAussi, il est fondamental de commencer par maitriser les notions de base de la technologie cloud et les services proposés par AWS.\nPour cela, la certification AWS Cloud Practitioner Essentials est fortement recommandé avant MLS-C01 afin de maitriser le fonctionnement des services AWS comme S3, IAM, EC2, Glue, Kinesis et SageMaker ainsi que de comprendre des concepts de base comme la sécurité, la scalabilité, l’élasticité et la tolérance des pannes du cloud.\nC\u0026rsquo;est pourquoi, pour cette certification, il faut être à la fois un bon data engineer, un bon data scientist et un bon MLOps engineer.\n Les certifications AWS  3.\tObjectifs de la certification A la fin de cette expérience, vous serez (normalement) capable de :\n Comprendre les cas d’usage qui nécessitent une solution Machine Learning et ses enjeux Sélectionner et justifier l’approche Machine Learning appropriée pour un cas d’usage donné Identifier les services AWS appropriés pour l’implémentation d’une solution Machine Learning Concevoir et développer des solutions Machine Learning scalables, optimisées en terme de coûts, fiables et sécurisées  4.\tDétails de l’examen MLS-C01 L’examen de la certification de spécialité AWS Machine Learning (MLS-C01) est composé de 65 questions à choix multiples et questions à réponses multiples. Vous avez 180 minutes pour répondre à toutes les questions. Les détails de l’examens sont présentés dans le tableau suivant :\n   Code de l’examen MLS-C01     Type de l’examen Speciality   Durée 180 minutes   Cout de l’examen 300 $   Format Questions à choix multiple et questions à réponses multiple   Notation de l’examen Note graduée de 100 à 1000   Score de réussite \u0026gt;750   Langues Anglais, japonais, coréen et chinois simplifié    5.\tContenu technique Le certificat de spécialité AWS Machine Learning porte sur 4 domaines :\n Ingénierie des données Analyse exploratoire de données Modélisation Mise en œuvre et implémentation du Machine Learning  Dans cette section, nous revenons sur les points les plus important dans chaque domaine. Il s’agit de ma cheat sheet que j’ai révisé avant l’examen.\n Domaines couverts par la certification AWS Machine Learning Speciality  Avant de commencer avec les notes techniques, voici les services AWS les plus utilisés dans la préparation de l’examen.\n Les services AWS les plus couverts par la certification  5.1.\tIngénierie des données 5.1.1.\tCréation des répertoires et entrepôts de données pour les projet Machine Learning  Le choix du système de fichiers utilisé devrait dépendre de plusieurs facteurs comme le temps de chargement des données Généralement, Amazon S3 est le système de stockage le plus utilisés dans les projets ML pour plusieurs raison. En effet, S3 est utilisé pour tous les types et les tailles de données et il est infiniment scalable. De plus, il est très abordable en terme de budget. S3 Identification des sources de données: supported-data-sources Les systèmes de stockage les plus utilisés dans les projets ML à part Amazon S3 sont Redshift, RDS Database, Elastic File System (EFS) et Elastic Block Store (EBS): Redshift | RDS | Host-instance-storage-volumes | Amazon Elastic File System S3 : La taille d’un fichier est entre 0 bytes et 5Tb. Par contre, la capacité de stockage de données est illimitée. Le nom d’un bucket devrait être unique Amazon Relational Database Service (RDS) : BD relationnelles (MySQL, SQL Server, Oracle, Postgres, Aurora, MariaDB) DynamoDB : est un service de base de données propriétaire NoSQL entièrement géré qui prend en charge les structures de données de type clé-valeur et document RedShift : c’est une BI datawarehouse. Elle permet d’utiliser Quicksight pour la visualisation des données TimeStream : Amazon Timestream est un service de base de données de séries temporelles rapide, évolutif et sans serveur, qui facilite le stockage et l\u0026rsquo;analyse de milliards d\u0026rsquo;événements DocumentDB : Amazon DocumentDB est un service compatible avec MongoDB. C’est une base de données document JSON native entièrement gérée qui facilite et réduit le coût de l\u0026rsquo;exploitation de charges de travail de documents critiques à pratiquement n\u0026rsquo;importe quelle échelle, sans gérer l\u0026rsquo;infrastructure. Les outils AWS pour la migration de données sont :  Data Pipeline Database Migration Service (DMS) Glue   DMS est utilisé pour la migration de la data qui est stocké sur:  Des serveurs locaux Une instance Amazon RDS DB Une instance Amazon EC2   DMS est utilisé aussi pour la migration des données d’une DB vers S3 Une seule transformation possible en utilisant DMS : le changement des noms des colonnes Glue est un ETL qui permet de simplifier la catégorisation, le nettoyage, l’enrichissement et le transfert des données d’une manière fiable entre plusieurs Data stores AWS Lake Formation est un service qui permet de mettre en place facilement un lac de données sécurisé en quelques jours.  5.1.2.\tMaitriser les méthodes d’ingestion de données  Il y a deux types d’ingestion de données : traitement par lots (Batch processing) et traitement en continu ou en temps réel (Stream processing) Maitriser les pipelines d’ingestion de données en se concentrant sur le service Kinesis Kinesis Data Firehose peut écrire les données dans votre data lake S3. Kinesis Data Analytics doit être alimenté en données en continu par Kinesis Data Streams ou Kinesis Data Firehose Kinesis Data Analytics ne peut pas ingérer les données directement. Apache Flink peut écrire vos données sur S3 à l\u0026rsquo;aide du puits de fichiers en continu, mais il écrit dans les formats AVRO et Parquet, et non GZIP Kinesis Data Streams ne peut pas écrire directement sur S3. Il a besoin d\u0026rsquo;une application Kinesis Consumer Library pour recevoir les données et les écrire ensuite sur S3 L’interaction avec Kinesis Data Streams est possible via :  Kinesis Producer Library (KPL) pour écrire. Ceci se fait à travers des Java wrappers automatiques et configurables Kinesis API (SDK) Kinesis Client Library (KCL) pour interagir avec KPL afin de traiter les données   Amazon Elastic MapReduce (EMR): Link AWS Glue est très utilisé dans ce contexte. Il est un service d’intégration de données sans serveur qui rend la préparation de données plus simple, plus rapide et surtout moins chère AWS Glue Format de fichier avec Athena : Le format de fichier Parquet est un format basé sur des colonnes, et il supporte le partitionnement. L\u0026rsquo;autre format de fichier basé sur des colonnes supporté par Athena est ORC. Ces formats de fichiers basés sur des colonnes sont plus performants que les formats tabulaires tels que CSV et TSV lorsqu\u0026rsquo;Athena travaille avec de très grands ensembles de données. XML n\u0026rsquo;est pas un format de données pris en charge pour la formation dans SageMaker. La source de données la plus couramment utilisée pour SageMaker est un bucket S3. Cependant, vous pouvez également utiliser Athena, EMR et Redshift comme sources de données pour SageMaker. Pour afficher des tableaux de bord en utilisant un outil BI, le pipeline est assuré par les services suivants : Kinesis Data Firehose -\u0026gt; S3-\u0026gt; Lake Formation -\u0026gt; QuickSight Apache Flink permet d’écrire sur S3 en utilisant streaming file sink dans un format AVRO ou Parquet. Parfois, c’est plus efficace d’écrire son code de transformation de données en utilisant la fonction lambda et puis transférer les données dans S3 avec le même service Apache Kafka est utilisé comme une source de données en continu (streaming) où les features sont directement alimentés dans le Feature Store en ligne Kinesis Data Analytics avec la fonction Lambda pourrait être utilisé comme une source de données en continu (streaming) où les features sont directement alimentés dans le Feature Store en ligne Amazon QuickSight prend en charge une variété de sources de données que vous pouvez utiliser pour fournir des données pour les analyses. Certaines sources de données comme DynamoDB ne sont pris en charge. La liste des sources de données est présentée dans ce guide : Supported data sources . Parmi ces sources, on trouve Snowflake, Presto, Teradata et S3. AWS Glue ne peut pas charger les données directement vers le notebook SageMaker pour lancer l’apprentissage d’un modèle ML. Pour résoudre ce problème, Glue devrait placer les données dans un bucket S3 qui sera charger par la suite par le notebook SageMaker AWS Lake Formation permet de collecter et cataloguer les données à partir de plusieurs sources de données, transformer les données et les charger en un S3 data lake IoT Core est utilisé pour collecter de l’information à partir des capteurs IoT IoT Greengrass est utilisé pour déployer le modèle et tester l’inférence sur les capteurs connectés  5.1.3.\tMaitriser les méthodes de transformation de données  AWS Glue est le service ETL le plus utilisé pour extraire, transformer et charger les données how-to-extract-transform-and-load-data-for-analytic-processing-using-aws-glue? Glue ETL est utilisé dans le traitement par lots (batch processing) et ne peut pas être utilisé dans le traitement en temps réel Apache Spark Streaming est un moteur analytique utilisé pour traiter les données volumineuses en exécutant des jobs de traitement distribués Kinesis Data Analytics ne possède pas les capacités de Apache Spark pour traiter les données volumineuses AWS Glue Data Catalog est un référentiel central permettant de stocker les metadata structurelles et opérationnelles de tous les actifs de données AWS Glue crawlers se connecte à un data store, passe par une liste hiérarchisée de classificateurs pour extraire le schéma des données et d\u0026rsquo;autres statistiques, puis alimente le Glue Data Catalog avec ces metadata Traiter les données spécifiques au ML à l\u0026rsquo;aide de map reduce (Hadoop, Spark, Hive) large-scale-machine-learning-with-spark-on-amazon-emr | apache-spark | create-and-manage-amazon-emr-clusters-from-sagemaker-studio-to-run-interactive-spark-and-ml-workloads EMR est utilisé pour la transformation des big data à travers plusieurs EC2 via différents frameworks comme Spark, Tensorflow et Hive Athena est un service sans serveur qui permet de lancer des requêtes SQL sur S3  5.2.\tAnalyse exploratoire de données 5.2.1.\tPréparation de la donnée pour le modèle ML  L’identification et le traitement des données manquantes est un concept important pour améliorer les performances des modèles de prédiction. managing-missing-values La normalisation et la standardisation des données sont deux techniques de traitement de données La normalisation redimensionne les données afin que toutes les valeurs soient comprises entre 0 et 1 La standardisation centres les valeurs des features autour de la valeur moyenne La standardisation est plus efficace que la normalisation sur la gestion des outliers AWS propose des services d’annotation de données comme Amazon SageMaker Ground Truth and Mechanical Turk  5.2.2.\tFeature engineering  L’extraction des features à partir des datasets: Feature Extraction Binning : est une technique permettant de réduire la cardinalité des données continues et discrètes. Le binning regroupe des valeurs dans des bacs afin de réduire le nombre de valeurs distinctes. Tokenization : est utilisée dans le traitement du langage naturel (NLP) pour diviser les paragraphes et les phrases en unités plus petites auxquelles on peut plus facilement attribuer une signification. Outliers : Comment traiter les valeurs aberrantes dans un modèle de classification d\u0026rsquo;images ? Une des techniques utilisées pour résoudre ce problème est d’agir sur l\u0026rsquo;hyperparamètre learning_rate qui régit la vitesse à laquelle le modèle s\u0026rsquo;adapte à des données nouvelles ou changeantes. Les valeurs valides vont de 0,0 à 1,0. En définissant cet hyperparamètre à une valeur faible, telle que 0,1, le modèle apprendra plus lentement et sera moins sensible aux valeurs aberrantes. Réduction de la dimensionnalité One hot encoding : est la conversion d\u0026rsquo;informations catégorielles dans un format qui peut être introduit dans des algorithmes d\u0026rsquo;apprentissage automatique pour améliorer la précision des prédictions. C’est une méthode courante pour traiter les données catégorielles dans l\u0026rsquo;apprentissage automatique Les features synthétiques TF-IDF : permet de déterminer l’importance des mots dans différents documents Parmi les techniques utilisées pour la sélection et l’engineering des features :  Supprimer les features qui ne sont pas liés à l’apprentissage supprimer les features qui ont les mêmes valeurs, une très faible corrélation, une très faible variance ou beaucoup de valeurs manquantes. Appliquer le Principal Component Analysis (PCA) pour réduire la dimensionnalité, c-à-d la réduction du nombre des features Appliquer des techniques comme One-hot encoding and label encoding pour convertir les strings en des features numériques afin de simplifier l’apprentissage Appliquer la normalisation pour gérer les données avec une large variance   Parmi les techniques de traitement des données manquantes :  Supprimer les features avec beaucoup de données manquantes Imputation à l\u0026rsquo;aide de valeurs moyennes/médianes - valable uniquement pour les valeurs numériques et non pour les features catégorielles ; ne tient pas compte de la corrélation entre les caractéristiques Imputation à l\u0026rsquo;aide de k-NN, Imputation multivariée par équation chaînée (MICE), Deep Learning, factorise la corrélation entre les caractéristiques   Parmi les techniques pour gérer les données non équilibrées :  Demander plus de données Sur-échantillonnage de la classe minoritaire et sous-échantillonnage de la classe majoritaire Augmentation des données à l\u0026rsquo;aide de techniques telles que SMOTE   Smoothing est une technique qui permet de supprimer le bruit du dataset d’apprentissage Les deux métriques utilisées par SageMaker Data Wrangler pour visualizer les rapports des fuites cibles sont R2 et AOC-ROC Multivariate imputation est une technique d’imputation qui permet d’utiliser d’autres variables de la data pour prévoir les données manquantes Mean imputation est une technique d’imputation qui permet d’utiliser la valeur moyenne de la colonne pour remplir les données manquantes Le nettoyage des données et la préparation des features pourrait se faire en créant un SageMaker Processing job avec SageMaker Python SDK et le package scikit-learn SKLearnProcessor ou Spark PySparkProcessor qui permettent de préparer les données et les diviser en train et test datasets  5.2.3.\tAnalyse et visualisation de données  Les différents types de graphe les plus utilisés par les data scientists : Scatter, histogram, box plot Elbow plot pour les modèles de clustering Catplot est utilisé pour visualiser la relation entre une valeur numérique et une ou plusieurs valeurs catégoriques en utilisant des visualisations comme boxenplot Swarm plot et utilisé pour visualiser la distribution des valeurs de chaque feature Pair plot est utilisé pour visualiser la relation entre les pairs de features et la distribution d’une variable par rapport à l’autre La matrice de covariance permet de montrer le degré de corrélation entre deux features  5.3.\tModélisation 5.3.1.\tCas d’usage du Machine Learning  L\u0026rsquo;approche heuristique est utilisée lorsqu\u0026rsquo;une approche d\u0026rsquo;apprentissage automatique n\u0026rsquo;est pas nécessaire. Un exemple est le taux d\u0026rsquo;accélération d\u0026rsquo;une particule dans l\u0026rsquo;espace. Il existe des formules bien connues pour la vitesse, l\u0026rsquo;inertie et la friction qui permettent de résoudre un tel problème Apprentissage supervisé Vs Apprentissage non supervisé Il important de bien lire l’énoncé de la question pour orienter le sens de la réflexion : algorithme de classification, de régression, etc.  5.3.2.\tSélection du modèle ML pour un problème donnée  Voici un overview sur les modèles les plus utilisés pour résoudre un problème en ML: Algorithmes ML:   Cheat Sheet des algorithmes de Machine Learning en AWS   Expliquer l’intuition derrière chaque modèle : Xgboost, logistic regression, K-means, linear regression, decision trees, random forests, RNN, CNN, Ensemble, Transfer learning La liste des algorithmes pré-entrainés : les algorithmes intégrés d\u0026rsquo;Amazon SageMaker ou les modèles pré-entraînés Time serie data est un point important: Forecasting financial time series with dynamic deep learning on AWS Le célèbre moteur de recommandation d\u0026rsquo;Amazon.com est construit à l\u0026rsquo;aide d\u0026rsquo;une méthode de filtrage collaboratif neuronal. Cette méthode est optimisée pour trouver des similitudes dans des environnements où l\u0026rsquo;on dispose de grandes quantités d\u0026rsquo;actions d\u0026rsquo;utilisateurs à analyser. Random Cut Forest (RCF) est utilisé pour la détection des anomalies. L\u0026rsquo;allocation latente de Dirichlet LDA est un algorithme d\u0026rsquo;apprentissage non supervisé. Le scénario décrit un ensemble d\u0026rsquo;apprentissage supervisé et noté. En outre, LDA est un modèle de \u0026ldquo;sac de mots\u0026rdquo;, ce qui signifie que l\u0026rsquo;ordre des mots n\u0026rsquo;a pas d\u0026rsquo;importance. Ce modèle n\u0026rsquo;est pas idéal pour l\u0026rsquo;analyse des sentiments Amazon Forecast CNN-QR est un le seul algorithme qui accepte les données des séries temporelles connexes sans valeurs futures Les modèles types fournies par Amazon Fraud Detector :  Online fraud insights Transaction fraud insights Account takeover insights   AWS Transcribe http/2 streaming client est utilisé dans le cas où on a des problèmes de connexion ou on passe en offline Long Short-Term Memory (LSTM) est utilisé pour le traitement des séries temporelles ou les données séquentielles Multinominal Naive Bayes est un algorithme qui est très utilisé pour la classification des documents en se basant sur le nombre d’occurrence d’un mot dans le document en question Bernouilli Naive Bayes est un algorithme qui est utilisé pour la classification des documents en se basant sur le fait de chercher l’apparence d’un mot dans le document en question QuickSight intègre un algorithme Random Cut Forest ce qui rend plus efficace de l’utiliser directement comme étant un outil BI pour la détection et l’affichage des outliers. Cet outil est utilisé avec Kinesis Data Firehose et S3 pour la détection des anomalies en temps réel  5.3.3.\tApprentissage des modèles ML  Etudier les techniques de Cross validation comme train_test_split, K-Fold, Stratified K-Fold, hold-out pour les séries temporelles, etc. K-Fold cross validation permet de diviser le dataset en K parties et lancer l’apprentissage du modèle K fois. A chaque fois, une partie est utilisée comme étant un test dataset et le reste, K-1 parties, est utilisé pour le train. A la fin, tous les résultats sont combinés pour avoir le résultat final Optimizer, gradient descent, loss functions, local minima, convergence, batches, probability, etc. Le choix des ressources (GPU, CPU) dépend de l’algorithme en AWS XGBOOST : SageMaker XGBoost ne prend actuellement en charge qu\u0026rsquo;une instance CPU ou une instance GPU à une seule instance pour l\u0026rsquo;apprentissage. Ajoutez Horovod à votre code et utilisez son cadre de formation d\u0026rsquo;apprentissage profond distribué pour TensorFlow. Parmi les techniques efficaces de la configuration des hyperparamètres du modèles en termes de performances : choisir un petit learning rate, un large nombre d’estimateurs et un early stoping Quand on possède un large datasets, on a généralement tendance à utiliser un réseau de neurones avec un grand nombre de couches cachées Amazon CodeGuru est utilisé pour automatiser la révision de code et détecter les failles de sécurité  5.3.4.\tOptimisation des hyperparamètres  La définition des hyperparamètres de l’algorithme SageMaker XGBoost dépend de l’application et certains hyperparamètres sont obligatoire avant de lancer l’apprentissage comme Num_round et Num_class pour une application de classification multi-class Les hyperparamètres du Amazon SageMaker XGBoost : xgboost_hyperparameters Les techniques de régularisation : L1/L2, Dropout, etc. Initialisation des modèles Architectures des réseaux de neurones Learning rate Les fonctions d’activation Les méthodes de tunning sont :  Grid search Random search : utilisé quand on lance les training jobs en parallèle Bayesian search : nécessite moins de jobs d’apprentissage pour générer les hyperparamètres optimaux   Généralement, avec les réseaux de neurones qui ont un problème d’overfitting on :  Augmente le dropout Augmente la régularisation Diminue les combinations des features   Ridge regression et Lasso regression sont utilisées pour réduire la complexité du modèle. Lasso regression élémine quelques features ce qui réduit effectivement la complexité  5.3.5.\tEvaluation des modèles ML  L’overfitting est parmi les points clés les plus évoqués dans les questions. Alors, il faut maitriser les techniques utilisées pour l’empêcher avec les différents modèles L’overfitting se produit lorsque le modèle ne peut pas généraliser et s\u0026rsquo;adapte trop étroitement à l\u0026rsquo;ensemble de données d\u0026rsquo;apprentissage. Overfitting Dropout est une technique couramment utilisée pour pénaliser les poids et empêcher l’overfitting. Mais, il s\u0026rsquo;agit d\u0026rsquo;une technique utilisée avec les réseaux neuronaux, et non avec les arbres de décision. Certaines questions propose de d’éviter l’overfitting en utilisant dropout avec des modèles supervisés, ce qui complétement faux Random Forest est utilisée pour éviter l’overfitting avec les arbres de décision Eviter l’underfitting: Underfitting-vs-Overfitting Les métriques d’évaluation sont : AUC-ROC, accuracy, precision, recall pour les modèles de classification, RMSE, MSE pour les modèles de régression et F1 Score pour les datasets non équilibrés La matrice de confusion pour les modèles de classification :  Un vrai positif est un résultat où le modèle prédit correctement la classe positive. De même, un vrai négatif est un résultat où le modèle prédit correctement la classe négative Un faux positif est un résultat où le modèle prédit incorrectement la classe positive. Et un faux négatif est un résultat où le modèle prédit de manière incorrecte la classe négative   L\u0026rsquo;aire sous la courbe ROC (Receiver Operating Characteristic) est la mesure la plus couramment utilisée pour comparer les modèles de classification K-means utilise msd (Mean Squared Distances) pour la validation du modèle. Cependant, vous voudrez minimiser cette métrique. K-means utilise ssd (Sum of the Squared Distances) pour la validation du modèle. Cependant, vous voudrez minimiser cette métrique. Dans cette référence, vous trouverez un ensemble de techniques couramment utilisées pour améliorer les performances des modèles de Deep Learning : How To Improve Deep Learning Performance La matrice de confusion qui permet de calculer les métriques comme la précision, le recall et l’accuracy :   La matrice de confusion   Précision : TP/(TP+FP) Recall : TP/(TP+FN) Accuracy : (TP+TN)/(TP+FN+TN+FP) F1 Score est une métrique qui combine la précision et le recall : F1-Score=2 (Precision .Recall)/(Precision+Recall) La métrique Mean Absolute Error (MAE) a la capacité de gérer les outliers  5.4.\tMise en œuvre et implémentation du Machine Learning 5.4.1.\tConstruire des solutions ML pour la performance, la disponibilité, l\u0026rsquo;évolutivité, la résilience et la tolérance aux pannes  Journalisation et surveillance en utilisant CloudWatch and CloudTrail Journalisation et surveillance Créez un tableau de bord CloudWatch pour afficher une vue des paramètres de latence, d\u0026rsquo;utilisation de la mémoire et d\u0026rsquo;utilisation du processeur de la variante du modèle SageMaker Deep Learning AMI: Link Docker est couramment utilisé dans le déploiement des modèles ML: Docker Mise à l\u0026rsquo;échelle automatique des modèles Amazon SageMaker: Scalabilité automatique des modèles Amazon SageMaker AWS Auto Scaling surveille vos applications et ajuste automatiquement la capacité pour maintenir des performances constantes et prévisibles au coût le plus bas possible En utilisant Auto Scaling, il faut définir une politique de scaling: Policies in EC2  Simple Scaling Target tracking Scaling Step Scaling   Meilleures pratiques du ML dans les services financiers: Best practices  5.4.2.\tServices AWS pour le Machine Learning  Comprehend : un service pour les applications NLP qui permet de trouver des features et des relations dans le texte Polly : générer la voix à partir d’un texte Lex : fournit des interfaces conversationnelles utilisant la voix et le texte, utiles pour créer des chatbots vocaux et textuels. Transcibe : fournit une capacité de conversion de la parole en texte Translate : fournit une traduction linguistique naturelle et fluide Rekognition : permet d’analyser des images et des vidéos Les limites des services AWS: SageMaker Points de terminaison et quotas Amazon Le format des données d’entrée dépend de l’algorithme pré-entrainé utilisé: algorithmes intégrés d\u0026rsquo;Amazon SageMaker ou les modèles pré-entraînés Apache Spark pourrait être utilisé avec les services AWS pour la gestion des données volumineuses Apache Spark Streaming supporte le traitement en temps réel des données Apache Spark MLlib est une bibliothèque Machine Learning qui permet d’exécuter des algorithmes conçus pour être mis à l\u0026rsquo;échelle des clusters pour la classification, la régression, le clustering et le collaborative filtering SageMaker lifecycle est utilisé pour installer les packages ou les notebooks dans ses proposes instances notebook , configurer le réseau et la sécurité des instances notebook et utiliser les script shell pour personnaliser les instances notebook SageMaker Hosting Services permet d’envoyer des requêtes vers un HTTPS endpoint pour tester des inférences SageMaker Batch Transform est utilisé pour tester des inférences sur un dataset en entier sans avoir besoin d’un endpoint persistent Le reste des services est présenté dans cette documentation: Les services ML d\u0026rsquo;Amazon  5.4.3.\tSécurité des solutions ML  La sécurité est traitée d’une manière très légère mais vous aurez quelques questions sur les services de sécurité de AWS comme IAM. SageMaker permet de lire des données stockées en S3 et cryptées en utilisant AWS Key Management Service (KMS). Il est important de s’assurer que les politiques de clé KMS incluent le rôle attaché à SageMaker utilisé IAM : garantit que les bonnes personnes et les bons rôles dans votre organisation (identités) peuvent accéder aux outils dont ils ont besoin pour faire leur travail S3 bucket policies: Exemples Security groups: agissent comme un pare-feu virtuel, en contrôlant le trafic qui est autorisé à atteindre et à quitter les ressources auxquelles il est associé. Par exemple, après avoir associé un groupe de sécurité à une instance EC2, il contrôle le trafic entrant et sortant de l\u0026rsquo;instance: Groupes de sécurité Amazon Virtual Private Cloud (VPC): permet de lancer des ressources AWS dans un réseau virtuel défini. Ce réseau virtuel ressemble beaucoup à un réseau traditionnel exploité dans le data center d\u0026rsquo;origine, avec les avantages de l\u0026rsquo;utilisation de l\u0026rsquo;infrastructure évolutive d\u0026rsquo;AWS. Il est logiquement isolé des autres réseaux virtuels dans le cloud AWS. Vous pouvez spécifier une plage d\u0026rsquo;adresses IP pour le VPC, ajouter des sous-réseaux, ajouter des passerelles et associer des groupes de sécurité. Un sous-réseau est une plage d\u0026rsquo;adresses IP dans votre VPC: Amazon VPC Encryption: AES-256 est la technologie que nous utilisons pour chiffrer les données dans AWS, y compris le chiffrement côté serveur d\u0026rsquo;Amazon Simple Storage Service (S3): L\u0026rsquo;imporatance du chiffrement  5.4.4.\tDéploiement des solutions ML  Exposer les endpoints et intéragir avec eux: amazon-api-gateway-and-aws-lambda Versionnement des modèles ML: Versionnement de modèle A/B testing: A/B testing Réapprentissage des modèles: Automating model retraining Quand on applique des changements sur le SageMaker endpoint, on modifie le SageMaker HTTPS endpoint sans mettre en arrêt le modèle déjà déployé. On change le type de l’instance du modèle existant et on ajouter un nouveau model variant. La création de la nouvelle configuration de l’endpoint et son déploiement se fait en utilisant l’action UpdateEndpoint Deep Learning AMI est une image machine Amazon fournie par AWS pour une utilisation sur Amazon EC2. Elle est conçue pour fournir un environnement d\u0026rsquo;exécution stable, sécurisé et performant pour les applications de Deep Learning exécutées sur Amazon EC2. Deep Learning AMI n’est pas utilisé pour charger un large dataset d’apprentissage à partir de S3  6.\tPréparation, ressources et planification La stratégie et le temps de préparation de l’examen dépendent systématiquement du niveau d’expertise du candidat. Initialement, j’ai commencé par le cours d’AWS et ses fournisseurs externes. Après, j’ai implémenté quelques solutions ML en utilisant les services AWS et principalement SageMaker et SageMaker Studio. Par la suite, j’ai consulté les documentations AWS des services déjà présentés dans la section précédente. Finalement, j’ai passé plusieurs test pratiques disponibles gratuitement sur plusieurs sites d’apprentissage.\nA la fin, je vous donne quelques ressources très utiles pour préparer l’examen :\n Vidéos:  Fully-Managed Notebook Instances with Amazon SageMaker - a Deep Dive Liste de vidéos Making Friends with Machine Learning - Google Cloud ML-YouTube-Courses   Github:  Amazon SageMaker Examples   Documentation AWS:  AWS Documentation Common Information About Built-in Algorithms Services sans serveur sur AWS   Examen de la certification AWS Machine Learning Speciality (MLS-C01):  Lien de l\u0026rsquo;examen    7.\tDerniers conseils  Commencer la préparation de l’examen en suivant le cours officiel sur la plateforme AWS training and certification: AWS Skill Builder Pendant la préparation de l’examen, il faut avoir de l’expérience pratique avec le service AWS SageMaker afin de maitriser ce service ainsi que d’autres services comme S3 et IAM. Practice makes perfect 😊 Pensez à s’entrainer avec des tests pratiques pour la certification AWS Machine Learning Speciality: Test pratique Il faut maitriser l’anglais ! Certaines questions pourraient être délicates et un seul mot pourrait changer son sens Utiliser l’option « flag the question » pour pouvoir réviser les questions difficiles. Dans ce cas, vous pouvez commencer par les domaines que vous maitrisez en laissant les points compliqués pour la fin. Pendant l’examen, vous serez devant votre écran pendant 180 minutes sans avoir le droit à une pause donc n’oubliez pas de manger un petit snack, d’aller aux toilettes et de charger votre ordinateur avant de commencer la session.  8.\tConclusion L’expérience avec la certification AWS et son parcours d’apprentissage très intense m’a permis de bien comprendre la façon de résoudre les challenges du Machine Learning à l’aide des services AWS. Cependant, ce n’est qu’un « Hello World » pas vers la maitrise de toutes les architectures et tous les paramètres des services développés par AWS. Aujourd’hui, Talan, à travers ses collaborateurs, est prête à aider ses clients et partenaires à relever les challenges autour de l’IA, Data Science et Cloud Computing en mettant à leur disposition ces compétences techniques.\n","date":"Feb 8, 2023","href":"https://blog.talanlabs.com/2023-01-09-aws-ml-speciality/","kind":"page","labs":null,"tags":["Machine Learning","AWS","Cloud","Data Science","Data Engineering"],"title":"AWS Machine Learning Speciality: Le parcours d'apprentissage"},{"category":null,"content":"disclaimer : cet article n’est pas un tutoriel, mais plutôt un condensé de partage d’expérience et de conseils / bonnes pratiques\n Hop hop hop Moussaillon ! Lève les mains de ton clavier !!! On ne commence pas un design system en codant mais en se posant LA bonne question.\n Quelle est ma cible ? Pour être plus clair, aujourd’hui les frameworks qui dominent le marché sont Angular, React, Vue. Donc quelle(s) est/sont votre/vos cible(s) ?\n Un seul des trois ? Les trois mon capitaine !\n Dans cet article nous allons aborder le cas de figure où vous travaillez pour une grande société. Et cette société n’a pas forcé ses équipes projet à utiliser un seul framework. De ce fait, il y a un certain nombre d’applications existantes dans différents frameworks. Vous souhaitez développer une librairie de composants qui puisse être adoptée par l’ensemble de ces applications, ainsi que toutes les futures applications. C’est pour cela que nous n’allons pas cibler seulement un framework.\n Heureusement pour vous il y a des solutions !\n   Les web components Afin d’être compatible avec l’ensemble des frameworks web, y compris ceux qui n’ont pas été cités plus haut, vous devez créer des \u0026#34;web components\u0026#34;.\n Ce que je veux dire par là, c’est que si vous réalisez un composant en Angular, il n’est évidemment pas utilisable en React. Et dans n’importe quelle combinaison Angular, React, Vue, on tombe sur le même constat.\n En revanche tous les frameworks sont compatibles avec les web components.\n   C’est quoi un web component ? Pour une fois en informatique, c’est très simple à comprendre ! 😁\n Voici un ensemble de web components que vous connaissez (liste non exhaustive) :\n \u0026lt;div\u0026gt;ceci est un web component\u0026lt;/div\u0026gt; \u0026lt;h1\u0026gt;ceci est un web component\u0026lt;/h1\u0026gt; \u0026lt;span\u0026gt;ceci est un web component\u0026lt;span\u0026gt; \u0026lt;ul\u0026gt; ceci est un web component \u0026lt;li\u0026gt;ceci est un web component\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt;   Oui oui, vous comprenez bien ce qui est écrit…​ Tous les éléments HTML natifs sont des web components.\n Et il existe des API pour créer nos propres composants web. Donc vous allez créer vos propres éléments HTML. C’est cool non ? 😁\n   On commence à rentrer dans la technique Vous pouvez créer vos propres éléments HTML à la main, en javascript. Si vous avez envie de vous faire mal, c’est possible. (voir documentation officielle)\n Mais il existe des frameworks / librairies pour le faire. Et moi je n’aime pas me faire mal…​\n Donc nous allons utiliser Stencil JS . Pourquoi ce choix ?\n  Parce qu’avec le même code, on peut avoir plusieurs cibles (Angular, React, Vue, Web components, …​)\n  Parce que l’output de Stencil, c’est une librairie de composants qui est facile à fournir à nos utilisateurs\n  Dernier point, je viens juste de le dire, j’aime pas me faire mal…​ Faut suivre un peu !\n     Comment ça plusieurs cibles ? Moussaillon, tant qu’il y a du vent, on peut visiter l’ensemble des mers !\n Comme on l’a dit précédemment, l’avantage majeur des web components, c’est qu’ils sont compatibles avec tous les frameworks. Sauf que c’est aussi leur plus gros défaut…​ 😔\n Les web components suivent l’API standard du HTML. C’est-à-dire que pour passer de la data à un composant, il faut la lui donner à travers un attribut standard, donc via une string.\n \u0026lt;my-component my-attribute=\u0026#34;je ne peux que passer des chaînes de caractères...\u0026#34;\u0026gt; \u0026lt;/my-component\u0026gt;   Hors, à travers les props de nos composants, on passe souvent des données structurées, des tableaux ou des objets.\n Cela a pour conséquence de ne pas pouvoir concevoir des API de composants trop complexes.\n Grâce à Stencil, nous allons pouvoir transpiler notre code dans autant de cibles que l’on souhaite : Angular, React, Vue, Web components…​ Ainsi, nous allons pouvoir retrouver la possibilité de transmettre des données structurées de manière plus complexe qu’en passant par une string.\n En gros, Stencil est capable de wrapper les web components dans des composants \u0026#34;natifs\u0026#34; de votre framework préféré. Regardez la documentation de Stencil pour savoir s’il est bien supporté.\n  Note : Au moment d’écrire cet article, Stencil est capable de wrapper les composants dans les trois principaux frameworks (Angular, React et Vue).\n   Au final vous auriez quatre librairies, puisque vous garderiez la librairie de web components pour les cas qui sortent des trois frameworks principaux. Quatre librairies à mettre à disposition des développeurs de votre entreprise, afin qu’ils commencent de nouveaux projets en intégrant votre design system ou qu’ils puissent faire la transition au rythme de leur développement pour les applications déjà existantes.\n   La suite au prochain épisode Je vous laisse regarder la documentation de Stencil pour commencer à setup votre projet en fonction de vos cibles.\n On se retrouve dans le prochain épisode pour quelques conseils et bonnes pratiques sur vos composants.\n Bon voyage moussaillons !\n   ","date":"Nov 3, 2022","href":"https://blog.talanlabs.com/2022-11-03-design-system-part-1/","kind":"page","labs":null,"tags":["npm","Design System","web component","librairie","stencil js"],"title":"Approche technique sur la réalisation d'un Design System - part 1"},{"category":null,"content":"Cover Photo by Barn Images on Unsplash\nDans le précédent article de cette série, nous avons pu voir les mécanismes autour du versioning de nos librairies via npm\nPartie 2 : Outils et fonctionnalités pour pimper votre expérience npm Commandes npm NPX Commande incluse dans Node qui vous permet d\u0026rsquo;exécuter un package. Cette commande va rechercher la présence du package en local (dans les dépendances du projet courant, ou dans une installation globale). Si elle ne le trouve pas, elle est capable de réaliser un téléchargement temporaire pour une exécution à la volée !\nnpx my-package Commande très utile pour tester des packages de type cli ou autres outils indépendants de votre projet (exemples dans la suite de l\u0026rsquo;article !!)\nnpm audit Vérifiez les failles de sécurité potentiellement présentes dans votre projet. Cette commande permet de réaliser une analyse de votre arbre de dépendance, mais aussi de proposer des correctifs en mettant à jour certains packages. Vous avez sûrement déjà vu ce genre de rapport, qui est automatiquement exécuté et affiché à la fin d\u0026rsquo;un npm install\nAnalyse Tips: Utilisez la commande npm audit --omit=dev pour exclure les devDependencies\nExemple de résultat d\u0026rsquo;analyse via la commande npm audit\nCorrections La commande est capable de proposer des corrections de version pour réduire au maximum les failles de sécurité présentes :\n Montée de version (si semver compatible =\u0026gt; x.Y.Z) Montée de version majeure (manuel, peut contenir des breaking-changes)  En utilisant la sous-commande npm audit fix, l\u0026rsquo;ensemble des patchs semver compatible sont appliqués automatiquement :\n En coulisse, lance un npm install, le package-lock.json est donc mis à jour aussi On peut utiliser l\u0026rsquo;option --force pour installer les updates majeures  Exemple de résultat de correctif via la commande npm audit fix\nPackages et config pour booster npm nvm : Gérer plusieurs versions de Node Cet outil permet une gestion simplifiée des différentes versions de node entre plusieurs projets. Il est compatible sur tous les OS (macOS, linux) et même windows (pas le même outil, mais un équivalent)\nnvm install 14.7.0 # installation d\u0026#39;une version spécifique nvm install 10.24.1 nvm use 10.24.1 # utilisation d\u0026#39;une version installée pour le répertoire courant Il est possible d\u0026rsquo;aller encore plus loin avec un fichier .nvmrc contenant la version de node attendue. Un simple nvm use permet de switcher sur la version indiquée.\nOn peut aussi ajouter à son terminal un script pour switcher automatiquement de version 🤩\nNote : le fichier de config est automatiquement détecté par IntelliJ\nAllez plus loin Pour les utilisateurs macOS et linux, vous pouvez aussi passer à l\u0026rsquo;étape supérieure en utilisant l\u0026rsquo;outil asdf qui permet la gestion de version d\u0026rsquo;un énorme panel d\u0026rsquo;outils (node, java, go, postgres, yarn, etc\u0026hellip;)\nLa documentation complète ici\nengines : Forcer une version de node Il est possible de donner des indications quant aux versions utilisées sur un projet. Il suffit de rajouter un bloc de configuration dans le package.json, ainsi qu\u0026rsquo;un fichier à la racine du projet, .npmrc, avec pour contenu engine-strict=true\n\u0026#34;engines\u0026#34;: { \u0026#34;node\u0026#34;: \u0026#34;\u0026gt;=18.0.0\u0026#34;, \u0026#34;npm\u0026#34;: \u0026#34;\u0026gt;= 8.15.0\u0026#34; } Exemple de blocage si on utilise des versions de node ou npm non compatibles avec celles spécifiées\nonly-allow : Forcer un package manager Vous connaissez peut-être des gestionnaires de package autre que npm : yarn, pnpm, .. Chacun possède une gestion du lockfile différente (pour npm, c\u0026rsquo;est le package-lock.json)\nIl peut être utile de bloquer l\u0026rsquo;utilisation d\u0026rsquo;un gestionnaire différent, afin de s\u0026rsquo;assurer une même résolution au sein d\u0026rsquo;une équipe.\nPour ma part, je rajoute un script au package.json, nommé preinstall. C\u0026rsquo;est un nom spécifique qui lui permet d\u0026rsquo;être exécuté avant chaque npm install. J\u0026rsquo;utilise le package only-allow qui fait échouer l\u0026rsquo;installation en cas de mauvais gestionnaire utilisé. (Vous remarquerez aussi l\u0026rsquo;utilisation de npx 🙄)\n\u0026#34;scripts\u0026#34;: { \u0026#34;preinstall\u0026#34;: \u0026#34;npx only-allow npm\u0026#34; } Exemple de blocage si on utilise yarn, en considérant un package.json avec le script indiqué précédemment\nnpm-check : Vérification des versions de nos packages Ce package vous permet d\u0026rsquo;afficher un rapport concernant les nouvelles versions de vos dépendances, ainsi que des propositions de mise à jour.\n ⚠️ Pas de respect du semver, latest visé  Il permet aussi de détecter des packages non utilisés (vérifiez avant de supprimer, d\u0026rsquo;expérience, les faux positifs sont courants)\nIl existe un mode interactif, qui vous permet de mettre à jour plusieurs packages d\u0026rsquo;un coup :\n npx npm-check -u En coulisse, lance une commande npm install \u0026lt;package\u0026gt;@\u0026lt;version\u0026gt;  Résumé Dans cette seconde partie, j\u0026rsquo;ai pu vous présenter plusieurs outils/packages pour vous aider dans vos projets node :\n npm audit pour l\u0026rsquo;analyse des failles de sécurité dans nos packages ; npx pour exécuter des packages ; nvm pour gérer plusieurs versions de node (et son equivalent plus global asdf) ; config engines pour spécifier les versions de node et npm ; only-allow pour spécifier le gestionnaire de package à utiliser ; npm-check pour verifier rapidement des versions des packages.  Vous n\u0026rsquo;êtes pas obligés d\u0026rsquo;utiliser tous ces outils pour être efficace, prenez ceux qui vont répondre à vos besoins et résoudre vos problématiques.\n","date":"Oct 14, 2022","href":"https://blog.talanlabs.com/npm-tout-ce-que-vous-n-avez-pas-compris-part2/","kind":"page","labs":null,"tags":["npm","Node"],"title":"npm - Des fonctionnalités et outils pour booster l'expérience développeur"},{"category":null,"content":"Une journée Ruche ? Un jeudi par mois, les Labiens se retrouvent pour partager et échanger autour de sujets qu\u0026rsquo;ils choisissent. Technique ou organisationnel, sous forme de présentation ou d\u0026rsquo;atelier, c\u0026rsquo;est l\u0026rsquo;occasion de présenter des sujets qui leur tiennent à coeur ou tout simplement s\u0026rsquo;entraîner en vue d\u0026rsquo;une conférence.\nLa dernière journée Ruche s\u0026rsquo;est déroulée, exceptionnellement, le Mercredi 21 Septembre 2022\nVoici le résumé des différentes sessions proposées en mode \u0026ldquo;hybride\u0026rdquo; (présentiel et distanciel).\nAvancées du projet Startup Network - SUN Talan Labs accompagne un réseau de Startups depuis 2019 afin de favoriser les synergies et proposer des produits et services de demain. Cette session était l\u0026rsquo;occasion de partager toutes les avancées du SUN depuis le début d\u0026rsquo;année.\nMerci à Nour et à Olfa pour ce partage.\nRendez-vous sur la page du SUN pour en savoir d\u0026rsquo;avantage\nEvent Storming Une présentation sur l\u0026rsquo;Event Storming, une approche qui permet de modéliser et visualiser des domaines métiers en équipe. En alternant théorie, exemples concrets et retour d\u0026rsquo;expérience, nous avons pu constater la force de l\u0026rsquo;Event Storming pour aider à détourer des domaines métiers parfois très complexe et surtout faire intéragir les participants en parlant le même langage.\nMerci à Diane pour cette présentation !\nIntelligence Artificielle Explicable Les résultats des modèles d\u0026rsquo;intelligence artifielle sont parfois décevants et il est difficile d\u0026rsquo;en trouver les raisons, c\u0026rsquo;est une boite noire. À travers des exemples concrets, nous avons pu voir comment certaines techniques permettent d\u0026rsquo;expliquer les résultats et de passer à un process plus transparent.\nMerci à Nabil pour ce sujet!\nVous pouvez retrouver plus d\u0026rsquo;informations sur l\u0026rsquo;article associé : Vers une intelligence artificielle explicable.\nArchitecture d\u0026rsquo;une application Front La matinée s\u0026rsquo;est terminée par un partage des bonnes pratiques d\u0026rsquo;architecture Front. En partant de la maquette, nous avons pu voir comment bien organiser son projet en pages, layout et composants.\nMerci à François pour ce récapitulatif très utile.\nAtelier Rust : Apprendre Rust en créant une application en ligne de commande Un atelier de 2h avec une vingtaine de participants pour apprendre Rust par la pratique L\u0026rsquo;occasion de tester le support et prendre en compte les retours de chacun Cet atelier sera proposé le 20 Octobre au DevFest Nantes.\nMerci à Julien pour cet atelier.\nMerci à toutes et tous ! Cette journée a été suivie d\u0026rsquo;une soirée organisée par le CSE où nous avons pu nous retrouver et fêter cela comme il se doit :)\nMerci à toutes et tous pour votre présence !\nRendez-vous fin Octobre pour la 38ème journée Ruche\nSi vous souhaitez présenter un sujet ou si certains sujets vous intéressent, contactez l\u0026rsquo;équipe Ruche\ncrédits photos : vecteezy\n","date":"Sep 28, 2022","href":"https://blog.talanlabs.com/recapitulatif-ruche-37/","kind":"page","labs":null,"tags":["ruche"],"title":"Résumé de la journée Ruche #37"},{"category":null,"content":"Dans son article «Artificial Intelligence as a Positive and Negative Factor in Global Risk» [1], Eliezer Yudkowsky a présenté les avantages et les inconvénients des outils basés sur l’intelligence artificielle (IA). Dans ce contexte, il a mentionné l’exemple de l’armée américaine qui a développé un réseau de neurones pour détecter les chars ennemis camouflés. Le modèle a montré des bonnes performances dans la phase de développement. Cependant, il n\u0026rsquo;a pas fait mieux que le hasard lors de la phase de test. Il s\u0026rsquo;est avéré que dans la base de données d’entraînement, les photos de chars camouflés avaient été prises par temps nuageux, tandis que les photos de forêts ordinaires avaient été prises par temps ensoleillé. Ainsi, ce modèle avait appris à distinguer les jours nuageux des jours ensoleillés, au lieu de distinguer les chars camouflés des forêts vierges.\nHeureusement, ce problème a été détecté rapidement, mais vous pouvez imaginer combien d\u0026rsquo;autres n\u0026rsquo;ont pas été détectés ! De ce fait, un réel besoin a émergé pour ouvrir la boîte noire des modèles d\u0026rsquo;IA et analyser son principe de fonctionnement. D\u0026rsquo;où la naissance du domaine de l\u0026rsquo;intelligence artificielle explicable (XAI). Avant d\u0026rsquo;avancer dans ce concept, il semble important de comprendre le besoin d\u0026rsquo;explication et quand celle-ci est nécessaire.\nBesoins d\u0026rsquo;explication Pour comprendre les besoins d’explication, il est plus pratique de comprendre dans quels cas l\u0026rsquo;explication n\u0026rsquo;est pas nécessaire. En effet, l\u0026rsquo;explication n\u0026rsquo;est pas nécessaire lorsque :\n il n\u0026rsquo;y a pas de conséquences importantes en cas de résultats insuffisants ; le problème est suffisamment bien étudié et validé dans des applications réelles pour avoir confiance dans le système de décision (même s\u0026rsquo;il n\u0026rsquo;est pas parfait).  Ainsi, le besoin d\u0026rsquo;explication ne concerne que les problèmes à fort enjeu et à formalisation incomplète, créant un obstacle fondamental à l\u0026rsquo;optimisation et à l\u0026rsquo;évaluation. Ainsi, de nombreuses techniques ont été récemment proposées pour expliquer les systèmes de décision basés sur l\u0026rsquo;IA de type boîte noire.\nTypes d\u0026rsquo;explications Les techniques d\u0026rsquo;IA explicable peuvent être divisées en deux groupes principaux :\n  Modèles d\u0026rsquo;IA transparents : modèles dont le comportement est explicable (Decision Tree, modèles linéaire, etc.). Ces techniques seront détaillées plus loin dans cet article ;\n  Techniques post-hoc : une sorte d\u0026rsquo;interprète appliqué à des modèles complexes pour expliquer leur comportement. Comme le montre la figure ci-dessous, ces techniques peuvent être divisées en 4 classes principales :\n   Global : donner une explication pour toutes les observations de l\u0026rsquo;ensemble de données ; Local : donner une explication pour juste un groupe d\u0026rsquo;observations ; Spécifique au modèle : s\u0026rsquo;appuier sur la structure du modèle d\u0026rsquo;analyse ; Modèle-agnostique : fonctionner pour tout type de modèles d\u0026rsquo;IA.  Dans ce contexte, les modèles d\u0026rsquo;IA transparents peuvent être considérés comme des techniques d\u0026rsquo;explication globale post-hoc. Cependant, il est important de noter que ces techniques manquent de précision comme tout modèle d\u0026rsquo;explication global. Ainsi, des modèles d\u0026rsquo;explication locaux sont introduits afin de produire des explications fiables même dans un contexte local. Dans cet article, nous nous concentrerons sur cette dualité (local-global). Ainsi, des exemples de techniques d\u0026rsquo;explication locale et globale sont détaillés ci-dessous.\nModèles d\u0026rsquo;explication L\u0026rsquo;explication des systèmes de décision basés sur l\u0026rsquo;IA de type boîte noire attire de plus en plus l\u0026rsquo;attention. Ainsi, plusieurs techniques d’explication sont développées.\nModèles d\u0026rsquo;explication globale (basés sur des techniques d\u0026rsquo;IA transparentes) Comme mentionné ci-dessus, ces techniques consistent à des modèles d\u0026rsquo;IA dont le comportement est explicable. Les techniques les plus utilisées dans ce contexte sont les modèles linéaires et les arbres de décision (Decision Trees). En effet, des arbres de décision (Decision Trees) ont été appliqués pour générer une explication globale du fonctionnement du réseau de neurones. D\u0026rsquo;autres techniques d\u0026rsquo;IA explicables, comme la régression linéaire, sont utilisées de la même manière pour expliquer le principe de fonctionnement de modèles d\u0026rsquo;IA complexes.\n  Les modèles linéaires sont extrêmement transparents et interprétables. Les coefficients de régression nous indiquent directement comment les données affectent la prédiction des modèles (voir la figure ci-dessus (a)), et des tests de signification simples peuvent nous indiquer les variables importantes pour le modèle [2].\n  Decision Trees (DT) est une méthode d\u0026rsquo;apprentissage supervisé non-paramétrique utilisée pour la classification et la régression. L\u0026rsquo;objectif est de créer un modèle qui prédit la valeur d\u0026rsquo;une variable cible en apprenant des règles de décision simples déduites des caractéristiques des données [2]. La méthode DT est basée sur un ensemble de règles de décision de type \u0026ldquo;si-alors\u0026rdquo; pour approximer les fonctions (voir la figure ci-dessus (b)). Plus l\u0026rsquo;arbre est profond, plus les règles de décision sont complexes et plus le modèle est adapté.\n  Toutefois, les explications globales ne sont pas toujours exactes, surtout lorsque le modèle d\u0026rsquo;IA est complexe. Ainsi, des techniques d\u0026rsquo;explication locale sont proposées pour fournir une explication locale du comportement du modèle d\u0026rsquo;IA pour une observation de données.\nModèles d\u0026rsquo;explication locale Ces modèles fournissent une explication de la décision prise pour une seule observation de données. Cette explication consiste à générer des points de données proches de l\u0026rsquo;observation à expliquer. Ensuite, le modèle d\u0026rsquo;IA est appliqué pour prédire la sortie des points de données générés et identifier les variables responsables de la décision prise. Comme mentionné ci-dessus, il existe deux types d\u0026rsquo;approches : les approches spécifiques au modèle et les approches agnostiques. Plusieurs modèles d\u0026rsquo;explication locale sont proposés tels que LIME (Local Interpretable Model-agnostic Explanations) [3]. LIME est la technique la plus connue dans ce domaine et est à l\u0026rsquo;origine de plusieurs autres méthodes comme SHAP [4] et X-PHM [5]. Toutefois, LIME reste la plus utilisée en raison de sa précision et de sa compatibilité avec plusieurs types de données (texte, images et tableaux). Dans ce qui suit, cette méthode est détaillée ainsi que son implémentation sur Python.\nDétails et implémentation de la technique LIME Comme le montre la figure suivante, l\u0026rsquo;intuition derrière LIME consiste à identifier les facteurs qui influencent les résultats d\u0026rsquo;un modèle d\u0026rsquo;IA autour d\u0026rsquo;une observation de données à expliquer (croix noire). Dans cet exemple, il s\u0026rsquo;agit d\u0026rsquo;un problème de classification binaire (deux classes : bleue et rouge). Pour comprendre le comportement local du modèle d\u0026rsquo;IA autour du point d\u0026rsquo;intérêt, un ensemble de données artificielles est généré et leurs classes sont prédites à l\u0026rsquo;aide du modèle d\u0026rsquo;IA. Ensuite, un modèle linéaire simple (indiqué par la ligne pointillée) est utilisé pour construire une approximation locale qui est un \u0026ldquo;explicateur local\u0026rdquo; pour le modèle d’IA.\nComme mentionné ci-dessus, LIME est applicable à différents types de données (texte, images et tableaux). Dans ce qui suit, nous présentons l\u0026rsquo;implémentation de cette technique, en Python, pour les données tabulaires (Le principe de fonctionnement de LIME et son implémentation pour les autres types de données sont presque les mêmes.).\nLe package LIME est sur PyPI et pour l’installer, il suffit d\u0026rsquo;exécuter :\npip install lime Dans ce qui suit, un exemple d\u0026rsquo;application de la technique LIME est illustré. À cette fin, la base de données Titanic [6] est utilisée. Cette application consiste à prédire la survie (ou non) des passagers du navire Titanic en fonction de plusieurs variables. Pour simplifier cette application, seules trois variables (âge, sexe et membre de la famille sur le navire) ont été conservées. Le choix de cette application se justifie par le fait que l\u0026rsquo;objectif est de vérifier la capacité de la méthode LIME à expliquer les prédictions d\u0026rsquo;un modèle d\u0026rsquo;IA de type boîte noire. En effet, le résultat, pour cette application, est assez intuitif puisque les passagers sauvés en priorité sont des femmes et des enfants. Ainsi, le fait d\u0026rsquo;être une femme ou un enfant augmente considérablement les chances de survie d\u0026rsquo;un passager. Les détails de la mise en œuvre de la méthode LIME sont illustrés ci-dessous :\n Import des librairies nécessaires  import numpy as np import matplotlib.pyplot as plt import pandas as pd Chargement de la base de données Titanic avec sklearn et prétraitement des variables  from sklearn.datasets import fetch_openml np.random.seed(42) X, y = fetch_openml(\u0026quot;titanic\u0026quot;, version=1, as_frame=True, return_X_y=True) X.drop(['pclass', 'name', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'], axis=1, inplace=True) X.sex.replace(['female', 'male'], [0, 1], inplace=True) X.age.fillna(X.age.mean(), inplace = True) Séparation des données en ensembles d\u0026rsquo;entraînnement et de test  from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2) Instanciation du modèle de prédiction : un classificateur à perceptron multicouche (MLP)  from sklearn.neural_network import MLPClassifier clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(7, 2), random_state=1) clf.fit(X_train, y_train) Vérification des performances du modèle sur l\u0026rsquo;ensemble de test  print('R2 score for the model on test set =', clf.score(X_test, y_test)) Instanciation du module d\u0026rsquo;explication  import lime.lime_tabular explainer_lime = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X.columns, verbose=True, mode='classification') Explication locale des résultats de prédiction  i = 1 # Indice de l'instance de données à expliquer k = 3 # - Nombre des principales variables exp_lime = explainer_lime.explain_instance(X_test.iloc[i], clf.predict_proba, num_features=k) Visualisation des explications  exp_lime.as_pyplot_figure() La figure ci-dessus montre un exemple d\u0026rsquo;explication locale des résultats de la prédiction pour un enfant, une femme adulte et un homme adulte (de gauche à droite). D\u0026rsquo;après ces résultats, il est clair que la variable \u0026ldquo;sexe\u0026rdquo; est la plus importante dans le processus de décision. Prenons l\u0026rsquo;exemple de la femme, le fait qu\u0026rsquo;elle soit une femme augmente significativement ses chances de survie, malgré le fait qu\u0026rsquo;elle soit adulte et qu\u0026rsquo;elle ait des membres de sa famille sur le bateau. Deuxièmement, la variable \u0026ldquo;âge\u0026rdquo; est très importante dans le cas d\u0026rsquo;un passager de sexe masculin. Cette explication est parfaitement cohérente avec la réalité (les femmes et les enfants sont secourus en priorité), ce qui prouve le grand potentiel de la technique LIME.\nSo what ?! L\u0026rsquo;explication des modèles d\u0026rsquo;IA de type boîte noire semble, à première vue, une représentation simplifiée d\u0026rsquo;un modèle complexe. Malgré la réalité de cette affirmation, elle reste une évocation superficielle du sujet. En effet, le pouvoir d\u0026rsquo;explication d\u0026rsquo;un modèle d\u0026rsquo;IA peut révolutionner la façon dont nous résolvons les problèmes. Nous savons tous que Thomas Edison a effectué près de 1 000 tests avant de trouver le bon filament de bambou carbonisé pour sa lampe. Imaginez qu\u0026rsquo;au lieu de faire cela, il ait développé un modèle d\u0026rsquo;IA pour prédire la fiabilité d\u0026rsquo;un matériau comme filament et qu\u0026rsquo;il ait expliqué ce modèle pour identifier les caractéristiques typiques d\u0026rsquo;un bon filament. Ainsi, le nombre d\u0026rsquo;expériences nécessaires pour résoudre le problème peut être diminué, ce qui a évidemment un impact sur le temps et les ressources nécessaires. Et comme l\u0026rsquo;application des techniques d\u0026rsquo;IA concerne de plus en plus de domaines tels que l\u0026rsquo;industrie, la recherche et la médecine, l\u0026rsquo;IA explicable peut avoir un impact sur de nombreux aspects de notre vie sans perdre le contrôle de ces technologies !\nQ : ok, l\u0026rsquo;IA explicable présente de nombreuses opportunités, donc nous allons l\u0026rsquo;utiliser dans toutes nos applications ?\nR : la réponse est un peu plus complexe, car les techniques d\u0026rsquo;explication existantes ne fournissent qu\u0026rsquo;une représentation simplifiée du modèle original. Ces explications ne représentent donc qu\u0026rsquo;une partie de la réalité, ce qui a un impact sur la qualité des décisions.\nQ : Hmmm, il y a donc une différence de précision entre les méthodes d\u0026rsquo;explication. Alors, quelle technique d\u0026rsquo;explication est la plus précise ? Mais attendez, comment pouvons-nous mesurer la précision de ces techniques ?\nR : les réponses à toutes ces questions seront détaillées dans un autre article \u0026hellip;\nRéférences  Yudkowsky, E. (2008). Artificial intelligence as a positive and negative factor in global risk. Global catastrophic risks, 1(303), 184. scikit-learn.org Ribeiro, M. T., Singh, S., \u0026amp; Guestrin, C. (2016, August). \u0026quot; Why should i trust you?\u0026quot; Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144). Lundberg, S. M., \u0026amp; Lee, S. I. (2017). A unified approach to interpreting model predictions. Advances in neural information processing systems, 30. Omri, N., Al Masry, Z., Mairot, N., Giampiccolo, S., \u0026amp; Zerhouni, N. (2021). X-PHM: Prognostics and health management knowledge-based framework for SME. Procedia CIRP, 104, 1595-1600. Eaton, J. P., \u0026amp; Haas, C. A. (1995). Titanic, triumph and tragedy. WW Norton \u0026amp; Company.  Cover Photo by Kelli McClintock on Unsplash\n","date":"Sep 22, 2022","href":"https://blog.talanlabs.com/vers-une-intelligence-artificielle-interpretable/","kind":"page","labs":null,"tags":["IA","XAI","explication"],"title":"Vers une intelligence artificielle explicable"},{"category":null,"content":"Depuis la dernière décennie et avec l\u0026rsquo;apparition de services tels que Facebook, Twitter ou TikTok, tout le monde parle de réseaux sociaux. Mais, en fait, qu\u0026rsquo;est-ce qu\u0026rsquo;un réseau ? Pourquoi les utilisons-nous ? Dans cet article, nous allons essayer de comprendre la théorie des réseaux et de montrer leur importance et leur utilité dans le monde actuel.\nLa base Un réseau peut être défini comme « un ensemble de points reliés par paires par des lignes » [3].\nBien que cette définition soit très générale, elle montre également à quel point l\u0026rsquo;utilisation des réseaux est vaste - les villes reliées par des routes, les familles de langues, l\u0026rsquo;internet - tous peuvent être considérés comme des types de réseaux. D\u0026rsquo;une manière générale, l\u0026rsquo;analyse de réseau consiste à modéliser des systèmes du monde réel. Maintenant, si je vous demandais de dessiner votre réseau d\u0026rsquo;amis, que dessineriez-vous ? Je soupçonne que, dans la grande majorité des cas, vous dessineriez quelque chose comme ceci :\n Rien qu\u0026rsquo;avec ce simple exemple de réseau, nous pouvons déjà observer les types d\u0026rsquo;informations qui peuvent être extraites du réseau : des informations sur les interactions entre les différentes parties du réseau. Ces modèles d\u0026rsquo;interaction (Dans notre exemple : entre les personnes, mais nous pouvons également imaginer des interactions entre les ordinateurs, les entités de transport ou les protéines.) peuvent avoir une influence différente sur le comportement de l\u0026rsquo;ensemble du réseau. Par conséquent, en modélisant le réseau et les interactions, nous pouvons ensuite déterminer leur structure et leurs propriétés. Aussi, nous pouvons faire des prédictions sur les processus dans le réseau, leur dynamique (par exemple, le mode de diffusion des informations, les parties fragiles du réseau, etc.).\nLe réseau d\u0026rsquo;amis que nous venons de voir peut également être appelé un graphe. Quelle est la différence entre les deux ? Afin de dissiper toute confusion, je vais citer Albert-László Barabási, l\u0026rsquo;un des plus célèbres spécialistes des réseaux [1] :\nIn the scientific literature the terms network and graph are used interchangeably:    Network Science Graph Theory     Network Graph   Node Vertex   Link Edge     Yet, there is a subtle distinction between the two terminologies : the {network, node, link} combination often refers to real systems: The WWW is a network of web documents linked by URLs; society is a network of individuals linked by family, friendship or professional ties; the metabolic network is the sum of all chemical reactions that take place in a cell. In contrast, we use the terms {graph, vertex, edge} when we discuss the mathematical representation of these networks: We talk about the web graph, the social graph (a term made popular by Facebook), or the metabolic graph. Yet, this distinction is rarely made, so these two terminologies are often synonyms of each other.\n Par conséquent, chaque réseau (graphe) est créé à partir d\u0026rsquo;un ou plusieurs nœuds (sommets) qui sont reliés par des liens (bords). Lorsque deux nœuds ont un lien entre eux, on les appelle connectés ou adjacents. Le voisinage d\u0026rsquo;un nœud est constitué de tous les nœuds qui lui sont connectés. Nous verrons plus en détail la terminologie des réseaux dans un instant. Mais d\u0026rsquo;abord, nous allons détailler les différents types de réseaux qui existent.\nLes types des réseaux Auparavant, nous avons déjà énuméré certains des réseaux possibles (amis, villes avec les routes, etc.). Mais la structure du réseau est utilisée dans de nombreuses autres disciplines. En général, nous pouvons distinguer 5 classes de réseaux différentes :\n Réseaux technologiques - ce sont les infrastructures physiques qui sont connectées à toutes sortes de systèmes technologiques. Exemples : les réseaux électriques, l\u0026rsquo;internet, les réseaux téléphoniques, les réseaux de transport. Réseaux biologiques/écologiques - tout type de réseau qui représente l\u0026rsquo;interaction entre des éléments biologiques : réseaux métaboliques, réseaux d\u0026rsquo;interaction entre protéines, réseaux de neurones, réseaux alimentaires, réseaux épidémiques. Réseaux économiques - ce sont toutes les réseaux liés au monde financier, par exemple : réseaux de commerce international, transactions financières. Réseaux sociaux - tout type de réseau où les nœuds sont des personnes et les liens des interactions entre elles : réseau de travail, arbre généalogique, réseau d\u0026rsquo;amis, réseaux d\u0026rsquo;échange de messages. Réseaux culturels - qui traitent des groupes culturels et de leurs interactions, par exemple les familles de langues, les réseaux historiques (par exemple l\u0026rsquo;afflux de personnes entre différents continents), les réseaux sémantiques.  Avec toutes ces exemples et catégories, on peut voir qu\u0026rsquo;on est capable de modéliser des entités qui peuvent être abstraites ou réelles, ou les deux en même temps.\nLa terminologie Dans la section précédente, nous avons déjà vu quelques notions utilisées pour représenter les graphes. Maintenant, nous allons plonger plus profondément dans la terminologie des graphes.\nCaractéristiques des bords Les bords peuvent être orientés ou non orientés : les bords orientés (visualisés par des flèches) peuvent être utiles lorsqu\u0026rsquo;un lien entre les nœuds existe dans une direction mais pas dans l\u0026rsquo;autre - par exemple, dans les routes à sens unique ou le réseau de citation. Dans le cas de nœuds non dirigés, on suppose que le lien est réciproque, par exemple, dans un réseau d\u0026rsquo;amis. En outre, un poids peut être attribué à un bord, ce qui signifie une sorte de coût d\u0026rsquo;utilisation, comme le coût d\u0026rsquo;utilisation d\u0026rsquo;une autoroute. Ce poids peut être représenté par des chiffres au-dessus des bords, mais aussi par l\u0026rsquo;épaisseur de la flèche elle-même (plus elle est épaisse, plus elle a de poids). Dans le cas des graphes simples, entre chaque paire de nœuds, on peut avoir soit un bord non dirigé, soit deux bords dirigés. Il existe également des graphes multiples, dans lesquels nous pouvons avoir plusieurs bords (dirigés ou non dirigés) entre des paires de nœuds, et des auto-boucles - où un bord connecte un nœud à lui-même [4].\n Caractéristiques des nœuds Les nœuds peuvent être caractérisés par le nombre de bords qu\u0026rsquo;ils possèdent. Dans le cas d\u0026rsquo;un réseau dirigé, nous pouvons distinguer deux caractéristiques d\u0026rsquo;un nœud :\n Indegree - le nombre de bords qui quittent le nœud ; Outdegree - le nombre de bords qui y entrent. Pour un réseau non dirigé, nous parlons simplement du degré du nœud. Souvent, la taille du nœud représente l\u0026rsquo;une de ses mesures de degré.   D’ailleurs, les nœuds peuvent également être différenciés par leur classe, qui peut être représentée visuellement par des couleurs ou des formes différentes des nœuds. Les classes (ou les communautés) peuvent représenter des divisions d\u0026rsquo;entités du monde réel, comme les ordinateurs et les routeurs dans le cas d\u0026rsquo;un réseau informatique, ou peuvent être calculées (en utilisant des algorithmes de détection de communautés).\n Caractéristiques au niveau du réseau Centralité\nGrâce à la notion de centralité, nous pouvons effectuer le classement pour comparer les nœuds, en fonction de notre mesure de centralité. Récemment, cette notion a gagné en popularité grâce aux réseaux sociaux qui utilisent parfois la métrique de centralité pour mesurer l'\u0026ldquo;influence\u0026rdquo;. Il existe maintenant plusieurs façons de calculer la métrique qui sera utilisée pour comparer les nœuds. Nous présenterons ici les mesures les plus simples mais toujours populaires [2].\n Centralité de degré (Degree centrality)​ - Le degré du nœud dont nous avons parlé dans le paragraphe précédent, peut aussi parfois être appelé une métrique de la centralité de degré du nœud (la notion peut aussi être étendue à un graphe). Centralité de proximité (Closeness centrality) – Il s\u0026rsquo;agit d\u0026rsquo;une distance moyenne du chemin le plus court entre le nœud et tous les autres nœuds du réseau, c\u0026rsquo;est-à-dire que plus la centralité du nœud est élevée, plus il est proche de tous les autres nœuds. Centralité d\u0026rsquo;intermédiarité (Betweenness centrality) – Définie pour le nœud, c\u0026rsquo;est une mesure qui indique dans quelle mesure le nœud a été le pont du chemin le plus court entre deux autres nœuds.  Comme nous pouvons le constater, chacune des centralités donne des résultats différents et présente donc des perspectives différentes sur le même graphique. Le choix de l\u0026rsquo;une ou l\u0026rsquo;autre dépend strictement du type de réseau et d\u0026rsquo;analyse que l\u0026rsquo;on fait.\n Indépendamment du type concret de centralité que nous prenons, nous pouvons également observer le niveau de centralité du réseau : haute centralité pour les réseaux plutôt centralisés et basse centralité pour les réseaux dans lesquels tous les nœuds sont connectés les uns aux autres.\n Modularité\nIl s\u0026rsquo;agit d\u0026rsquo;une autre mesure de la structure du réseau. La modularité évalue le niveau de division du réseau en différents modules (groupes, communautés). Une haute modularité signifie peu de connexions entre les différents groupes et beaucoup de connexions au sein de chaque groupe. A l\u0026rsquo;inverse, le réseau à basse modularité a en général plus de connexions transversales [5].\n L’utilisation Les termes présentés ci-dessus constituent la base de la terminologie utilisée en analyse de réseau.\nL\u0026rsquo;analyse de réseau utilise et combine de nombreux autres termes issus de la théorie des graphes, de la théorie de l\u0026rsquo;information et de l\u0026rsquo;exploration de données. Étant donné que le réseau est simple mais générique, il peut facilement être utilisé pour modéliser et structurer les données. Cet aspect, combiné à diverses méthodes d\u0026rsquo;analyse de réseau, est la raison de la grande utilité et de la popularité de l\u0026rsquo;analyse de réseau, qui apporte souvent une réponse à de nombreux problèmes du monde réel. Nous pouvons les regrouper en termes de questions de recherche [6]:\nQuels groupes pouvons-nous former ?\nL\u0026rsquo;idée est de trouver des divisions \u0026ldquo;naturelles\u0026rdquo; d\u0026rsquo;un réseau en différentes communautés. Nous utilisons des guillemets ici, car parfois la division pour nous, humains, est assez évidente, par exemple les groupes de directeurs et de managers dans un réseau social professionnel. Mais parfois, la définition d\u0026rsquo;une fonction permettant de diviser le réseau en communautés n\u0026rsquo;est pas du tout évidente. Par conséquent, afin de les détecter, différents algorithmes de détection de communautés sont utilisés. Les communautés peuvent être utilisées dans différents domaines, comme la détection d\u0026rsquo;anomalies et de fraudes, la réduction de grands réseaux en sous-réseaux, la recherche de modèles et de dynamiques cachés dans le réseau, etc.\nQuel est le trajet optimal?\nLa traversée d\u0026rsquo;un graphe est une question importante et très difficile. Les problèmes de recherche d\u0026rsquo;un chemin entre deux nœuds, de détermination du chemin optimal ou de comparaison des coûts de différents chemins sont au cœur de la théorie des graphes. Différents algorithmes de recherche de chemin peuvent donc être utilisés pour planifier le routage des ordinateurs, le meilleur itinéraire de transport, etc.\nQui est le plus … (puissant, important) ?\nTrouver les nœuds importants peut être crucial pour manipuler les réseaux de la manière la plus efficace. Qu\u0026rsquo;il s\u0026rsquo;agisse de déterminer l\u0026rsquo;influence de l\u0026rsquo;utilisateur, de trouver l\u0026rsquo;attaquant qui diffuse des logiciels malveillants ou de découvrir le meilleur film, tout peut être mesuré par l\u0026rsquo;utilisation de méthodes de l’analyse des réseaux telles que les centralités, le PageRank ou le HITS.\nQu\u0026rsquo;y a-t-il d\u0026rsquo;inhabituel ?\nÉtant donné que l\u0026rsquo;analyse de réseau paramètre le réseau et définit sa dynamique, on est en mesure de définir l\u0026rsquo;état \u0026ldquo;normal\u0026rdquo; (par défaut) du réseau. Par conséquent, si quelque chose d\u0026rsquo;inhabituel se produit, cela sera visible dans la structure, les caractéristiques et/ou la dynamique du réseau. L\u0026rsquo;identification des objets anormaux (nœuds, bords) ou la détermination de l\u0026rsquo;exactitude d\u0026rsquo;une structure (sous-graphe, interactions entre nœuds) est une partie importante des techniques de détection des fraudes et des anomalies.\nConclusion Dans cet article, nous avons abordé ce qu\u0026rsquo;est un réseau et pourquoi il est utilisé. Nous avons détaillé la terminologie de base et l\u0026rsquo;utilisation de l\u0026rsquo;analyse de réseau. Nous avons également présenté quelques problèmes que l\u0026rsquo;analyse de réseau peut aider à résoudre.\nEn bref, comme nous pouvons le constater, l\u0026rsquo;analyse de réseau est un outil puissant. Il est déjà incroyablement utile dans de multiples disciplines et dans divers contextes, tels que le commerce (moteurs de recommandation, centres clients), la logistique (chaînes d\u0026rsquo;approvisionnement, routage), les ressources humaines (ciblage des candidats) ou les banques (détection des fraudes), pour n\u0026rsquo;en citer que quelques-uns. Avec l\u0026rsquo;avancée de la technologie, le nombre croissant de dispositifs IoT et l\u0026rsquo;augmentation des volumes de données, l\u0026rsquo;analyse de réseau en proposant des techniques de structuration et de compréhension de l\u0026rsquo;information deviendra encore plus indispensable.\nCover Photo by Alina Grubnyak on Unsplash\nRéférences [1] Barabási, Albert-László. \u0026ldquo;Network science.\u0026rdquo; Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 371.1987 (2013): 20120375. accessed via: http://networksciencebook.com/chapter/2#networks-graphs on 2022-05-23\n[2] Menczer, Filippo, Santo Fortunato, and Clayton A. Davis. A first course in network science. Cambridge University Press, 2020.\n[3] Newman, Mark. Networks. Oxford university press, 2018.\n[4] Zinoviev, Dmitry. Complex network analysis in Python: Recognize-construct-visualize-analyze-interpret. Pragmatic Bookshelf, 2018.\n[5] https://cphss.wustl.edu/methodsandstrategies/social-network-analysis/network-analysis-101/\n[6] \u0026ldquo;5 Graph Data Science Basics Everyone should know\u0026rdquo;, https://go.neo4j.com/rs/710-RRC-335/images/5-Graph-Data-Science-Basics-Everyone-Should-Know.pdf, accessed 31/05/2022,\n","date":"Aug 22, 2022","href":"https://blog.talanlabs.com/analyse-des-reseaux-quand-comment-pourquoi/","kind":"page","labs":null,"tags":["analyse-reseaux","network","graphes"],"title":"Analyse des réseaux: quand, comment, pourquoi ?"},{"category":null,"content":"Une journée Ruche ? Un jeudi par mois, les Labiens se retrouvent pour partager et échanger autour de sujets qu\u0026rsquo;ils choisissent. Technique ou organisationnel, sous forme de présentation ou d\u0026rsquo;atelier, c\u0026rsquo;est l\u0026rsquo;occasion de présenter des sujets qui leur tiennent à coeur ou tout simplement s\u0026rsquo;entraîner en vue d\u0026rsquo;une conférence.\nLa dernière journée Ruche s\u0026rsquo;est déroulée le Jeudi 21 Juillet 2022\nVoici le résumé des différentes sessions proposées en mode \u0026ldquo;hybride\u0026rdquo; (présentiel et distanciel).\nREX Coaching Agile Talan Labs prône l\u0026rsquo;agilité à travers ses missions, mais certaines sont spécifiquement dédiées à évangéliser et accompagner dans la transformation d\u0026rsquo;organisation projet. Après une mise en contexte et une présentation des enjeux de la transformation vers l\u0026rsquo;Agilité, Guillaume nous a donné ses retours d\u0026rsquo;expérience après 8 mois d\u0026rsquo;accompagnement des équipes vers le Scrum.\nMerci à Guillaume pour ce partage.\nL\u0026rsquo;analyse de réseaux Une présentation sur la terminologie et représentation des réseaux ainsi que des différentes métriques pour calculer l\u0026rsquo;influence dans les réseaux. Les réseaux des personnages de Game of Thrones ont été implémentés avec networkx pour illustrer et clore la présentation. Un sujet nouveau et inspirant pour la grande partie de l\u0026rsquo;auditoire.\nMerci à Monika pour cette présentation !\nProperty Base Testing Une explication sur les origines et la pertinence des tests PBT qui utilisent les invariants (ou propriétés) pour créer des tests. Cette approche permet une meilleure exhaustivité et qualité des cas testés comparée à l\u0026rsquo;écriture \u0026ldquo;manuelle\u0026rdquo; des cas. Des exemples s\u0026rsquo;appuyant sur la librairie QuickCheck ont permis d\u0026rsquo;illustrer cette autre façon de créer les tests.\nMerci à Guillaume pour ce sujet !\nLes bonnes pratiques en Infrastructure As Code L\u0026rsquo;utilisation du Cloud est presque systématique dans nos projets. Connaître les bonnes façons de faire dans la conception et création de l\u0026rsquo;infrastructure Cloud est essentiel. A travers des exemples concrets déployés sur Azure Platform, nous avons pu voir les pièges à éviter dans nos projets Terraform.\nMerci à Ajina pour avoir présenté cette session.\nMerci à toutes et tous ! À nouveau une bonne participation grâce à ce mode hybride.\nMerci à toutes et tous pour votre présence !\nRendez-vous à la rentrée de Septembre pour la 37ème journée Ruche\nSi vous souhaitez présenter un sujet ou si certains sujets vous intéressent, contactez l\u0026rsquo;équipe Ruche\ncrédits photos: vecteezy\n","date":"Jul 26, 2022","href":"https://blog.talanlabs.com/recapitulatif-ruche-36/","kind":"page","labs":null,"tags":["ruche"],"title":"Résumé de la journée Ruche #36"},{"category":null,"content":"Parce que créer une startup viable sur le long terme, ce n\u0026rsquo;est pas si simple. Parce que sans la bonne approche, même la meilleure idée peut devenir un échec, un startup studio se présente alors comme une bonne idée pour débuter votre projet sur des bases solides.\nLes startups studios accompagnent les nouvelles startups pendant au moins 6 mois en leur apportant du capital financier et humain. Ils mettent au service de celles-ci toute leur équipe d\u0026rsquo;experts et en échange, le studio prend une participation dans le capital de cette startup. Il prend également tous les frais en charge durant la période de collaboration et une fois que la startup est suffisamment mature, elle quitte le studio en devenant financièrement indépendante.\nMode de fonctionnement Chaque idée entrepreneuriale est soigneusement analysée en fonction des opportunités du marché actuel. Le projet passe ensuite par plusieurs étapes de validation qui suivent la LEAN Methodology, l\u0026rsquo;objectif final étant d\u0026rsquo;ajouter de la valeur à la startup afin d\u0026rsquo;accélérer sa croissance et son entrée sur le marché.\nParmi les startups studios les plus connus, on retrouve :\nRocket Internet  Crée en 2007, Rocket Internet est à l\u0026rsquo;origine du succès de ce phénomène de startups studios. L\u0026rsquo;entreprise s\u0026rsquo;appuie sur le modèle Copy-Build-Sell afin de permettre aux startups de devenir numéro 2 sur le marché. Rocket Internet se cache ainsi derrière plusieurs entreprises à succès comme Zalando, la plateforme de e-commerce allemande spécialisée dans la vente de vêtements et de chaussures. On pourra aussi citer HelloFresh, une société allemande de kits repas cotée en bourse. Ainsi, le défi de ces startups studios est lié à la problématique du recrutement. En effet, dès que l\u0026rsquo;idée du projet est validée par le studio, il faut \u0026ldquo;recruter\u0026rdquo; les entrepreneurs qui vont diriger cette startup. Cela signifie cerner le profil adéquat qui serait prêt à s\u0026rsquo;engager dans cette aventure entrepreneuriale et développer le projet.\nLa Fabrique by CA  La Fabrique by CA s\u0026rsquo;est donnée pour objectif de créer des entreprises à fort potentiel de croissance en s\u0026rsquo;appuyant sur les compétences et la force de frappe du Groupe Crédit Agricole pour amener le futur de la banque dans la vie de tous. Parmi les pépites issues du startup studio, on pourra citer la néo-banque Blank qui opère sur le segment des professionnels indépendants en proposant à la fois des services bancaires mais également des services de gestion administrative et financière.\nAdVentures  Crée en 2010, AdVentures s\u0026rsquo;est spécialisé dans les startups liées au numérique, à l\u0026rsquo;IA et à la génétique. Il met en œuvre la méthode du Rubik\u0026rsquo;s cube qui consiste à explorer les meilleures voies de développement avec un système de feedbacks et de changement de paramètres récurrents.\nLe saviez-vous ?  Des acteurs comme La Banque Publique d\u0026rsquo;Investissement collaborent également avec des startups studios à travers des appels à manifestation d\u0026rsquo;intérêt pour financer des secteurs comme le numérique, l\u0026rsquo;aéronautique ou encore la santé.\nCover Photo by Marvin Meyer on Unsplash\n","date":"Jul 21, 2022","href":"https://blog.talanlabs.com/startup-studio/","kind":"page","labs":null,"tags":["Innovation","Start-up"],"title":"Les startups studios"},{"category":null,"content":"Le 15,16,17 et 18 juin 2022, nos équipes du SUN se sont rendues à VivaTech: le rendez-vous mondial des startups et des leaders pour célébrer l\u0026rsquo;innovation.\n Venez découvrir avec nous les expositions qui nous ont le plus marquées :\nLa startup Blue Frog Robotic  La startup Blue Frog Robotics était l\u0026rsquo;invitée de La Fabrique by CA. Elle développe à travers l\u0026rsquo;intelligence artificielle des robots entièrement conçus et fabriqués en France, pour différents secteurs comme l\u0026rsquo;éducation ou encore la santé. Cette année, elle a vendu 1500 robots à l\u0026rsquo;Education Nationale pour venir en aide aux enfants déscolarisés dans des chambres d\u0026rsquo;hôpitaux et pour favoriser leur inclusion. La spécificité de ce robot est qu\u0026rsquo;il est personnalisable, doté d\u0026rsquo;une caméra, il peut également lever le doigt en classe, se déplacer et même parler pour permettre aux enfants de suivre le cours comme s\u0026rsquo;ils étaient présents en classe.\nLa plateforme Binance \rBinance, la 1ère plateforme mondiale pour l\u0026rsquo;achat et la vente de cryptomonnaies, a fait du métavers et du Web 3.0 les axes majeurs de son stand. En effet, elle a collaboré avec la marque de luxe Jimmy Choo pour développer une édition limitée de NFTs ou encore avec des agences évènementielles pour transformer les billets de concert en NFT, pour attester de leur caractère personnel en les sauvegardant sur la blockchain.\nLes lunettes de soleil Meta \rDes lunettes de soleil pour faire des vidéos, prendre des photos ou encore écouter de la musique ? C\u0026rsquo;est désormais possible avec le nouveau partenariat Meta x Ray Ban, disponible au grand public sur le site de Meta. Anciennement appelé Facebook, Meta est parmi les géants qui dominent le marché du numérique en contribuant aujourd\u0026rsquo;hui à construire un monde où chacun peut jouer et se connecter au métavers.\nComment ça marche ? Il s\u0026rsquo;agit de lunettes connectées dotées de deux capteurs de 5 mégapixels présents au niveau des branches. Une touche permet de prendre des photos grâce à un simple appui. Trois microphones sont également intégrés aux branches pour passer des appels et écouter de la musique.\nLes livraisons HDRONES \rNous sommes partis à la rencontre de HDRONES, l\u0026rsquo;invité du laboratoire d\u0026rsquo;innovation de KPMG, qui développe aujourd\u0026rsquo;hui des drones pour faciliter la livraison de médicaments en dehors des grandes métropoles grâce à une application mobile. En effet, le pharmacien reçoit une notification avec l\u0026rsquo;ordonnance du patient et dépose les médicaments dans un hub précis. Par la suite, le drone livre le colis en le déposant dans le hub le plus proche du patient.\nDes NFT pour les abeilles \rGuerlain a également proposé un stand qui présente son récent projet de réensauvagement de la Vallée De La Millière, véritable havre de la biodiversité situé dans le département des Yvelines.\nComment ? En mettant en vente une collection unique de 1828 NFT, appelée CryptoBee, pour aider la nature à reprendre ses droits. En effet, 100% des ventes sont investies dans la renaturalisation de la réserve et les propriétaires de ces NFT pourront bénéficier d\u0026rsquo;un accès exclusif au domaine et d\u0026rsquo;invitations pour les évènements Guerlain.\nPourquoi l\u0026rsquo;abeille comme symbole ? L\u0026rsquo;abeille est depuis 1858 l\u0026rsquo;emblème de la Maison Guerlain. Elle était présente dès les premiers flacons de parfum et même aujourd\u0026rsquo;hui sur la célèbre gamme de soin Abeille Royale pour marquer le souci environnemental de la Maison et son engagement pour la protection de ses polinisateurs.\n","date":"Jul 20, 2022","href":"https://blog.talanlabs.com/viva-tech-2022/","kind":"page","labs":null,"tags":["Innovation","Start-up"],"title":"Retour sur l'évènement VivaTech 2022"},{"category":null,"content":"Pour calculer la température du corps humain, il existe plusieurs types des thermomètres : les thermomètres au Gallium, les thermomètres électroniques ou numériques, les thermomètres infrarouges et les caméras thermiques.\nSuite à la pandémie du Covid19, les thermomètres infrarouges et les caméras thermiques sont devenus les plus utilisés, notamment dans les lieux publics (supermarchés, gares, centres commerciaux, hôpitaux\u0026hellip;) pour le contrôle de la température du corps humain puisqu’ils sont des outils sans contact.\nNous nous intéressons dans ce qui suit sur les caméras thermiques et l’imagerie thermique en général et comment elle est utilisée pour le calcul de la température du corps humain par des techniques de vision par ordinateur.\nL’imagerie thermique L\u0026rsquo;imagerie thermique consiste à transformer les mesures du rayonnement infrarouge en une image radiométrique [1]. Le rayonnement infrarouge émis par un objet correspond à la chaleur émise par ce dernier.\nLes caméras thermiques enregistrent les variations de la température des objets visés, puis attribuent à chaque température une nuance de couleur, ce qui vous permet de voir la quantité de chaleur dégagée par rapport aux objets de son environnement proche.\nLes températures plus froides sont souvent associées à une teinte de bleu, de violet ou de vert, tandis que les températures plus chaudes peuvent être associées à une teinte de rouge, d’orange ou de jaune.\n (Source : https://www.flir.fr)\nApplications de l’imagerie thermique L\u0026rsquo;imagerie thermique est aujourd\u0026rsquo;hui utilisée à de nombreuses fins :\n  Bâtiments et énergies : L’imagerie thermique est un moyen puissant et non invasif pour surveiller et diagnostiquer l’état d’un bâtiment et détecter les problèmes très tôt. Elle permet, entre autres de visualiser les déperditions d’énergie, de détecter les défauts d’isolation, de trouver les défauts dans les canalisations d’alimentation et le réseau de chauffage urbain ou d’identifier les problèmes électriques\n  Sécurité et surveillance : Les caméras thermiques sont un excellent outil pour détecter les intrus ou les potentielles menaces grâce au contraste thermique. Elles peuvent filmer dans l\u0026rsquo;obscurité totale. Elles permettent d’obtenir une excellente image de jour comme de nuit, et dans toutes les conditions atmosphériques.\n  Météo : Les stations météorologiques l’utilisent pour anticiper et suivre les tempêtes et les ouragans à travers leur signature thermique mais aussi pour le suivi des sécheresses et de la désertification.\n  Médecine : Utilisée pour diagnostiquer divers troubles et pathologies (détection précoce de cancer de sein [2], Analyse régulière de la glycémie pour les personnes diabétiques, Surveillance quotidienne des personnes à risques respiratoires\u0026hellip;)\n  Maintenance préventive : L’imagerie thermique permet de quantifier et mesurer la propagation de la chaleur des machines ou des cartes électroniques, donc prévoir la surchauffe de composants.\n  Comment fonctionne une caméra thermique ? Une caméra thermique enregistre l\u0026rsquo;intensité du rayonnement dans la partie infrarouge du spectre électromagnétique et la convertit en image visible.\nL\u0026rsquo;existence de l\u0026rsquo;infrarouge est découverte en 1800 par l\u0026rsquo;astronome Frederick William Herschel. Voulant savoir si la lumière produit des températures différentes selon sa couleur, il utilise un prisme pour diviser un rayon de soleil. Il mesure la température de chaque couleur du spectre, et constate que les températures augmentent du violet au rouge.\nLa lumière infrarouge est invisible à l\u0026rsquo;œil nu : son spectre se situe entre celui de la lumière visible et des micro-ondes. Cependant, elle peut etre associée à une sensation de chaleur lorsque son intensité est suffisamment élevée. Le spectre suivant présente les différents rayonnements électromagnétiques par fréquence et longueur d\u0026rsquo;onde dans le vide ou énergie photonique.\n Tous les objets émettent un rayonnement infrarouge qui est une conséquence de leurs températures et c’est l’un des moyens par lesquels la chaleur est transmise.\nPlus un objet est chaud, plus il produit un rayonnement infrarouge intense. Les caméras thermiques peuvent capter ce rayonnement et le convertir en une image que nous pouvons ensuite visualiser sur l’écran de l’appareil, un peu comme une caméra de vision nocturne peut capturer la lumière infrarouge invisible et la convertir pour que nous puissions la visualiser.\nApplication : Comment calculer la température du corps à partir d’une image thermique ? Dans cet article, nous présentons un exemple simple d’application du Computer Vision avec l’imagerie thermique pour déterminer la température du corps à travers l’analyse d’images thermiques. Ce cas d’usage est tiré d’un de nos projets actuels dans le domaine de la santé où nous utilisons l’IA et l’imagerie thermique dans le contexte de la Neuro-réanimation.\nIl a été prouvé que la température du coin interne de l\u0026rsquo;œil est la plus proche de la température centrale du corps, qui est de 37 degrés Celsius [3] [4]. Comparée à la peau, cette région absorbe et émet moins de rayonnement. Afin de mesurer la temperature emise par le coin interne de l\u0026rsquo;œil, il est nécessaire que la distance entre la caméra et la personne soit minimale, sinon la lecture ne sera pas précise.\nCaméra thermique FLIR Lepton 3,5 : Pour cette application une caméra thermique Lepton de FLIR 3,5 est utilisée pour faire la capture d’une vidéo thermique et afficher la température en temps réel.\n(Fiche technique : https://flir.netx.net/file/asset/22582/original/attachment )\n Pourquoi une caméra FLIR ? La caméra FLIR Lepton® est une solution de caméra LWIR OEM (longwave infrared) à capacité radiométrique. Elle s’intègre facilement dans des appareils mobiles ou autres équipements électroniques comme un capteur IR ou une caméra thermique. Ses capacités radiométriques lui permettent de capturer sans contact des données thermiques précises et calibrées, pour chaque pixel de l\u0026rsquo;image capturée [5].\nCarte Nvida Jetson Nano Developper kit + Batterie 5V : Pour traiter la vidéo capturée, nous utilisons une carte Nvidia Jetson Nano :\n (Source : https://developer.nvidia.com/embedded/jetson-nano-developer-kit)\nPourquoi une carte Nvidia ? Notre choix s’est porté sur la carte Nvidia Jetson Nano pour la puissance du traitement qu’elle offre dans le contexte de notre projet en Neuro-réanimation\nA noter que Nvidia propose plusieurs produits de type mini-ordinateurs puissants dédiés à des applications d’intelligence artificielle embarquées, robotique, ou IoT ( Jetson Nano, Jetson Xavier NX, Jetson AGX Xavier, Jetson AGX Orin [6]).\nLibrairie utilisée : OpenCV Pour la capture et le traitement de chaque image de la vidéo, nous utilisons la bibliothèque open-source OpenCV de python. OpenCV est une bibliothèque graphique libre, initialement développée par Intel, spécialisée dans le traitement d\u0026rsquo;images en temps réel [7].\nAssemblage Après avoir fait le choix du matériel, nous passons à son assemblage. La caméra FLIR de Lepton est connectée à la carte Nvidia Jetson nano developer kit via un port USB.\nIl existe plusieurs façons d’alimenter la carte, soit par un connecteur coaxial 2.1mm 5V 4A soit par un port micro USB 5V 2A. Dans notre cas, nous avons choisi d’alimenter la carte avec une batterie qui fournit une tension 5V en utilisant le connecteur coaxial.\n Prise en main avec la Nvidia Jetson nano developper kit La carte Nvidia Jetson Nano utilise une carte microSD comme périphérique de démarrage et de stockage principal. Il est important d\u0026rsquo;avoir une carte qui soit rapide et avec suffisamment de mémoire pour vos projets. La première étape de lancement de la carte Nvidia consiste en la préparation du système d’exploitation : on télécharge l\u0026rsquo;image de la carte SD du Jetson Nano Developer Kit, puis on grave cette image sur la carte SD [6].\nUne fois que la carte microSD est prête, nous passons à la configuration du kit de développement. Il existe deux façons d’interagir avec la carte, soit en utilisant un écran, une souris et un clavier ou bien en utilisant le mode “headless mode” via un autre ordinateur. Au premier démarrage, on passe par des étapes d’acceptation de conditions d’utilisation, sélection de langage, création d’utilisateurs et du mot de passe, etc [6].\nla carte est prête enfin pour l’utilisation. Nvidia fournit quelques exemples de base préinstallés sur la carte pour tester des algorithmes de Deep Learning.\nPassons à la préparation de l’environnement pour exécuter notre exemple : Pour réaliser cette application nous avons besoin d’installer les versions de python et de OpenCV nécessaires, dans notre cas nous avons utilisé les versions 3.6.9 de python et 4.1.0 de OpenCV.\nApplication L’étape suivante consiste à developper le code de l’application sur un éditeur de texte en donnant l’extension .py à ce fichier pour qu’il sera traité comme un code python. Ensuite, l’exécution de ce code se fait avec un terminal (invite de commande) en utilisant la commande : python3 nom_fichier.py\n Selon ce qui précède, nous calculons la température du coin interne de l’œil qui est identifiée comme la zone la plus chaude du visage. Donc si nous prenons une image thermique du visage ou bien précisément de l’œil, la zone la plus chaude sera ce coin interne de l’œil. Notre algorithme permet d’identifier le pixel avec la plus grande valeur qui correspond à la zone la plus chaude. Ci-dessous, l’algorithme utilisé est bien détaillé avec le résultat obtenu :\nAlgorithme de traitement :  Enregistrer une vidéo thermique de l’œil et lire la vidéo frame par frame pour faire le traitement :  video = cv2.VideoCapture(video_source) ret=True while ret: ret, frame = video.read()  Transformer chaque image du vidéo au niveau gris :  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  Appliquer un filtre gaussien (pour diminuer le bruit) :  gray = cv2.GaussianBlur(gray, (3,3), 0)  Chercher dans chaque image la valeur max et min des pixels ainsi que leurs positions avec la fonction minMaxLoc de OpenCV :  (minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(gray)  Sauvegarder la valeur Max dans une liste :  Temp.append((maxVal*38/255)) # Temp est une liste où on stocke la température max dans chaque image   Afficher la moyenne des températures Max dans toutes les images de la vidéo capturée :  print(sum(Temp)/len(Temp)) La figure ci-dessous montre un exemple de l\u0026rsquo;emplacement de la zone la plus chaude dans une image avec la valeur de la température moyenne calculée :\n Remarque : Dans le cas où nous augmentons la distance entre la caméra et la personne (capture de l\u0026rsquo;ensemble du visage), la précision des résultats se dégrade comme le montre le résultat suivant :\n Conclusion Dans cet article, nous avons présenté un exemple simple de prise en main de la carte Nvidia Jetson Nano avec l’utilisation de la bibliothèque OpenCV. Dans les prochains articles, nous allons plus loin dans l’utilisation de la Jetson Nano en invoquant son architecture pour passer à des applications de Deep Learning.\nRéférences:   [1] Richard Taillet, Loïc Villain et Pascal Febvre, Dictionnaire de physique, Bruxelles, De Boeck, 2013\n  [2] Mambou, S. J., Maresova, P., Krejcar, O., Selamat, A., \u0026amp; Kuca, K. (2018). Breast Cancer Detection Using Infrared Thermal Imaging and a Deep Learning Model. Sensors (Basel, Switzerland), 18(9), 2799.\n  [3] Ferrari, Claudio et al. “Inner Eye Canthus Localization for Human Body Temperature Screening.” 2020 25th International Conference on Pattern Recognition (ICPR) (2021) 0: 8833-8840.\n  [4] Yangling Zhou, Pejman Ghassemi, Michelle Chen, David McBride, Jon P. Casamento, T. Joshua Pfefer, Quanzeng Wang, \u0026ldquo;Clinical evaluation of fever-screening thermography: impact of consensus guidelines and facial measurement location,\u0026rdquo; J. Biomed. Opt. 25(9) 097002 (12 September 2020)\n  [5] https://www.flir.fr/\n  [6] https://developer.nvidia.com/embedded/jetson-developer-kits\n  [7] https://opencv.org/\n  Cover Photo by Ion Fet on Unsplash\n","date":"Jul 7, 2022","href":"https://blog.talanlabs.com/computer-vision-et-imagerie-thermique/","kind":"page","labs":null,"tags":["IoT","Imagerie thermique","Computer vision","Nvidia Jetson Nano"],"title":"Computer vision et imagerie thermique : Comment calculer la température du corps ?  "},{"category":null,"content":"Une journée Ruche ? Un jeudi par mois, les Labiens se retrouvent pour partager et échanger autour de sujets qu\u0026rsquo;ils choisissent. Technique ou organisationnel, sous forme de présentation ou d\u0026rsquo;atelier, c\u0026rsquo;est l\u0026rsquo;occasion de présenter des sujets qui leur tiennent à coeur ou tout simplement s\u0026rsquo;entraîner en vue d\u0026rsquo;une conférence.\nLa dernière journée Ruche s\u0026rsquo;est déroulée le Jeudi 23 Juin 2022\nVoici le résumé des différentes sessions proposées en mode \u0026ldquo;hybride\u0026rdquo; (présentiel et distanciel).\nAccessibilité Web Après une présentation des différents types de handicap, il nous a été exposé des moyens techniques et des outils de diagnostic pour répondre aux obligations liées à l\u0026rsquo;accessibilité aux sites Web (RGAA). Un sujet qui concerne toute équipe produit, l\u0026rsquo;accessibilité numérique étant un droit pour tous et qui peut toucher chacun un jour.\nMerci à Cécile, Guillaume et François pour ce partage.\nL\u0026rsquo;IA Explicable (XAI) A partir de l\u0026rsquo;exemple d\u0026rsquo;une application de détection de chars camouflés, il a été expliqué pourquoi un modèle d\u0026rsquo;intelligence artificielle peut montrer de bonnes performances dans la phase de développement, mais ne pas faire mieux que le hasard lors de la phase de tests.\nMerci à Nabil pour cette présentation qui a malheureusement été écourtée suite à des problèmes de connexion. A refaire !\nLa perfect Team existe : comment y arriver ? Une équipe Scrum est venue partager ses astuces, outils et expériences pour créer une bonne cohésion au sein d\u0026rsquo;une équipe, malgré la diversité des profils et personnalités. De quoi nous rappeler que le succès ne repose pas que sur les compétences techniques, mais aussi sur l\u0026rsquo;harmonie humaine et organisationnelle.\nMerci à Natia, Anouchka, Damien et Aubin pour ce retour d\u0026rsquo;expérience !\nMob programming pour une application interne La ruche est aussi l\u0026rsquo;occasion de faire des sessions de programmation en groupe. Objectif de cette après-midi : implémenter la fonction \u0026ldquo;ajouter des groupes utilisateurs\u0026rdquo; dans un outil interne de réservation de place de bureau. A tour de rôle, une personne code 10/15 minutes, en partageant son écran aux autres et en expliquant ses idées et sa démarche, ou en demandant des conseils. Un moyen convivial pour que chacun s\u0026rsquo;approprie le projet tout en faisant évoluer le produit.\nMerci à Nicolas pour avoir préparé et animé cette session.\nMerci à toutes et tous ! À nouveau une bonne participation grâce à ce mode hybride.\nMerci à toutes et tous pour votre présence !\nRendez-vous le Jeudi 21 Juillet pour la 36ème journée Ruche\nSi vous souhaitez présenter un sujet ou si certains sujets vous intéressent, contactez l\u0026rsquo;équipe Ruche\ncrédits photos: vecteezy\n","date":"Jul 5, 2022","href":"https://blog.talanlabs.com/recapitulatif-ruche-35/","kind":"page","labs":null,"tags":["ruche"],"title":"Résumé de la journée Ruche #35"},{"category":null,"content":"Développer une librairie plutôt qu\u0026rsquo;une application web plus classique amène de nouvelles problématiques. Parmi elles se trouve la question de comment versionner cette librairie. C\u0026rsquo;est sur la base de cette problématique ainsi que d\u0026rsquo;un retour d\u0026rsquo;expérience que nous allons tenter d\u0026rsquo;apporter des éléments de réponse.\nLe projet sur lequel nous allons nous appuyer dans cet article consiste en la création d\u0026rsquo;un Design System pour un client.\nL\u0026rsquo;objectif de ce Design System est d\u0026rsquo;homogénéiser l\u0026rsquo;ensemble de leurs applications en leur apportant une identité graphique. Cela passe donc par l\u0026rsquo;uniformisation des composants et des styles graphiques pour chacune des applications utilisées par le client. Le tout en apportant un réel gain de temps dans la production de nouvelles maquettes de design et dans le développement front.\nPour cela, nous avons développé une librairie de web components. L\u0026rsquo;objectif de ces composants étant qu\u0026rsquo;ils puissent être utilisés par n\u0026rsquo;importe quel framework.\nNous allons donc présenter dans cet article la manière dont nous avons appréhendé la gestion des versions de notre librairie. Que ce soient les points d\u0026rsquo;attention à garder à l\u0026rsquo;esprit, les erreurs commises ainsi que les solutions trouvées.\nLa gestion sémantique de version (semver) Nous nous sommes basés sur semver pour réaliser nos montées de versions. Voyons rapidement les grands principes :\nPartons d\u0026rsquo;une version A.B.C, avec :\n A : numéro de version majeur B : numéro de version mineur C : numéro de version de correctif (plus communément appelé \u0026ldquo;patch\u0026rdquo;)  Une montée de version majeure implique un ou plusieurs Breaking changes, c\u0026rsquo;est-à-dire des changements dans la librairie qui impliquent aux personnes qui l\u0026rsquo;utilisent de devoir effectuer certaines modifications dans leur utilisation. Sans quoi certains services de la librairie, dans notre cas des composants web, par exemple, risquent de ne plus être fonctionnels. Une montée de version majeur n\u0026rsquo;est donc pas rétrocompatible.\nUne montée de version mineure implique l\u0026rsquo;ajout d\u0026rsquo;une fonctionnalité qui ne provoque pas de Breaking change, qui est donc rétrocompatible. Par exemple, on peut imaginer l\u0026rsquo;ajout d\u0026rsquo;un nouveau composant, ou bien l\u0026rsquo;ajout d\u0026rsquo;une fonctionnalité sur un des composants existants.\nUne montée de version de correctif (patch) implique la plupart du temps la correction d\u0026rsquo;un bug.\nComment et quand monter de version ? Le développement d\u0026rsquo;une librairie implique de devoir définir comment et quand monter de version. Comme vu dans le paragraphe précédent, la montée de version se fait selon le contenu de cette dernière et débouchera sur une montée de version de correctif, mineure ou majeure.\nIl est important d\u0026rsquo;en livrer régulièrement pour faire vivre la librairie et assurer son développement et son adoption par les utilisateurs. Dans notre cas, il va s\u0026rsquo;agir principalement de livrer de nouveaux composants. Cependant, il arrive que certains composants soient plus longs à développer que d\u0026rsquo;autres, impliquant un plus long délai pour sortir une nouvelle version mineure. Des versions de correctif peuvent donc être livrées entre-temps pour assurer la régularité des livraisons de nouvelles versions. Et au-delà de cette notion de régularité, les versions de correctif assurent la robustesse de la librairie en corrigeant les problèmes. Elles sont donc primordiales, en plus d\u0026rsquo;assurer un contact régulier avec les utilisateurs, souvent à l\u0026rsquo;origine des remontées d\u0026rsquo;anomalies.\nUne fois la montée de version effectuée, il est nécessaire que les utilisateurs de la librairie aient accès à une page leur expliquant les modifications qu\u0026rsquo;elle apporte. Plusieurs moyens peuvent être utilisés.\nNous avons décidé, pour ce projet, d\u0026rsquo;utiliser Storybook pour présenter notre librairie de composants à nos utilisateurs. Une page Release Notes dans l\u0026rsquo;arborescence du Storybook peut alors être mise-à-jour lors de la montée de version afin d\u0026rsquo;informer des changements apportés à chaque version. On pourra y trouver le numéro de la version, sa date de livraison ainsi qu\u0026rsquo;une liste des nouveautés. En plus de cette page, il peut être opportun de notifier les utilisateurs d\u0026rsquo;une nouvelle version, par le biais d\u0026rsquo;envois de mails, par un réseau social ou autre.\nUne de nos erreurs a été dans un premier temps de monter de version à chaque changement sur la branche master, ce qui provoquait beaucoup de montées de version. Il est en fait préférable de grouper plusieurs changements dans une seule livraison. Un moyen d\u0026rsquo;y parvenir peut être de déclencher la montée de version non plus sur un changement de la branche master, mais sur celui d\u0026rsquo;une autre branche, release par exemple. Ainsi, la montée s\u0026rsquo;effectuera uniquement lorsque l\u0026rsquo;on décidera de fusionner (merge) la branche master, qui contient tous les changements, sur la branche release. Pour ne pas inclure certains changements dans la prochaine montée de version, il faudra donc attendre avant de les fusionner sur la master.\nEt dans le cas de breaking changes ? Une montée de version majeure, impliquant en général des breaking changes, ne peut pas se faire de la même manière que les versions mineures et patch. En effet, une montée de version majeure doit se préparer et ne doit pas avoir lieu trop souvent. Cependant, des évolutions impliquant des breaking changes arrivent régulièrement, le plus souvent dûs à des choix antérieurs qui n\u0026rsquo;étaient pas les bons. Cela arrivera dans tous les cas mais ce n\u0026rsquo;est pas forcément grave.\nEn attendant la version majeure, une solution est de passer, lorsque que cela est possible, par ce qu\u0026rsquo;on appelle du deprecated, c\u0026rsquo;est-à-dire une solution de transition où plusieurs manières de faire peuvent coexister. Cela en précisant bien aux utilisateurs la nouvelle manière de faire à l\u0026rsquo;aide de warnings dans la console du navigateur par exemple, en plus d\u0026rsquo;une mise-à-jour de la documentation de la librairie.\nPrenons l\u0026rsquo;exemple d\u0026rsquo;un renommage d\u0026rsquo;attribut d\u0026rsquo;un composant :\nPrenons un composant \u0026lt;my-button\u0026gt;, pour lequel on avait défini un attribut boolean isDisabled. Ce nommage est correct dans la plupart des cas pour un attribut boolean (par exemple lors d\u0026rsquo;une application classique dans un framework JS tel que React, Vue, Angular), mais ne répond pas aux standards HTML quant au nommage d\u0026rsquo;attribut de composants Web, où l\u0026rsquo;on privilégiera des attributs nommés le plus simplement tels que disabled.\nPour aller plus loin, selon les standards HTML, un attribut boolean a la valeur false par défaut, ce qui implique que la présence ou non de l\u0026rsquo;attribut sur la balise HTML du composant lui attribue sa valeur.\nComposant \u0026lt;my-button\u0026gt; non disabled (absence de l\u0026rsquo;attribut disabled impliquant une valeur égale à false) :\n\u0026lt;my-button\u0026gt;\u0026lt;/my-button\u0026gt; Composant \u0026lt;my-button\u0026gt; disabled (présence de l\u0026rsquo;attribut disabled impliquant une valeur égale à true) :\n\u0026lt;my-button disabled\u0026gt;\u0026lt;/my-button\u0026gt;  Note : les standards relatifs aux attributs des balises HTML recommandent une écriture en snake-case (en reprenant l\u0026rsquo;exemple : is-disabled), et pas camelCase (isDisabled). Le camelCase pourra par ailleurs provoquer des problèmes de compatiblité avec certains frameworks.\n Revenons-en maintenant à ce que ce renommage implique. Si nous décidions de renommer purement et simplement l\u0026rsquo;attribut dans le composant et que nous effectuions la montée de version dans la foulée, cela représenterait un breaking change car si des utilisateurs utilisaient un composant \u0026lt;my-button\u0026gt; avec l\u0026rsquo;attribut isDisabled et non pas disabled, le composant cesserait de fonctionner correctement et cela demanderait forcément une mise-à-jour de chaque composant \u0026lt;my-button\u0026gt; dans leur application.\nPour éviter un changement soudain et trop brutal, nous pouvons aussi décider de mettre l\u0026rsquo;attribut isDisabled en deprecated dans la documentation et dans les warnings tout en le gardant dans le composant. Ce dernier devra alors pouvoir supporter les 2 attributs isDisabled et disabled en même temps. Ainsi, les utilisateurs de la librairie auront le temps de se mettre à jour quant au nouvel attribut durant cette période de transition mais n\u0026rsquo;y seront pas contraints jusqu\u0026rsquo;à l\u0026rsquo;arrêt du support de l\u0026rsquo;ancien attribut, dans une prochaine version majeure à définir.\nComme vous l\u0026rsquo;aurez compris, la gestion des breaking changes dans le développement d\u0026rsquo;une librairie est assez délicate et demande du temps. C\u0026rsquo;est pourquoi il est préférable de grouper plusieurs breaking changes dans une même version majeure, de manière à ne pas avoir à monter le numéro majeur de la version trop souvent.\nEt Git dans tout ça ? Comme évoqué plus haut, nos livraisons de nouvelles versions en production sont effectuées par notre CI/CD lors de changements sur la branche release, de manière à pouvoir regrouper plusieurs sujets sur une même montée de version.\nDe plus, juste avant de fusionner la branche master sur la branche release, nous tirons une branche depuis la master afin de monter la version (npm version minor par exemple) et de mettre-à-jour la release note avec les modifications apportées.\nVoici un schéma récapitulatif :\nLes erreurs que nous avons faites Nous avons commis plusieurs erreurs lors du développement de la librairie. Et comme évoqué précédemment, certaines ont pu impliquer une montée de version majeure en passant ou non par du deprecated. Le mauvais nommage d\u0026rsquo;attributs de certains de nos composants ou le fait d\u0026rsquo;avoir livré de nouvelles versions à chaque changement sur la branche master en font partie.\nBilan Pour récapituler, nous nous sommes basés sur semver pour versionner notre librairie de composants, oscillant entre montées de versions majeures, mineures et de correctif. L\u0026rsquo;un des points d\u0026rsquo;attention du semver se trouve dans l\u0026rsquo;utilisation des numéros majeurs de version, impliquant dans de nombreux cas des Breaking changes. Ces derniers demandent une attention particulière des développeurs dans leur gestion, en décidant de passer ou non par du deprecated. Dans le cas contraire, il faudra bien faire attention à ne pas changer de version majeure trop souvent, de peur d\u0026rsquo;être une contrainte trop grande pour les consommateurs de la librairie.\nUne librairie, comme toute autre application web, a besoin d\u0026rsquo;un environnement adapté. La CI/CD doit être en mesure de livrer en production de nouvelles versions, contenant un ensemble maîtrisé de fonctionnalités et de correctifs. Pour cela, nous utilisons une branche dédiée à la livraison des nouvelles versions, nommée release.\nPour terminer, il faut garder à l\u0026rsquo;esprit que faire au plus simple pour les utilisateurs de la librairie reste l\u0026rsquo;un des points les plus cruciaux, et que c\u0026rsquo;est autour de ce principe que les solutions doivent se baser.\n","date":"Jun 24, 2022","href":"https://blog.talanlabs.com/2022-06-24-version-a-library/","kind":"page","labs":null,"tags":["design system","library","versioning"],"title":"Versionner une librairie - Design System"},{"category":null,"content":"Une journée Ruche ? Un jeudi par mois, les Labiens se retrouvent pour partager et échanger autour de sujets qu\u0026rsquo;ils choisissent. Technique ou organisationnel, sous forme de présentation ou d\u0026rsquo;atelier, c\u0026rsquo;est l\u0026rsquo;occasion de présenter des sujets qui leur tiennent à coeur ou tout simplement s\u0026rsquo;entraîner en vue d\u0026rsquo;une conférence.\nLa dernière journée Ruche s\u0026rsquo;est déroulée le 19 Mai 2022\nVoici le résumé des différentes sessions proposées en mode \u0026ldquo;hybride\u0026rdquo; (présentiel et distanciel).\nTatouage des algorithmes d\u0026rsquo;Intelligence Artificielle Le tatouage numérique est une technique consistant à insérer un message, appelé marque, dans un document numérique (image, base de données, modèle d\u0026rsquo;IA etc.) pour assurer la protection des droits d\u0026rsquo;auteur.\nAprès une explication des enjeux et des concepts du tatouage numérique, l\u0026rsquo;objectif a été de montrer les techniques utilisées pour appliquer un tatouage sur les modèles de Deep Learning.\nMerci à Iheb pour ce partage, son article sur le même sujet est disponible ici.\nLe projet Neuromonitor Il y a 2 ans, le projet Neuromonitor est né d\u0026rsquo;un partenariat entre Talan et la start-up Aiintense. L\u0026rsquo;objectif est de développer un service de téléexpertise et de télé-RCP (Réunion de Concertation Pluridisciplinaire) pour une prise en charge du patient personnalisée.\nL\u0026rsquo;équipe actuelle nous a présenté l\u0026rsquo;application fonctionnelle et technique, mais aussi l\u0026rsquo;organisation sur le projet et les enjeux liés au domaine de la santé.\nMerci à Nicolas, Pauline, Vincent, Lamis et Sami pour cette présentation.\nComment créer un CLI avec Node Après quelques rappels sur Node, une session de live coding a permis de montrer comment créer un CLI en utilisant ce framework.\nUne application concrète et intéractive qui donne envie de creuser une fois chez soi.\nMerci à Alexis pour cette session.\nAtelier Go L\u0026rsquo;après-midi a été l\u0026rsquo;occasion de pratiquer un atelier Go. Après une présentation des concepts et fonctions principales de Go, les participants ont pu passer à la pratique grâce à des exercices à réaliser seul ou à plusieurs.\nL\u0026rsquo;occasion de découvrir un langage en plein essor dans une ambiance conviviale.\nMerci à Jonathan pour avoir préparé et animé cet atelier.\nMerci à toutes et tous ! À nouveau une bonne participation grâce à ce mode hybride.\nMerci à toutes et tous pour votre participation !\nRendez-vous le Jeudi 23 Juin pour la 35ème journée Ruche\nSi vous souhaitez présenter un sujet ou si certains sujets vous intéressent, contactez l\u0026rsquo;équipe Ruche\ncrédits photos: vecteezy\n","date":"May 30, 2022","href":"https://blog.talanlabs.com/recapitulatif-ruche-34/","kind":"page","labs":null,"tags":["ruche"],"title":"Résumé de la journée Ruche #34"},{"category":null,"content":"Le marquage des produits et contenus originaux est une pratique bien connue chez leurs créateurs pour vérifier leur authenticité et les protéger contre le vol et l\u0026rsquo;imitation. Cette pratique est adaptée pour répondre au besoin de marquage des documents numériques, plus faciles à copier et à transférer. Ce besoin a donné naissance à un nouveau concept de marquage qui est le tatouage numérique.\nLe tatouage numérique est une technique qui consiste à insérer un message appelé marque dans un document numérique (qui peut être une image, une base de données, un modèle d\u0026rsquo;intelligence artificielle (IA), etc.) pour assurer souvent, mais pas seulement, la protection des droits d\u0026rsquo;auteur. Dans cet article, nous nous intéressons particulièrement au tatouage des modèles de deep learning (DL). Ainsi, il semble intéressant de commencer par présenter les concepts de base du tatouage numérique et de détailler ses techniques traditionnelles pour ensuite passer aux techniques appliquées au tatouage des modèles DL.\nConcepts de base du tatouage numérique Le tatouage a plusieurs cas d’utilisation tel que la protection de droit d’auteur. En outre, il est utilisé pour le traçage (fingerprinting), en insérant une marque différente dans chaque élément livré. Ainsi, il est possible d’identifier la personne responsable de la divulgation des données. Un autre cas d’usage est le contrôle d’intégrité. Dans ce cas, une signature est insérée pour vérifier si le document a été altéré.\nLorsque nous parlons de tatouage, il y a un triangle auquel nous pensons directement, dans lequel il y a un compromis entre la fidélité, la capacité et la robustesse. Donc lorsque quelqu’un réfléchit à faire du tatouage numérique il faut qu’il se place dans ce triangle en fonction de ses besoins de sécurité.\n(Exigences sur la méthode de tatouage numérique)\n  La fidélité représente le pouvoir du document numérique à compléter la tâche pour laquelle il a été conçu.\n  La capacité est la taille de message que nous pouvons insérer.\n  La robustesse est la résistance du tatouage aux manipulations extérieures.\n  Selon le cas d\u0026rsquo;usage, le tatouage peut être multi-bit ou zéro bit. Dans le premier cas, la marque est un message composé de plusieurs bits qui est récupéré lors de la phase d\u0026rsquo;extraction. Quant au second, la phase d\u0026rsquo;extraction permet uniquement de vérifier l\u0026rsquo;existence ou non d\u0026rsquo;une marque.\n(Tatouage multi-bit vs tatouage zéro bit)\nUne marque peut subir différentes manipulations :\n  Il y a d’abord les manipulations malicieuses dans laquelle un attaquant vise à endommager ou à enlever la marque ;\n  puis, il y a les manipulations non-malicieuses qui s’explique dans les traitements involontaires ou inévitables qui peuvent perturber la marque cachée, comme la compression par exemple.\n  A partir de ces définitions, le tatouage peut être classé en :\n  Tatouage fragile qui ne résiste à aucune de ces manipulations. Il est utilisé pour la vérification d’intégrité.\n  Tatouage robuste qui peut résister aux manipulations non-malicieuses.\n  Tatouage sécurisé qui survit aussi aux manipulations malicieuses.\n  Il existe deux manières pour faire la détection, soit par détection aveugle, soit par détection non aveugle. La différence entre les deux est que dans le premier cas la marque est récupérée sans avoir besoin de comparer le modèle non-tatoué et le modèle tatoué. Dans le deuxième nous utilisons le contenu original non-tatoué pour l\u0026rsquo;extraction de la marque.\nModèles classiques de tatouage Les premières méthodes de tatouage numérique des images ont été développées par le laboratoire de recherche IBM de Tokyo pour des applications telles que la distribution de contenu en ligne. Comme le montre la figure, l\u0026rsquo;image sert de teaser que les utilisateurs peuvent visualiser et obtenir gratuitement. La marque sera supprimée si les utilisateurs s\u0026rsquo;acquittent de frais supplémentaires.\n(Exemple de tatouage des images par la méthode d’IBM)\nGénéralement, les techniques traditionnelles sont basées sur la transformation des objets en une autre représentation, par exemple une représentation spectrale ou autre, à l\u0026rsquo;aide d’une transformation (transformée en cosinus, transformée de Fourier, transformée en ondelettes, etc.). Ensuite, la marque est insérée et l\u0026rsquo;objet est retransformé en sa représentation initiale à l\u0026rsquo;aide de la transformation inverse. Une clé est utilisée lors de l\u0026rsquo;insertion de la marque afin de garantir que personne ne puisse l\u0026rsquo;enlever même s\u0026rsquo;il connaît la technique d\u0026rsquo;insertion.\n(Principe de fonctionnement des techniques de tatouage traditionnelles)\nTatouage des modèles de deep learning (DL) Bien que les concepts de tatouage décrits dans la section précédente peuvent être largement appliqués aussi au tatouage des modèles DL, deux autres nouvelles caractérisations spécifiques au tatouage DL ont été ajoutées :\n  l’extraction de marque en boite blanche si les paramètres internes du modèle DL sont disponibles,\n  ou la récupération de la marque en boite noire si seulement la sortie du modèle est disponible.\n  Une autre distinction très importante des technologies de tatouage DL est à noter :\n  les méthodes statiques où l’insertion de la marque se fait dans les poids du modèle DL. Ces poids sont déterminés dans la phase d’entraînement et ont une valeur fixe qui ne dépend pas des entrées du modèle.\n  les méthodes dynamiques où la marque est associée au comportement du modèle en correspondance à des entrées spécifiques.\n  Le sens des exigences définies précédemment diffère lorsqu’il s’agit du tatouage DL. Pour la robustesse, la marque insérée doit résister aux différents types de traitement comme le fine tunning (ré-entrainer un modèle pour faire une autre tâche) et Network pruning (simplifier un modèle DL complexe en supprimant les neurones et les liens non significatifs). En ce qui concerne la sécurité, la marque doit être sécurisée contre les attaques intentionnelles comme l’attaque de :\n  Ré-écriture de la marque : un attaquant qui connaît la méthodologie utilisé pour insérer la marque mais ne connaît pas la clé peut essayer d\u0026rsquo;insérer une nouvelle marque et écraser l’ancienne.\n  Extraction du modèle : un attaquant tente de répliquer la fonctionnalité d’un DNN cible, en l’utilisant pour annoter une nouvelle base de données. Cette base de données servira à entraîner un nouveau modèle qui imite l’original.\n  Quant à la fidélité, elle signifie dans ce cas que l’insertion de la marque ne doit pas affecter significativement la précision de l’architecture du modèle DL. La capacité dans le tatouage multi-bit, le schéma de tatouage doit allouer l’insertion du maximum possible d’information dans le modèle DL cible.\nL’intégrité est que le taux d’erreur binaire (ou bien le BER) doit être égale à zéro ou négligeable dans le cas du tatouage multi-bit et que la probabilité de fausse alarme et la détection échouée soit très faible dans le cas zéro-bit. Deux nouvelles exigences qui s’ajoutent dans le cas de tatouage DL. La première est la généralisation qui se résume dans applicabilité de la méthodologie de tatouage sur les différentes architectures et bases de données. La deuxième est l’efficience qui signifie que l’insertion et l’extraction de la marque doivent être négligeable en terme de calcul.\nAvec les développements dans le domaine du deep learning, plusieurs méthodes de tatouage ont été proposées. Le tableau suivant résume ces méthodes en détaillant leurs caractéristiques.\n(Résumé des techniques de tatouage statiques)\n(Résumé des techniques de tatouage dynamiques)\nDéfis à relever Comme présenté précédemment, il existe plusieurs méthodes de tatouage numérique pour les modèles DL permettant de lutter contre plusieurs types de manipulations malicieuses ou non-malicieuses. Malgré que le tatouage dynamique offre plusieurs nouvelles opportunités, il reste beaucoup de questions auxquelles nous devons répondre pour mieux comprendre ses potentiels :\n  Combien de données de déclenchement (données qui provoquent un comportement de prédiction inhabituel dans le modèle marqué) peut-on définir sans affecter la performance du modèle ?\n  Est-il préférable que les données de déclenchement suivent la même distribution que les données standard ou doivent-elles avoir une distribution différente ?\n  Quel est l\u0026rsquo;impact du fine tuning , du ré-entraînement , du pruning , sur le comportement du modèle en fonction des entrées de déclenchement de la marque?\n  La robustesse face au fine tuning , au pruning du modèle et, encore plus, l\u0026rsquo;apprentissage par transfert, est l\u0026rsquo;un des défis les plus difficiles à relever. Il est donc important de développer une technique de tatouage qui soit résistante à différents types d\u0026rsquo;attaques. Dans ce contexte, l\u0026rsquo;exploitation des points forts des techniques existantes peut être une approche à suivre. Cependant, les méthodes de tatouage permettant de lutter contre l\u0026rsquo;extraction du modèle restent un sujet peu développé. En effet, les techniques existantes sont basées sur l\u0026rsquo;insertion dynamique d\u0026rsquo;une marque dans la sortie du modèle. Bien que cette solution soit très efficace dans le cas où la sortie est une image (insertion d\u0026rsquo;une marque visible), elle est inutile dans le cas où la sortie est une classe ou un score. Dans ce contexte, le problème n\u0026rsquo;est plus de protéger le modèle de DL mais plutôt de protéger l\u0026rsquo;expertise intégrée dans le modèle.\nRéférences:   (1) Uchida, Y., Nagai, Y., Sakazawa, S., Satoh, S., 2017. Embedding watermarks into deep neural networks, in: Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval, pp. 269–277\n  (2) Li, Y., Tondi, B., Barni, M., 2020a. Spread-transform dither modulation watermarking of deep neural network. arXiv preprint arXiv:2012.14171\n  (3) Chen, H., Rouhani, B.D., Fu, C., Zhao, J., Koushanfar, F., 2019c. Deepmarks: A secure fingerprinting framework for digital rights management of deep learning models, in: Proceedings of the 2019 on In-ternational Conference on Multimedia Retrieval, pp. 105–113\n  (4) Rouhani, B.D., Chen, H., Koushanfar, F., 2019. Deepsigns: an end-to-end watermarking framework for protecting the ownership of deep neural networks, in: The 24th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS). ACM\n  (5) Szyller, S., Atli, B.G., Marchal, S., Asokan, N., 2019. Dawn: Dynamic adversarial watermarking of neural networks. arXiv preprintarXiv:1906.00830\n  (6) Zhang, J., Chen, D., Liao, J., Fang, H., Zhang, W., Zhou, W., Cui, H.,Yu, N., 2020. Model watermarking for image processing networks,in: Proceedings of the AAAI Conference on Artificial Intelligence,Association for the Advancement of Artificial Intelligence (AAAI).pp. 12805–12812. doi: 10.1609/aaai.v34i07.6976\n  Cover Photo by Michael Dziedzic on Unsplash\n","date":"May 16, 2022","href":"https://blog.talanlabs.com/tatouage-des-algorithmes-d-intelligence-artificielle/","kind":"page","labs":null,"tags":["IA","DL","Sécurité"],"title":"Tatouage des algorithmes d'intelligence artificielle"},{"category":null,"content":"La réalité étendue (XR: Extended reality) est un terme générique désignant tous les environnements combinant éléments réels et virtuels. Cela inclut la réalité augmentée (AR: Augmented reality), virtuelle (VR: Virtual reality) et mixte (MR: Mixted reality).\nLa réalité virtuelle (Beat Saber est une des experiences les plus jouées en VR: Site officiel)\nIl s\u0026rsquo;agit de la technologie qui nous plonge dans des environnements totalement numériques grâce au port d\u0026rsquo;un casque de réalité virtuelle. Cela permet de voyager dans des mondes inexistants pour une expérience de jeux très immersive.\nIl existe de nombreux casques VR : certains sont autonomes et d\u0026rsquo;autres non (reliés à un ordinateur ou un smartphone).\nSur ce marché, Meta (ex-Facebook) est leader. En effet l\u0026rsquo;entreprise représente 78 % de toutes les ventes de casques AR/VR en 2021. Cela explique pourquoi la société communique autant sur le sujet des métaverses. Etant en quasi-monopole sur le matériel, l\u0026rsquo;entreprise est capable de grandement influencer et de contrôler la trajectoire future de la VR.\nSource\nLa réalité augmentée La réalité augmentée peut être utilisée à partir de plusieurs appareils différents (lunette, ordinateur, tablette ou smartphone).\nPour comprendre simplement la réalité augmentée, il suffit de prendre en exemple le jeu Pokémon GO. Grâce à la combinaison du flux de la caméra de son smartphone et de technologies de géolocalisation, l\u0026rsquo;appareil affiche des personnages numériques (les pokémons) dans le monde réel.\nSite officiel Pokemon Go Live\nSnapchat et Instagram sont aussi de bons exemples. En effet sur ces réseaux sociaux, il est possible d\u0026rsquo;appliquer des filtres sur des photos et des vidéos afin d\u0026rsquo;y ajouter une couche d\u0026rsquo;éléments virtuels (2D ou 3D). Ces plateformes utilisent des algorithmes capables de détecter les surfaces planes et certaines parties du corps humains, ce qui permet de faire apparaitre des objets virtuels comme s\u0026rsquo;ils étaient intégrés aux utilisateurs.\nPour résumé, la réalité augmentée permet de rajouter de l\u0026rsquo;information contextuelle à une image réelle mais ne permet pas une grande interaction entre le monde réel et virtuel.\nCependant, il existe une autre technologie permettant une expérience plus immersive, plus interactive et plus précise : la réalité mixte.\nLa réalité mixte La réalité mixte (ou hybride) est, comme la réalité augmentée, la fusion entre le monde réel et virtuel. Pour son utilisation, on a besoin d\u0026rsquo;un casque de réalité mixte qui possède une visière transparente sur laquelle sont projetées des éléments virtuels.\nLa réalité mixte est bien plus immersive que la réalité augmentée. Elle comprend l\u0026rsquo;environnement qui nous entoure et détecte le mouvement de nos mains. Elle nous permet d\u0026rsquo;évoluer dans notre environnement direct et de manipuler des objets virtuels.\nNous avons donc la possibilité de vivre aujourd\u0026rsquo;hui ce que l\u0026rsquo;on peut voir dans le film \u0026ldquo;Minority report\u0026rdquo; mais sans gants et avec l\u0026rsquo;interface directement projetée sur la visière du casque/lunette.\nIl existe peu de casques de réalité mixte sur le marché. Le plus connu étant le casque Microsoft “Hololens”, mais son prix élevé (3000€ ou 5000€) en fait un produit principalement dédié aux professionnels. Meta possède également son casque \u0026ldquo;META 2\u0026rdquo; à un prix plus agressif de moins de 1000€.\nEt la suite ? Les grands groupes investissent fortement le secteur afin de démocratiser les usages. Cette popularisation passera forcément par des avancées technologiques qui nous donnerons du matériel plus performant et moins encombrant (sans fils, casques remplacés par des lunettes et pourquoi pas des lentilles par la suite ?).\nA ce mix de technologie, il faudrait également ajouter une brique IOT pour manipuler le monde réel depuis le monde virtuel. On pourrait alors penser à de nouvelles interfaces qui nous permettrait de contrôler des objets réels depuis n\u0026rsquo;importe où dans le monde.\nPrenons l\u0026rsquo;exemple d\u0026rsquo;un constructeur automobile qui pourrait concevoir en réalité mixte des pèces spécifiques. Le projet permettrait de cocréer et de lancer des impressions 3D sur l\u0026rsquo;appuis d\u0026rsquo;un bouton virtuel.\n","date":"May 10, 2022","href":"https://blog.talanlabs.com/la-realite-etendue/","kind":"page","labs":null,"tags":["AR","VR","MR"],"title":"La réalité étendue"},{"category":null,"content":"Une journée Ruche ? Tous les troisièmes jeudi du mois, les Labiens se retrouvent pour partager et échanger autour de sujets qu\u0026rsquo;ils choisissent. Technique ou organisationnel, sous forme de présentation ou d\u0026rsquo;atelier, c\u0026rsquo;est l\u0026rsquo;occasion de présenter des sujets qui leur tiennent à coeur ou tout simplement s\u0026rsquo;entraîner en vue d\u0026rsquo;une conférence.\nLa dernière journée Ruche s\u0026rsquo;est déroulée le 28 Avril 2022\nVoici le résumé des différentes sessions proposées en mode \u0026ldquo;hybride\u0026rdquo; (présentiel et distanciel).\nSi t\u0026rsquo;as mal quand tu codes c\u0026rsquo;est pas normal ! Quel développeur n\u0026rsquo;a jamais ressenti une douleur aux lombaires ou au poignet à la fin de la journée ? Cette présentation a permis de parcourir quelques symptômes courants liés à notre métier mais aussi à nos mauvaises habitudes\u0026hellip; Au programme, evolutions des IHM, postures et ergonomie ! Certains d\u0026rsquo;entre nous ont découvert des claviers dont ils ignoraient l\u0026rsquo;existence (et l\u0026rsquo;usage)\nMerci à Nicolas pour ce partage.\nSupervision chez Azure Un rappel sur les enjeux du monitoring appliqués dans un environnement cloud Azure.\nAprès quelques rappels sur les bonnes pratiques de log, nous avons pu voir comment le service Application Insight permet de centraliser le monitoring.\nOn n\u0026rsquo;oubliera pas : Collecter, Visualiser, Alerter.\nMerci à Jonathan pour cette présentation.\nApprendre Rust en écrivant une application CLI Rust est un langage qui a la réputation d\u0026rsquo;avoir une courbe d\u0026rsquo;apprentissage assez longue. Après des rappels sur l\u0026rsquo;origine de Rust et sa syntaxe, l\u0026rsquo;objectif était de créer une ligne de commande en live coding.\nÀ retenir : le compilateur et le LSP sont des amis :)\nMerci à Julien pour cette session.\nAtelier Event Storming L\u0026rsquo;après-midi a été l\u0026rsquo;occasion de pratiquer un atelier d\u0026rsquo;Event Storming. Munis de nos post-it, nous avons redessiné la future application \u0026lsquo;Meetup\u0026rsquo; !\nUn super moment d\u0026rsquo;échange où chacun a pu participer à la conception de cette killer app.\nCet atelier a également été réalisé à Devoxx par Nicolas. Merci à lui pour l\u0026rsquo;animation de cet atelier.\nMerci à toutes et tous ! À nouveau une bonne participation grâce à ce mode hybride.\nMerci à toutes et tous pour votre participation !\nRendez-vous le 19 Mai pour la 34ème journée Ruche\nSi vous souhaitez présenter un sujet ou si certains sujets vous intéressent, contactez l\u0026rsquo;équipe Ruche\ncrédits photos: vecteezy\n","date":"May 5, 2022","href":"https://blog.talanlabs.com/recapitulatif-ruche-33/","kind":"page","labs":null,"tags":["ruche"],"title":"Résumé de la journée Ruche #33"},{"category":null,"content":" Et voila l’édition 2022 du Devoxx est terminée. Passé le #DevoxxBlues on a eu envie de vous faire un recap de notre Devoxx 2022\n Devoxx 2022 Day 1 Qui dit Day 1 dit session de 3h et tools-in-action !!! Université ou Lab, c’est là qu’on descend en profondeur dans les sujets. Et les Tools-In-Action c’est donc pour…​ voire les…​ tools…​ en…​ action…​ #MerciCaptainObvious C’est aussi ce jour là qu’on sort avec mal au crane ! Mais comme c’est pour ca qu’on est venu on en sort quand meme avec le sourire !\n La révolution (wasm) est incroyable parce que vraie Philippe CHARRIÈRE et Laurent DOGUIN\n  Laurent et Philippe nous ont parlé de WASM, du tooling, de l’intégration dans les browsers, des utilisations hors browser et tout et tout. Et tout ça a chaque fois avec des démos associées dans plein de langages différents Donc si vous voulez vous faire démystifier le Web Assembly, allez voir la video quand elle sera dispo !  — Nicolas SAVOIS Old Dev Loving Devoxx    Kafka: carte des pièges à l’usage des développeurs et des ops Emmanuel BERNARD et Clement ESCOFFIER\n  Super Université qui montre bien les problématiques pour maintenir en conditions opérationnelles, un kafka dans un kubernetes. L’université a été construite avec une alternance entre la vision Dev et Ops, Ce qui rend le talk vivant et plein d’informations !  — Nicolas SAVOIS Old Dev Loving Devoxx    Loom nous Protègera-t-il du Braquage Temporel ? José PAUMARD et Remi FORAX\n  Bonne université qui présente le projet Loom pour Java qui a pour but d’introduire les threads virtuels, plus légères et performantes que les threads historiques. Avec des explications claires et des démonstrations de code basées sur une version preview du Java SDK, c’est une très bonne introduction pour ce projet prometteur censé arriver en version preview dans Java 20 en septembre 2022.  — Aubin GUILHEM Dev Discovering Devoxx    Sécuriser son cluster Kubernetes on-premise from scratch Ludovic TOISON, Emilien Lancelot et Marion NICOLÉ\n  Conférence intéressante qui tacle les problèmes liés à la sécurité d’un cluster Kubernetes. Ce talk abordait d’abord comment gérer la sécurité à l’intérieur de son cluster grâce aux outils fournis par Kubernetes (SecurityContext, RBAC, AdmissionPolicy, etc…​) avant de continuer avec l’utilisation d’outils externes comme popeye, kube-hunter ou Falco.  — Alice Di Nunno Dev Discovering Devoxx      Devoxx 2022 Day 2 Comment j’ai aidé ma fille à lire avec le machine learning Vincent OGLOBLINSKY\n  Le speaker présente sans prétention la manière dont il a développé un outil reposant sur le machine learning pour aider sa fille à apprendre à lire. L’outil en question est très intuitif : il affiche une syllabe à l’écran et se base sur du speach-to-text pour identifier si la prononciation de l’enfant correspond à la syllabe affichée. Le texte s’affiche ensuite en rouge ou en vert pour indiquer à l’enfant si sa prononciation est correcte. Ce talk a pour objectif de présenter l’outil développé, mais surtout les recherches et difficultés qui ont mené à sa conception.  — Pauline Maitre Dev Discovering Devoxx    En quête du Clean Code avec Sonar, 20 000 lieues sous un océan de code! Nicolas PERU et Claire VILLARD\n  Présentation sur l’intégration de Sonar à un projet existant, par l’exemple de l’implémentation dans le projet Sonar lui-même. Ce talk explique pourquoi et comment faire du clean code grâce à Sonar. Très dynamique et inspirant !  — Pauline Maitre Dev Discovering Devoxx    Dans les coulisses du \u0026#34;Cloud\u0026#34; Cécile MORANGE\n  Dans cette présentation, Cécile nous fait découvrir son métier au sein d’un Data Center et nous montre l’envers du décor d’un \u0026#34;outil\u0026#34; qui fait désormais partie de notre quotidien. Très intéressant en tant que développeur de découvrir cet aspect plus infra, de sa mise en place aux différents moyens mis en place pour assurer fiabilité et résilience au système et à ses données.  — Aubin GUILHEM Dev Discovering Devoxx    Apprendre la musique - developer edition Florent BIVILLE, Sylvain COUDERT et Jean-Baptiste LIEVREMONT\n  Une conférence au ton léger, parfait pour une deuxième partie d’après-midi, qui nous emmène à la découverte du son et comment il est retranscrit par nos machines. Et surtout à la découverte de Sonic Pi (https://sonic-pi.net/), un outil de création de musique avec une approche code, via la création de mélodies simples. Même pour un étranger à la création musicale comme moi, cela donne envie de se lancer dans les nombreux tutos de l’outil.  — Aubin GUILHEM Dev Discovering Devoxx    OAUTH 2.1 expliqué simplement (même si tu n’es pas dev) ! Julien TOPÇU\n  Conférence très intéressante pour comprendre le fonctionnement d’un protocole de délégation d’authentification. Le talk est construit de manière très ludique en faisant des parallèles avec le film The Grand Budapest Hotel, où M. Gustave tente de protéger au mieux l’accès aux pâtisseries de chez MendI’s contre des personnes non autorisées. Il explique ainsi le mécanisme de va-et-vient de token et d’authorization code et les évolutions qui ont mené à OAuth 2.1.  — Pauline Maitre Dev Discovering Devoxx    Developper des applications observables pour la production Pierre Zemb\n  Dans cette présentation, Pierre nous a mené à travers les différentes manières d’analyser les performances d’une application ainsi que des moyens de la débugger pour éviter des plantages inattendus. Il nous détaille comment utiliser au mieux les logs et les metrics via la télémetrie mais aussi comment visualiser ces informations de manière efficace afin de pouvoir faire des déploiements plus stables en production.  — Alice Di Nunno Dev Discovering Devoxx    Éliminez la compléxité de Kubernetes avec Lens Daniel VIRASSAMY, Lee NAMBA et Stéphane MONTRI\n  Présentation et démonstration de l’application Lens développée par Mirantis. Cette application permet de superviser et de controler son cluster Kubernetes à travers une interface visuelle accessible. Il permet gérer son cluster sans passer par la ligne de commande et permet d’aider les débutants à apprendre à utiliser Kubernetes tout en restant très complet pour les plus experimentés.  — Alice Di Nunno Dev Discovering Devoxx    De OUI à SNCF Connect, 10 ans de mobile natif à Flutter Gwenn GUIHAL et Adrien BODY\n  Lors de cette conférence l’équipe de développement de l’application SNCF Connect nous a expliqué pourquoi avoir choisi Flutter en migrant depuis une application native. Flutter répondait à leurs besoins notamment grâce à la présence d’une unique codebase qui leur permettait tout de même d’avoir une UI spécifique à iOS et Android. Mais aussi par la présence de nombreux acteurs tels qu’Ali Express ou Ebay qui ont utilisé cette solution, ce qui permet à Flutter d’avoir une importante communauté. Ils ont aussi communiqué sur pourquoi Flutter et non React Native par exemple.  — Alice Di Nunno Dev Discovering Devoxx      Devoxx 2022 Day 3 Réception d’image satellite 🛰️ avec un Raspberry Guillaume Membré\n  C’est impressionnant tout ce que l’on peut faire de chez soi avec un petit Raspberry pi. Guillaume explique comment il a récupéré des informations qui survolent nos têtes grâce aux ondes radios provenant de plusieurs satellites qui photographient et envoient en clair leurs images sur le plancher des vaches. C’est un beau projet réalisé pour son plaisir personnel et qui ouvre sur le vaste monde de la radio. De la conception de l’antenne à la visualisation de l’image, Guillaume à bien expliqué comment il en est parvenu à avoir de belles images météorologiques de la France.  — Grégoire Joncour Dev Discovering Devoxx      Merci à tous ! Encore un très bon moment partagé avec les équipes de TalanLabs !\n   A l’année prochaine pour une nouvelle édition !\n   ","date":"Apr 29, 2022","href":"https://blog.talanlabs.com/devoxx-2022/","kind":"page","labs":null,"tags":["Devoxx","Devoxx2022"],"title":"Devoxx 2022 - Le Labs y était !"},{"category":null,"content":"Pourquoi ne pas passer par l\u0026rsquo;App Store ? Si vous cherchez à distribuer votre application privée à un certain nombre de personnes, qu’ils soient des particuliers ou des clients, vous serez amenés à devoir vous affranchir de l’app store. Heureusement, il existe une méthode pour cela. Ce petit post vous montrera comment signer votre application et la déployer afin de l\u0026rsquo;installer à travers un site web pour finalement la lancer sur vos appareils.\nPrérequis  Un Mac avec Xcode Un iPhone ou un iPad Un compte Développeur Apple (99€/an) Un site web avec un certificat SSL  Glossaire IPA: IPhone Application, il s\u0026rsquo;agit du package de l\u0026rsquo;application générée qui sera installé sur vos appareils. UDID: Unique Device Identifier, c\u0026rsquo;est un identifiant unique qui représente votre appareil.\nEnregistrer votre UDID Avant de générer votre IPA, nous allons enregistrer l’UDID de votre appareil sur votre compte developpeur. Cela permettra de signer votre application afin qu’elle puisse se lancer sur votre iPhone/iPad.\n Afin de l’enregistrer il va nous falloir le récupérer. Lorsque votre appareil est connecté à votre ordinateur puis sur Xcode ouvrez la fenêtre « Device et simulateurs » (⌘ + ⇧ + 2). Sélectionnez votre appareil dans le menu de gauche. Vous retrouverez votre UDID dans le champ « identifier »   Pour cela rendez-vous sur votre compte developer.apple.com puis « Certificates, Identifiers \u0026amp; Profiles » puis « Devices »   Sur cette page vous aurez un résumé de tous les appareils que vous avez rajoutés.    Ouvrez la fenêtre d\u0026rsquo;ajout (bouton +)\n  Sélectionnez dans la rubrique platform: \u0026ldquo;iOS, tvOS, watchOS\u0026rdquo; Entrez un nom pour votre appareil (ex: \u0026ldquo;iPhone 6s\u0026rdquo;) ainsi que l\u0026rsquo;UDID de votre appareil puis cliquez sur « Continue »\n   Désormais cet appareil sera qualifié pour lancer vos IPA téléchargés depuis votre site web. Cet appareil comptera dans la limite de vos 100 appareils (maximum par plateforme).   Attention cet appareil ne pourra pas être supprimé de votre compte avant la fin de la période de validité de votre adhésion au programme développeur (À chaque renouvellement, vous aurez la possibilité de les supprimer).  Générer une IPA de votre application Avant de générer une nouvelle IPA j\u0026rsquo;ai dû réimporter les profils de développement, pour cela ouvrez xcode puis accédez aux paramètres (⌘ + ,), onglet \u0026ldquo;Accounts\u0026rdquo; puis sélectionnez \u0026ldquo;Download Manual Profiles\u0026rdquo; cela ne devrait pas prendre plus d\u0026rsquo;une dizaine de secondes le temps qu\u0026rsquo;XCode récupère l\u0026rsquo;intégralité de vos profils depuis les serveurs Apple.\nVous pouvez désormais fermer la fenêtre et charger votre projet\nAfin de générer une IPA pour le déploiement, commencez par sélectionner « Any iOS Device » dans le menu destination de votre application (Product \u0026gt; Destination \u0026gt; Any iOS Device)\npuis générez une archive de votre application (Product \u0026gt; Archive). Cela créera un build prêt à être déployé (que ça soit de manière publique via l’app store ou privée). Une fois la compilation terminée une fenêtre (Organizer) s’ouvrira avec la liste de vos archives, la dernière construite sera sélectionnée. Cliquez sur le bouton à droite \u0026ldquo;Distribute App\u0026rdquo;\nvous pourrez ensuite sélectionner \u0026ldquo;Ad Hoc\u0026rdquo; et non \u0026ldquo;App Store\u0026rdquo; (défaut) puis suivant.\nNote : il est possible de sélectionner Entreprise si vous avez un compte entreprise de ce fait passant outre l\u0026rsquo;étape de l\u0026rsquo;UDID (voir partie précédente) mais n\u0026rsquo;ayant pas de compte entreprise pour tester je n\u0026rsquo;élaborerais pas plus.\nsur la prochaine page vous aurez la possibilité de choisir ou non \u0026ldquo;App Thinning\u0026rdquo; qui vous permet d\u0026rsquo;optimiser la taille de votre IPA. Cette option étant facultative, elle reste à votre discretion.\nVous devrez cocher les deux options suivantes :\n \u0026ldquo;Rebuild from Bitcode\u0026rdquo; \u0026ldquo;Include manifest for over-the-air installation\u0026rdquo;  La première option permet de compiler le bitcode de votre app de telle manière à ce qu\u0026rsquo;il soit identique à une distribution app-store. Cela est recommandé. La seconde nous intéresse, car elle va générer des fichiers qui nous serons utile afin de déployer votre application sur votre site web.\nSur la page suivante, quelques métadonnées vous seront demandées, le nom de votre app (déjà rempli, inutile de s\u0026rsquo;en occuper), l\u0026rsquo;url ou votre IPA sera localisé ainsi que deux images (57 par 57 pixels ainsi que 512 par 512 pixels). l\u0026rsquo;outil d\u0026rsquo;export vous demande de renseigner toutes ces URL mais vous n\u0026rsquo;etes pas obligé de fournir les images par la suite. Notez que vous devrez obligatoirement avoir un certificat SSL pour accéder à la page en HTTPs faute de quoi votre appareil refusera d\u0026rsquo;installer l\u0026rsquo;application pour des raisons de sécurité.\nLa prochaine étape concernera la signature de votre application. Vous pouvez laisser \u0026ldquo;Automatically manage signing\u0026rdquo; afin qu\u0026rsquo;XCode s\u0026rsquo;en occupe à votre place.\nà l\u0026rsquo;issue de cette dernière étape, XCode va compiler, signer et packager votre application. Cette procedure peux prendre un certain temps selon la taille et la complexité de votre application. Puis vous pourrez tout simplement appuyer sur le bouton export afin de sauvegarder les binaires et metadata afin de les déployer.\ndans le dossier d\u0026rsquo;export nous retrouverons :\n l\u0026rsquo;archive de votre application au format .ipa le manifest de l\u0026rsquo;application (manifest.plist)  Nous aurons besoin de ces deux fichiers pour le déploiement sur le site web.\n(Optionnel) Vérifier que votre appareil peut lancer l\u0026rsquo;application Il existe une méthode simple pour vérifier que votre appareil est autorisé à lancer l\u0026rsquo;application que vous venez de générer.\nEn effet l\u0026rsquo;archive IPA n\u0026rsquo;est autre qu\u0026rsquo;un fichier ZIP qui contient les binaires de votre application et les metadata.\nfaites une copie de votre app et remplacez son extension par .zip, vous pourrez désormais extraire son contenu.\nEn accédant à l\u0026rsquo;archive, vous trouverez un dossier Payload puis \u0026ldquo;votre_app.app\u0026rdquo; dans ce dossier.\n\u0026ldquo;votre_app.app\u0026rdquo; est un dossier qui contient le contenu de votre application. Accédez-y et vous devriez trouver un fichier \u0026ldquo;embedded.mobileprovision\u0026rdquo;\nen prévisualisant ce fichier avec Quicklook sur votre mac (espace en sélectionant le fichier) vous aurez un résumé de la signature de votre application. Dans la section \u0026ldquo;Provisioned Devices\u0026rdquo; vous pourrez voir les devices autorisés à lancer votre application et devriez retrouver l\u0026rsquo;UDID de l\u0026rsquo;appareil que vous venez d\u0026rsquo;enregistrer.\nPréparer votre site web pour la distribution Lors de la génération de l\u0026rsquo;archive, vous avez mentionné une URL pour votre IPA. Uploadez l\u0026rsquo;IPA que vous avez générée à cet endroit via votre méthode de transfert préférée ainsi que votre fichier manifest.plist.\nsur votre site web rajoutez un lien vers le fichier manifest.plist (non l\u0026rsquo;IPA !) de cette manière : \u0026lt;a href='itms-services://?action=download-manifest\u0026amp;url=https://demo.adinunno.fr/manifest.plist'\u0026gt;Demo\u0026lt;/a\u0026gt; n\u0026rsquo;oubliez pas de remplacer l\u0026rsquo;URL par le chemin de votre manifest.plist.\nIl est important de noter que vous devez utiliser une URL itms-services et non une url classique, et ce, malgré que vous ne passez pas par iTunes Connect ou l\u0026rsquo;App Store cela indiquera à votre iPhone qu\u0026rsquo;il doit installer une application et ou il la trouvera.\nIl est aussi à noter que votre site web doit servir ces deux fichiers avec les mime-types suivant :\n application/octet-stream pour l\u0026rsquo;application IPA text/xml pour le manifest.plist  Si ces mimes-types ne sont pas respectés, votre appareil refusera l\u0026rsquo;installation de votre application.\nSi tout se passe bien vous devriez pouvoir charger votre page web sur votre iPhone et cliquer sur le lien vous informera que le site souhaitera installer l\u0026rsquo;application.\nAcceptez et revenez au menu home de votre iPhone, l\u0026rsquo;application devrait apparaitre et vous pouvez la lancer.\nConcernant les mises à jour de l\u0026rsquo;application Cette méthode de déploiement ne vous permettra pas de distribuer des mises à jour de votre application de manière automatique. Il est possible de mettre à jour l\u0026rsquo;application en réuplodant la nouvelle version sur le site web et en la réinstallant de la meme manière que précedemment. Proposition d\u0026rsquo;amélioration: Vous pouvez si vous le souhaitez, intégrer un message de mise à jour dans votre application et proposer un lien de renvoi vers le manifest en comparant la version de l\u0026rsquo;application à la version uploadée sur votre serveur.\nPour aller plus loin  Documentation officielle sur le déploiement  ","date":"Apr 7, 2022","href":"https://blog.talanlabs.com/distribution-app-iphone-privee/","kind":"page","labs":null,"tags":["ipa","deploiement","web","distribution"],"title":"Comment distribuer son application sans passer par l'App Store ?"},{"category":null,"content":"Une journée Ruche ? Tous les troisièmes jeudi du mois, les Labiens se retrouvent pour partager et échanger autour de sujets qu\u0026rsquo;ils choisissent. Technique ou organisationnel, sous forme de présentation ou d\u0026rsquo;atelier, c\u0026rsquo;est l\u0026rsquo;occasion de présenter des sujets qui leur tiennent à coeur ou tout simplement s\u0026rsquo;entraîner en vue d\u0026rsquo;une conférence.\nLa dernière journée Ruche s\u0026rsquo;est déroulée le 24 mars 2022\nVoici le résumé des différentes sessions proposées en mode \u0026ldquo;hybride\u0026rdquo; (présentiel et distanciel).\nNodeJS avec Typescript Suite de la construction pas à pas d\u0026rsquo;un serveur NodeJS initiée lors de la dernière Ruche. Cette fois-ci, l\u0026rsquo;objectif est d\u0026rsquo;ajouter le support de Typescript.\nAprès une présentation des spécificités de Typescript, le live coding a permis de comprendre comment migrer facilement un projet Javascript vers Typescript. Quelques conseils bien pratiques à utiliser sur tous nos projets.\nMerci à Alexis pour cette nouvelle présentation.\nAiintense et le pupillomètre V2 Un REX sur l\u0026rsquo;avancée du projet pupillomètre, un des différents projets où le Lab accompagne la startup Aiintense.\nCe projet a pour objectif de mieux détecter et mesurer la douleur chez les patients qui n\u0026rsquo;ont pas toujours la possibilité de communiquer.\nLa version 1 permet de mesurer le diamètre de la pupille suite à différents stimulis afin de détecter des incidents neurologiques.\nLa version 2 intègre une caméra thermique afin d\u0026rsquo;ajouter la mesure de la température corporelle. Cette dernière version du pupillomètre montre une corrélation entre le diamètre de la pupille, la température, la fréquence cardiaque et la tension artérielle.\nL\u0026rsquo;équipe prévoit une campagne de collecte de mesures afin d\u0026rsquo;entraîner et améliorer le modèle de Machine Learning.\nMerci à Nabil et Sabri pour cette présentation.\nConstruire un système Linux pour l\u0026rsquo;embarqué Julien a présenté la construction sur mesure d\u0026rsquo;une distribution Linux avec Buildroot L\u0026rsquo;objectif est de construire une distribution la plus petite et personnalisée à ses besoins.\nLa présentation a commencé par un rappel sur le fonctionnement de Linux, du kernel, du bootoader ainsi que du système de fichiers. S\u0026rsquo;en est suivi une démo avec la construction en live d\u0026rsquo;une distribution et son déploiement sur un Raspberry Pi 3b+. En bonus, un code Rust permettant de gérer une diode a été intégré à la configuration de la distribution.\nMerci à Julien pour cette plongée dans le fonctionnement de Linux.\nMerci à toutes et tous ! À nouveau une bonne participation grâce à ce mode hybride.\nMerci à toutes et tous pour votre participation !\nRendez-vous le 28 Avril pour la 33ème journée Ruche\nSi vous souhaitez présenter un sujet ou si certains sujets vous intéressent, contactez l\u0026rsquo;équipe Ruche\ncrédits photos: vecteezy\n","date":"Mar 30, 2022","href":"https://blog.talanlabs.com/recapitulatif-ruche-32/","kind":"page","labs":null,"tags":["ruche"],"title":"Résumé de la journée Ruche #32"},{"category":null,"content":"Qui connait les RFC 7515, RFC 7516, RFC 7517, et RFC 7519 ? Bon la question manque un peu de contexte. En fait elles sont plus connues sous les noms de JWS (Json Web Signature), JWE (Json Web Encryption), JWK (Json Web Key) mais la plus connue reste certainement JWT (Json Web Token).\nElles existent depuis 2010 et permettent principalement de normer les échanges de données entre 2 parties (En général un client et un serveur) Il existe bien des façons d\u0026rsquo;adresser ce point. Je pense notamment aux cookies, token SWT ou SAML. L\u0026rsquo;expérience a montré les limites à l\u0026rsquo;utilisation de ces solutions et nous ne les aborderons pas ici.\nUn des pains-point connus pour un backend consiste à s\u0026rsquo;assurer de l\u0026rsquo;identité du client avec lequel il communique et c\u0026rsquo;est là que JWT intervient. En effet, il permet d\u0026rsquo;identifier et de vérifier l\u0026rsquo;identité d\u0026rsquo;un client. Son format correspondant à du JSON il est \u0026ldquo;facile\u0026rdquo; d\u0026rsquo;en lire le contenu pour en déduire les operations adéquates pour le backend.\nLe format : À quoi ça peut bien ressembler cette affaire ? Elles se décomposent en 3 parties séparées par des '.' :\n eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9. eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IlRBTEFOIExhYnMiLCJpYXQiOjE2NDQ4NTU0MDR9. LFPlnuy_RGaBcFRCsR130lAu0ilOxsMff0OqvUlgtmU  Note : Il est possible de le décoder en utilisant jwt.io \nBon c\u0026rsquo;est coloré tout ça, mais ça veut dire quoi ce charabia ? En décrypté on obtient 3 parties : le header, la payload et la signature\nHeader C\u0026rsquo;est le json d\u0026rsquo;en-tête du token. Il est composé au minimum de la claim alg correspondant au JWA utilisé et de la claim typ indiquant le type du token. L\u0026rsquo;en-tête est amené à changer selon que l\u0026rsquo;on fasse du JWS ou JWE pour présenter d\u0026rsquo;autres informations obligatoires ou non.\n{ \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } Payload Concrètement, il s\u0026rsquo;agit là des données à envoyer dans le token. La payload contient un ensemble de claims, c\u0026rsquo;est-à-dire un ensemble de paire \u0026lt;clé, valeur\u0026gt; indiqué en JSON.\n{ \u0026#34;sub\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Talan Labs\u0026#34;, \u0026#34;iat\u0026#34;: 1644855404 } Signature Comme son nom l\u0026rsquo;indique, elle permet de signer le token afin de vérifier l\u0026rsquo;intégrité du token ainsi que l\u0026rsquo;identité de l\u0026rsquo;expéditeur du token.\nHMACSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), secret ) C\u0026rsquo;est bien de le savoir, mais existe-t-il une différence entre tout ça ? A priori oui sinon il n\u0026rsquo;y aurait pas autant de RFC 😄\nEn réalité on emploie souvent JWT par abus de langage, mais dans la pratique on utilise plus facilement du JWS. En réalité JWT peut aussi bien correspondre à un objet JWE ou JWS selon le cas\nIl faut donc voir JWT comme une abstraction a JWS et JWE. Il permet d\u0026rsquo;encoder des données à échanger, JWS permet de signer les données à échanger, JWE quant à lui, il permet d\u0026rsquo;encrypter les données à échanger. Notez qu\u0026rsquo;il y a une différence entre encrypter et encoder (en savoir plus)\nOn a un JWT, du coup, à la fin, il n\u0026rsquo;y a pas vraiment de différence ?\nSur le format non, sinon ce ne serait pas du JWT.\nEn revanche le contenu oui. Ça ne devrait pas sauter aux yeux du premier coup d\u0026rsquo;œil, mais essayons de voir à quoi cela peut-il bien ressembler.\nJWA  This specification registers cryptographic algorithms and identifiers to be used with the JSON Web Signature (JWS) [JWS], JSON Web Encryption (JWE) [JWE], and JSON Web Key (JWK) [JWK] specifications. It defines several IANA registries for these identifiers. All these specifications utilize JSON-based [RFC7159] data structures. This specification also describes the semantics and operations that are specific to these algorithms and key types.\nRegistering the algorithms and identifiers here, rather than in the JWS, JWE, and JWK specifications, is intended to allow them to remain unchanged in the face of changes in the set of Required, Recommended, Optional, and Deprecated algorithms over time. This also allows changes to the JWS, JWE, and JWK specifications without changing this document.\n Tout est dans le nom. JWA regroupe l\u0026rsquo;ensemble des algorithmes utilisables dans le cadre des spécifications JWK, JWS, et JWE.\nIl correspond donc à l\u0026rsquo;indication placée dans la propriété alg du header. On y retrouve HS256 (exemple par defaut sur jwit.io ). Ceux qui sont désomais deprecated sont retirés de la RFC 7518 et c\u0026rsquo;est la raison pour laquelle est recommandé d\u0026rsquo;utiliser ES256 à date.\nsource : RFC 7518\n   Paramètre alg Description Requirements     HS256 HMAC using SHA-256 Required   HS384 HMAC using SHA-384 Optional   HS512 HMAC using SHA-512 Optional   RS256 RSASSA-PKCS1-v1_5 using SHA-256 Recommended   RS384 RSASSA-PKCS1-v1_5 using SHA-384 Optional   RS512 RSASSA-PKCS1-v1_5 using SHA-512 Optional   ES256 ECDSA using P-256 and SHA-256 Recommended+   ES384 ECDSA using P-384 and SHA-384 Optional   ES512 ECDSA using P-521 and SHA-512 Optional   PS256 RSASSA-PSS using SHA-256 and MGF1 with SHA-256 Optional   PS384 RSASSA-PSS using SHA-384 and MGF1 with SHA-384 Optional   PS512 RSASSA-PSS using SHA-512 and MGF1 with SHA-512 Optional   none No digital signature or MAC performed Optional     JWK  A JSON Web Key (JWK) is a JavaScript Object Notation (JSON) data structure that represents a cryptographic key. This specification also defines a JWK Set JSON data structure that represents a set of JWKs. Cryptographic algorithms and identifiers for use with this specification are described in the separate JSON Web Algorithms (JWA) specification and IANA registries established by that specification.\n Contrairement aux autres RFC, JWK ne correspond pas directement à un token JWT. C\u0026rsquo;est plutôt un objet JSON décrivant un lien entre une clé publique et un token JWT. Cet objet est stocké dans une map et est utilisé dans le cadre des RFC JWS et JWE. À noter d\u0026rsquo;ailleurs que le token JWT stocké est issu d\u0026rsquo;un serveur d\u0026rsquo;authentification et signé avec l\u0026rsquo;algorithme JWA\nIl est possible de le générer en utilisant la librairie nimbus (source: medium)\nimport java.util.*; import com.nimbusds.jose.jwk.*; import com.nimbusds.jose.jwk.gen.*; class ExempleBlog { public static void main(String[] args) { // Generate 2048-bit RSA key pair in JWK format, attach some metadata  RSAKey jwk = new RSAKeyGenerator(2048) .keyUse(KeyUse.SIGNATURE) // indicate the intended use of the key  .keyID(UUID.randomUUID().toString()) // give the key a unique ID  .generate(); // Output the private and public RSA JWK parameters  System.out.println(jwk); // Output the public RSA JWK parameters only  System.out.println(jwk.toPublicJWK()); } } Exemple de JWK\nChacun des champs présents à un rôle spécifique que nous ne détaillerons pas ici. (en savoir plus)\n{ \u0026#34;kty\u0026#34;:\u0026#34;EC\u0026#34;, \u0026#34;crv\u0026#34;:\u0026#34;P-256\u0026#34;, \u0026#34;x\u0026#34;:\u0026#34;f83OJ3D2xF1Bg8vub9tLe1gHMzV76e8Tus9uPHvRVEU\u0026#34;, \u0026#34;y\u0026#34;:\u0026#34;x_FEzRu9m36HLN_tue659LNpXW6pCyStikYjKIWI5a0\u0026#34;, \u0026#34;kid\u0026#34;:\u0026#34;\u0026lt;Public key used\u0026gt;\u0026#34; } JWS Les payloads du JWS sont signées avec une signature (clé publique) qui peut être vérifiée uniquement par le serveur avec une clé de signature secrète (clé privée). Cela garantit que les payloads n\u0026rsquo;ont pas été falsifiées lorsqu\u0026rsquo;elles sont transmises entre le client et le serveur.\nLe contenu du token JWS est encodé en Base64 mais non chiffré. En conséquence JWS est à utiliser uniquement lorsque vous souhaitez échanger des données non sensibles dans la payload du token.\npublic class ExempleBlog { ... /** * * @param DATA_TO_ENCRYPT The JWE content * @param keyId Key ID that help the recipient determine the valid certificate to use to decrypt the JWE * @param rsaPublicKey Public Key of the certificate to use * @return Serialized JWS * @throws JOSEException */ public static String encryptJWS( String DATA_TO_ENCRYPT, String keyId, RSAKey rsaPublicKey ) throws JOSEException { // RSA signatures require a public and private RSA key pair,  // the public key must be made known to the JWS recipient to  // allow the signatures to be verified  RSAKey rsaJWK = new RSAKeyGenerator(2048) .keyID(keyId) .generate(); RSAKey rsaPublicJWK = rsaJWK.toPublicJWK(); Payload payload = new Payload(DATA_TO_ENCRYPT); JWSHeader header = new JWSHeader.Builder(JWSAlgorithm.RS256).build(); JWSObject jws = new JWSObject(header, payload); JWSSigner signature = new RSASSASigner(rsaPublicJWK); jws.sign(signature); return jws.serialize(); } } Exemple de JWS\n eyJpYXQiOjE2NDY0MTU4NDksImFsZyI6IlJTMjU2Iiwia2lkIjoiYWI3Njk1ZjAtODk0Yi00MGZjLTlmNTctNjNkZTFiMTQ4ZWFiIn0. eyJzdWIiOiJKV0UtRXhhbXBsZS1CbG9nIiwicm9sZSI6IkF1dGhvciIsIm5iZiI6MTY0NjQxNTg0OSwiaXNzIjoibmljb2xhcy5ldGllbm5lIiwidXNlcklkIjoiZjlmNDNlOTI tOTdkNy00ZTE3LWIwZmQtZmY5ZTcwYzI0ZTk5IiwiZW1haWwiOiJuaWNvbGFzLmV0aWVubmVAdGFsYW5iYXMuZnIiLCJqdGkiOiIwOTI4NmZmMy0zOGVlLTRhZTQtYTQwYy1hOGEwNTY0MDMyM2IifQ. iQxW7J4BHS5hbj9SZzb0_HfO7VvdScfutlvadO2VBV0EHNPHllyB_K5XS3dyEMkKHMFIrcBl4J3oOqTcKBU6GOtCslVo6kb1t_MERwCFDx7uzVQs027AUJRDCrDhFdyLF_ bbGoZ2jVvWIwEAWm5Bc6KNkFpJBoOZPqluBSs2DAUUAW24vL1E8ar5saCl4tYNLejJz1mLesXip5Fve4QElAw0VhAYqh9JUGamNjpqei36kI6JQj1Fe8FnyP00eFYAZYJMSmrg0SI2jC2IK3-mcRhKj-8-inz3O_gke51UKB-LZ4anDSH3_MguNgrv-II9-Wi44VNLYOnQoubGopv91g  Mais alors comment distingue-t-on la différence entre JWS et JWT ?  Indice: Comparez le header et la signature et vous aurez la réponse.\n JWE Le schéma JWE crypte le contenu au lieu de le signer. Le contenu chiffré ici correspond à la payload du token JWT. JWE, apporte ainsi la confidentialité, mais peut également être signé. Ainsi, on obtient à la fois le cryptage et la signature assurant la confidentialité, l\u0026rsquo;intégrité et l\u0026rsquo;authentification.\nÇa à l\u0026rsquo;air bien ton truc, mais comment fait-on en pratique ?\nPas de problème, voila un exemple de génération de JWE avec la librairie nimbus\nimport com.nimbusds.jose.*; ... public class ExempleBlog { ... /** * * @param DATA_TO_ENCRYPT The JWE content * @param keyId Key ID that help the recipient determine the valid certificate to use to decrypt the JWE * @param rsaPublicKey Public Key of the certificate to use * @return Serialized JWE * @throws NoSuchAlgorithmException * @throws JOSEException */ public static String encryptJWE( String DATA_TO_ENCRYPT, String keyId, RSAPublicKey rsaPublicKey ) throws NoSuchAlgorithmException, JOSEException { // Create the JWE header and specify:  // {\u0026#34;enc\u0026#34;:\u0026#34;A128CBC-HS256\u0026#34;,\u0026#34;iat\u0026#34;:1489420432,\u0026#34;alg\u0026#34;:\u0026#34;RSA-OAEP-256\u0026#34;,\u0026#34;kid\u0026#34;:\u0026#34;54831214775126\u0026#34;}  JWEHeader header = new JWEHeader.Builder(JWEAlgorithm.RSA_OAEP_256, EncryptionMethod.A128CBC_HS256) .keyID(keyId) .customParam(JWTClaimNames.ISSUED_AT, Instant.now().getEpochSecond()) .build(); // Initialized the EncryptedJWT object  JWEObject jweEncrypted = new JWEObject(header, new Payload(DATA_TO_ENCRYPT)); // Generate the Content Encryption Key (CEK)  KeyGenerator keyGenerator = KeyGenerator.getInstance(\u0026#34;AES\u0026#34;); keyGenerator.init(EncryptionMethod.A128CBC_HS256.cekBitLength()); // Create an RSA encrypted with the specified public RSA key  RSAEncrypter encrypter = new RSAEncrypter(rsaPublicKey); // Doing the actual encryption  jweEncrypted.encrypt(encrypter); return jweEncrypted.serialize(); } } Exemple de JWE\n eyJlbmMiOiJBMTI4Q0JDLUhTMjU2IiwiaWF0IjoxNjQ2NDE1MDc1LCJhbGciOiJSU0EtT0FFUC0yNTYiLCJraWQiOiI1ZTBlZDQ3Ny0xMmU2LTQzMjEtODliNS0wNWMwMWU1ZTQyNDAifQ. AB_5L82Qg-RKRRl1CfaNmUV_rHcriOL5Ab09VJT2oKRm9SnZoPlL_sI2fgyrjgjX01ZtgsChgzG2T-F9w6TsFssGFpCXQqHIpdW-jz6oT31hEb3KizWzyWf4SFkzUJGfSn2rqdV4XUqjcwqXjo_ -lzY1qnrB_zCUamULlHE4YFjMydmWU5k5IHSZDvwN8wU1OTQllM_clw1WHaueEePtrbr6_b5it4RRQo-jPfTVbPEzgy0zxu7o2Tx1FsKYPJ2zjR2oUGex-q8H3g5S_0hiVjgeTy6RPY8EImV-Rs302RKQM6RXpAcCKC2ERBRoC-bYUHNVebThIgOom6h-a5O74A.WhAVg-7l4OdQYDUizKTMag. tr4Wif4GKes2U-jjzfiFYMdRaHiQIWmy0VMiH5i1QaRWkbUvVC6mK_Lpj0ICSKU7Mq8sLIPkHlCB0KDDxfXlG2u3YCyTlJFJDJ71y9T0xYEr2Sm_HOgZEcso849rVbNxp-CB_1JK-e5JQ6vPdGVV4R3D6aSrNK9cvGbuDrvY8pSR3j36kKCBCnagGL3loTRLl4Xpro3J8w4lJWRDg7g-TV10fvEFT4GT8bwbEskVeIMXf94SrQQhPAic7enmaw_Y_p-8Anby0CG7dfVQt9KbTzxkINxVXn-SBzF57h0wHPA.qkEI79YgMrjOvp0z8ivUsg  Euh, c\u0026rsquo;est quasiment le même code et le même rendu. Tu n\u0026rsquo;essaierais pas de m\u0026rsquo;enfumer par hasard !? La différence notable se situe au niveau du header et de la signature. Comme indiqué précédemment on remarquera que seul le JWE garantie la sécurité et confidentialité des données (claims) contenu dans le token.\n   JWS Décrypté JWE Décrypté          Mais comment faire pour lire le contenu de mon token depuis mon client ? À partir du moment où on veut lire le contenu d\u0026rsquo;un token JWT ou JWS côté client, cela pose des problèmes de sécurité. Dans le cas d\u0026rsquo;un JWE il s\u0026rsquo;agit de stocker la clé publique (ou le secret partagé) sur le client. En revanche, pour JWS, et JWT \u0026ldquo;pas de problème\u0026rdquo; car rappelez-vous que le token est simplement encodé et non encrypté 😉 (en savoir plus)\nEn conséquence, on comprendra aisément une des contraintes de l\u0026rsquo;utilisation du JWE à savoir, l\u0026rsquo;utilisation d\u0026rsquo;une clé pour le décodage. L\u0026rsquo;utilisation de ce type de token sera donc plutôt réservée dans le cadre d\u0026rsquo;échange serveur → serveur.\nUn cas d\u0026rsquo;utilisation intéressant pourrait être l\u0026rsquo;échange de données entre de 2 serveurs qui ne se connaissent pas. Un token JWE emit par un serveur peut alors transiter par client puis vers le second serveur sans que le client(ou tout autre intermédiaire) soit en capacité de lire le contenu de ce token.\nOhh attend 1 minute, dans ce cas JWT c\u0026rsquo;est sécure ou pas !!!? Si on ne fait pas attention aux claims contenues dans le token les conséquences en cas d\u0026rsquo;interception peuvent être grave. Il est préconisé de ne jamais y faire figurer de donnée sensible aussi bien dans un jwt qu\u0026rsquo;un jws. En revanche, si ce point pose problème il faut alors utiliser un token JWE.\nNota\nLe header d\u0026rsquo;un token JWT contient plusieurs informations dont une intitulée alg. Celle-ci indique le JWA utilisé pour la payload. Donc, dans le cas du JWT non sécurisé, cette propriété est simplement laissée à la valeur \u0026lsquo;none\u0026rsquo;. En d\u0026rsquo;autres mots les JWT non sécurisés, correspondent à un JWS sans signature (étrange de faire cela, mais bon, c\u0026rsquo;est possible)\nDes alternatives : Pff j\u0026rsquo;connais JWT c\u0026rsquo;est pas terrible, il y a mieux Il est possible de comparer JWT à d\u0026rsquo;autres solutions telles que les cookies, SWT (simple Web Token), SAML (Security Assertion Markup Language Tokens) mais nous ne rentrerons pas dans le détail de chaque.\nCe qu\u0026rsquo;il faut savoir :\nCookies\nLe plus souvent un cookie est une chaîne de caractère random stockée coté serveur et coté client. Bien qu\u0026rsquo;elle ne contienne aucune donnée client, elle permet d\u0026rsquo;identifier une session utilisateur auprès du serveur. Elle ne permet pas de vérifier l\u0026rsquo;expéditeur de la requête du client vers le serveur, ni l\u0026rsquo;intégrité des données contenues dans ladite requête. Par ailleurs, son utilisation nécessite des actions coté serveur afin de stocker/invalider les sessions utilisateurs au fil du temps.\nCe type d\u0026rsquo;approche convient à une utilisation stateful d\u0026rsquo;une application. En revanche, pour une approche stateless, passez votre chemin.\nSWT (Simple Web Token)\nEn fait, ce qui se trouve à l\u0026rsquo;intérieur d\u0026rsquo;un SWT est visible de tous. C\u0026rsquo;est un simple token, après tout contenant un ensemble de paire clé-valeur encodées.\nCet encodage, repose sur une clé partagée entre le client et le serveur. Elle permet de générer un hash des claims transmises dans le token assurant ainsi son intégrité lors des échanges.\nSAML (Security Assertion Markup Language)\nSi sur le papier, le principe de JWT et de SAML est de concourir à l\u0026rsquo;identification d\u0026rsquo;un utilisateur, SAML va plus loin dans ce principe avec la mise en place d\u0026rsquo;un IdP (Identity Provider) et d\u0026rsquo;un Service Provider. En très simple, on ne joue pas dans la même cour ! (En savoir plus)\nEn résumé / Bilan des courses  JWA : c\u0026rsquo;est juste, l\u0026rsquo;ensemble des algorithmes à utiliser pour générer un token JWK : Ensemble de clés permettant de lier un token JWT à une clé publique JWT : La base à condition de ne pas transmettre d\u0026rsquo;info sensible hein ! JWS : Juste pour l\u0026rsquo;intégrité du token JWE : top du top d\u0026rsquo;un point de vue sécurité pour les tokens JSON. Il garantit l\u0026rsquo;intégrité, la sécurité et confidentialité des données contenues dans le token. À utiliser pour les échanges serveur → serveur.   Sources  https://auth0.com/learn/json-web-tokens https://auth0.com/blog/how-saml-authentication-works https://openid.net/developers/jwt https://datatracker.ietf.org/doc/html/rfc7515 https://datatracker.ietf.org/doc/html/rfc7516 https://datatracker.ietf.org/doc/html/rfc7517 https://datatracker.ietf.org/doc/html/rfc7518 https://datatracker.ietf.org/doc/html/rfc7519  ","date":"Mar 11, 2022","href":"https://blog.talanlabs.com/2022-03-11-jwt-jws-jwe/","kind":"page","labs":null,"tags":["JWT","JWE","JWS","JWA","JWK"],"title":"JWT, What else ?"},{"category":null,"content":"Une journée Ruche ? Tous les troisièmes jeudi du mois, les Labiens se retrouvent pour partager et échanger autour de sujets qu\u0026rsquo;ils choisissent. Technique ou organisationnel, sous forme de présentation ou d\u0026rsquo;atelier, c\u0026rsquo;est l\u0026rsquo;occasion de présenter des sujets qui leur tiennent à coeur ou tout simplement s\u0026rsquo;entraîner en vue d\u0026rsquo;une conférence.\nLa dernière journée Ruche s\u0026rsquo;est déroulée le 24 Février 2022\nVoici le résumé des différentes sessions proposées en mode \u0026ldquo;hybride\u0026rdquo; (présentiel et distanciel).\nProjet de la démarche Souffl Depuis le 3 Février, les équipes de Souffl ont rejoint Talan. C\u0026rsquo;était l\u0026rsquo;occasion de présenter et partager avec les Labiens leur démarche et savoir faire.\nUn bel exemple de projet a été présenté : conception et réalisation d\u0026rsquo;un capteur pour collecter et analyser les performances des chevaux de course en entraînement.\nMerci à Nicolas et Arnaud pour leur présentation très appréciée.\nN\u0026rsquo;hésitez pas à lire le communiqué Talan concernant l\u0026rsquo;arrivée de Souffl.\nComment créer un paquet NPM À travers une séance de live coding, nous avons suivi étape par étape la construction d\u0026rsquo;un serveur HTTP en NodeJS.\nPas d\u0026rsquo;utilisation de framework superflu et des astuces pour bien paramétrer les dépendences Node côté serveur.\nLa suite de cette session est déjà programmée !\nMerci à Alexis pour cette séance soutenue.\nIPFS le protocole alternatif au HTTP Une présentation d\u0026rsquo;IPFS, système distribué de fichiers pair à pair qui ne dépend pas de serveurs centralisés.\nUn rappel des enjeux du Web3 et de la décentralisation suivi d\u0026rsquo;un exemple de transfert de fichier.\nMerci à Aubin pour ce partage.\nExtraction des connaissances à partir de données textuelles Une introduction aux techniques de data science pour extraire de la connaissance à partir de sources de données semi-structurées ou non structurées.\nEn alternant théorie et exemples pratiques, nous avons pu comprendre la nécessité d\u0026rsquo;adapter les modèles au contexte pour rester pertinents.\nLa présentation s\u0026rsquo;est terminée par la démonstration de l\u0026rsquo;outil de veille \u0026ldquo;Talan Brain\u0026rdquo; qui met en pratique tout ce savoir-faire.\nMerci à Nabil pour cette présentation.\nMerci à toutes et tous ! Un forte participation grâce à ce mode hybride, près de 80% de présence.\nEnfin la possibilité de se retrouver en physique après de longs mois d\u0026rsquo;attente !\nUne journée suivie de quelques verres dans la bonne humeur.\nMerci à toutes et tous pour votre participation !\nRendez-vous fin Mars pour la 32ème journée Ruche\nSi vous souhaitez présenter un sujet ou si certains sujets vous intéressent, contactez l\u0026rsquo;équipe Ruche\ncrédits photos: vecteezy\n","date":"Mar 7, 2022","href":"https://blog.talanlabs.com/recapitulatif-ruche-31/","kind":"page","labs":null,"tags":["ruche"],"title":"Résumé de la journée Ruche #31"},{"category":null,"content":"Cet article fait suite à la partie 1, écrire soi même un outil de monitoring.\nArchitecture de l\u0026rsquo;outil de monitoring flowchart RL; A1[Agent 1]-- Send metric---M[ ]; A2[Agent 2]-- Send metric---M[ ]; A3[Agent 3]-- Send metric---M[ ]; subgraph Monitoring server M[Server]-- Save on disk ---F[(Filer)] end  mermaid.initialize({ startOnLoad: true, securityLevel: 'loose'}); Ce schéma décrit l\u0026rsquo;architecture minimale pour monitorer : l\u0026rsquo;outil ne peut pas accéder directement aux informations des machines.\nIl faut installer, sur chaque machine, un agent qui aura la charge de lire les métriques et de l\u0026rsquo;envoyer vers le serveur. On retrouve ce mécanisme chez Elastic avec les \u0026ldquo;beats\u0026rdquo; : metricbeat, heartbeat.\nPour gérer le heartbeat, une goroutine tournera à intervalle régulier pour vérifier si les urls renvoient une 200.\nCela implique d\u0026rsquo;avoir, pour chaque service, une url de test qui renvoie toujours un code HTTP 200.\n🔺 Les valeurs de la mémoire, cpu, l\u0026rsquo;espace disque et la température dépendent des OS. Vous pouvez utiliser les contraintes au build pour écrire du code spécifique à votre OS.\nSolutions pour écrire à la main Voici différentes façon d\u0026rsquo;écrire des données en gardant en mémoire cette règle : une écriture rapide implique souvent une lecture lente, et vice versa.\nSérialisation JSON Très rapide à mettre à place et supportée par de nombreux langages, les inconvénients sont nombreux dans notre cas :\n L\u0026rsquo;écriture est lente de base (introspection) Le stockage est plus important à cause de la redondance de la structure Impossible de streamer la lecture pour traiter au fil de l\u0026rsquo;eau Impossible d\u0026rsquo;ajouter des données, il faut tout réécrire  Sérialisation langage Gobencode en go, serialisable en Java :\n  Dépend du langage, ne peut pas être lu dans un autre langage\n  Illisible par un humain\n  Impossible d\u0026rsquo;ajouter des données, il faut tout réécrire\n  Go propose une solution pour écrire les données avec un buffer en implémentant io.writer mais elle est plutôt complexe à mettre en œuvre.\n  Chante Sloubi ou l\u0026rsquo;écriture manuelle La dernière solution est l\u0026rsquo;écriture manuelle où on est libre de définir notre structure et nos propres critères :\n Faible utilisation des resources (CPU \u0026amp; Ram) pour écrire Ne pas tout réécrire à chaque fois toutes les valeurs d\u0026rsquo;une métrique Écrire uniquement les informations nécessaires sans overload  On est d\u0026rsquo;accord, cette solution est la plus sympa, allons-y.\nEcrire ses fichiers à la main On ne souhaite pas stocker pas pour stocker, mais en réfléchissant à comment on souhaite accéder aux données. Dans mon cas, je souhaite afficher, pour chaque machine, une ou plusieurs métriques d\u0026rsquo;une journée. Cela implique, si je veux une recherche efficace, de stocker ensemble les métriques de la machine pour une même journée dans un même fichier. Au niveau de la volumétrie, pour 4 machines, j\u0026rsquo;aurai 120 fichiers créés pour un mois.\nVoici une proposition de structure. Le fichier se découpe en deux parties :\n Un header de taille fixe en début de fichier avec les informations sur chaque métrique stockée et où les trouver  La taille fixe permet de lire le header en une seule fois, beaucoup plus rapide. Pour cela, il faut fixer le nombre maximum de header Chaque header de métrique va pointer vers la position du fichier où se trouvent les données On fixe le nombre de métriques que l\u0026rsquo;on veut afin d\u0026rsquo;un header avec une taille fixe   Le corps du fichier avec les valeurs pour chaque métrique.  On stocke les métriques dans des blocs : l\u0026rsquo;espace est réservé et toutes les valeurs seront contigües S\u0026rsquo;il n\u0026rsquo;y a plus d\u0026rsquo;espace dans le bloc pour cette métrique    On peut représenter le fichier avec des structures :\ntype headerFileMetrics struct { name string\t// max length 48  headerMetrics []*headerMetric } type headerMetric struct { name string firstBlockPosition int64 // first block of data, used to read values \tcurrentBlockPosition int64\t// last block of metric. Used to know where to write new metrics } type MetricPoint struct{ Timestamp int64 Value float32 } // Header in each block type blockHeader struct { nbPoints int32 // current points in block  nextBlock int64 // position of next block if this is full  positionInFile int64 // position of this block in file  pointsByBlock int // nb points max by block } type block struct{ header blockHeader points []MetricPoint } La structure du fichier avec le détail pour chaque champ :\nL\u0026rsquo;écriture se fera en une seule fois en pré-calculant le bloc à écrire : le header sera gardé en mémoire, modifié et écrit en une fois. Chaque nouveau bloc (ou mise à jour) sera écrit en une fois en réservant tout l\u0026rsquo;espace nécessaire (même s\u0026rsquo;il n\u0026rsquo;y a pas de valeur encore).\nflowchart TD; subgraph Header Metric 1 H1[Header 1]--FB1[First block] FB1--CB1[Current block] end subgraph Header Metric 2 H2[Header 2]--FB2[First block] FB2--CB2[Current block] end subgraph Data 1 B11[Block 1]--BB12[Block 2] FB1--B11 CB1--BB12 end subgraph Data 2 FB2--B21[Block 1] CB2--B21 end  mermaid.initialize({ startOnLoad: true, securityLevel: 'loose'}); Go fournit plusieurs fonctions pour écrire des données octet par octet :\n String : Pour écrire une chaine de caractères de longueur n, on écrit d\u0026rsquo;abord la taille de la chaine (1o pour longueur de 255 par ex) Entier : choix de la taille, int8 (1 o) / 255 valeurs, int 16 (2 o) / 65000, int32 (4 o) / 4 milliards ou int64 (8 o). Attention au bit de poids fort LittleEndian / BigEndian Float : Go permet de représenter les floats sous forme d\u0026rsquo;entier de 32 bits :  math.Float32bits(float32(3.2)) // =\u0026gt; 1078774989 math.Float32frombits(1078774989) // =\u0026gt; 3.2 Voici un exemple d\u0026rsquo;algorithme pour écrire les points dans un block de données.\nconst pointsByBlock = 1440 func writePointsInBlock(f *os.File,bh *blockHeader,points []model.MetricPoint, hm * headerMetric){ size := bh.availableSpace() // available nb points in block \tpointsToWrite := points // If no enough space, juste write some point \tif size \u0026lt; len(points){ pointsToWrite = points[0:size] } // Write points in block \tf.WriteAt(getPointsAsBytes(pointsToWrite),bh.getPositionInBlock()) bh.updateNbPoints(len(pointsToWrite)) // If no enough space for all metrics, create new block and write inside \tif size \u0026lt; len(points){ nextBlock := createNewBlockHeader(getEndPosition(f),pointsByBlock) // Reserve space in file for while block \tf.WriteAt(make([]byte,bh.getSizeBlock()),nextBlock.positionInFile) // Link actual block to next block \tbh.nextBlock = nextBlock.positionInFile // Update header to link current block to the new one \thm.currentBlockPosition = nextBlock.positionInFile writePointsInBlock(f,nextBlock,points[size:],hm) } bh.flushHeader(f) } Désormais je suis capable d\u0026rsquo;écrire mes métriques à la main. La prochaine étape va consister à aller les lire.\nLançons une requête Voici le déroulé d\u0026rsquo;une requête :\nsequenceDiagram participant User participant Server participant Memory participant Disk participant File User-Server:ask metrics Server-Memory:Get last metrics not flushed Memory--Server:Return metrics Server-Disk:find file note right of Server:by instance and date File--Server:return good file Server-File:read header (971B) Server-Server:Find data position in header Server-File:read metric block File--Server:return list of timestamp/value Server-Server:Aggregate note right of Server:Aggregation of metrics from memory and disk Server--User:return values  mermaid.initialize({ startOnLoad: true, securityLevel: 'loose'}); La subtilité consiste à agréger les métriques provenant du fichier ainsi que les dernières métriques en mémoire (si elles correspondent à la date recherchée).\nEt ensuite ? Une petite API en Go pour lancer des requêtes, un front léger en svelte pour afficher les graphiques et voici le résultat :\nMon besoin de créer un outil de monitoring était guidé par les performances : peu d\u0026rsquo;utilisation des resources aussi bien pour le serveur que pour les agents.\n   Instance CPU Mémoire     Serveur 1% 1Mo   Agent 1 3% 1Mo   Agent 2 2.6% 1Mo   Agent 3 1% 1Mo    L\u0026rsquo;empreinte mémoire et CPU sont très faibles et c\u0026rsquo;est exactement ce que je voulais !\nPour aller plus loin J\u0026rsquo;ai pensé à plusieurs améliorations possibles :\n Stocker plusieurs journées dans un même fichier (par ex la semaine) afin de limiter le nombre de fichiers générés  On devra changer la structure de header pour inclure les dates On pourra trier les éléments du header pour rechercher rapidement dedans (sans devoir parser toutes les valeurs du header)   Mettre en place un système d\u0026rsquo;alerting pour éviter de regarder régulièrement les métriques  Vous pouvez retrouver tout le code du projet sur mon compte github.\n","date":"Mar 3, 2022","href":"https://blog.talanlabs.com/write_monitoring_p2/","kind":"page","labs":null,"tags":["Golang","Monitoring"],"title":"Écrire soit même son outil de monitoring, partie 2"},{"category":null,"content":"Dans le cadre personnel, je possède plusieurs Raspberry Pi (RPi) qui font tourner plusieurs services et je souhaitais comprendre ce qu\u0026rsquo;il se passait dessus. Voici, basiquement, à quoi ils servent :\nflowchart LR; subgraph RPi-1 M[Serveur musique] S[(Disque)] P[Serveur photo] end subgraph RPi-2 CP[Convertisseur photo] CV[Convertisseur vidéo] end subgraph RPi-3 B[Backup serveur] MP[Lecteur musique] end P--CP P--CV M--MP  mermaid.initialize({ startOnLoad: true, securityLevel: 'loose'}); Au travail, quand on pense monitoring, on pense immédiatement à de grosses solutions comme InfluxDB, ELK\u0026hellip;mais est-ce ce dont j\u0026rsquo;ai besoin ?\nMais qu\u0026rsquo;est ce que le monitoring ?\nMonitorer consiste à surveiller le bon fonctionnement d\u0026rsquo;un système et à réagir en fonction de critères : alerting, redémarrage de service\u0026hellip; Cette surveillance peut se faire à différents niveaux : machine, applications (logs, événements), réseau\u0026hellip;\nPour ma part, je souhaite :\n savoir quel est l\u0026rsquo;état physique des machines : CPU, mémoire, espace disque connaître la température du processeur, en particulier pour les RPi fanless qui peuvent chauffer surveiller l\u0026rsquo;état de plusieurs services (heartbeat)  Point lexique : métrique, supervision et hypervision\u0026hellip; Un point sur les différents termes :\n Une métrique est une mesure à un instant T. Exemple : Température CPU de 35.2° le 20220202-12:10:00 Un indicateur est une interprétation d\u0026rsquo;une ou plusieurs métriques. Température OK La supervision consiste à surveiller l\u0026rsquo;évolution de métriques. C\u0026rsquo;est un terme ancien intégré au monitoring Le monitoring englobe la supervision et ajoute la notion de surveillance et d\u0026rsquo;alerting L'hypervision est la centralisation des outils de supervision  La solution que je recherche est un outil de supervision. Je ne souhaite pas pour l\u0026rsquo;instant être notifié en cas d\u0026rsquo;échec.\n🔺 Dans le cadre d\u0026rsquo;un usage professionnel, afficher des graphiques n\u0026rsquo;a aucun intérêt : le nombre de services est trop important pour demander à un humain de les surveiller.\nIl faut prévoir des alertes en définissant sur quels critères pertinents elles seront lancées. Les graphiques seront utilisés ponctuellement pour comprendre un problème, pourquoi une alerte a été levée.\nVoyons les solutions du marché existantes ?\nElasticsearch, InfluxDB\u0026hellip; Plusieurs solutions existent pour faire du monitoring ou du stockage timeseries (évolution d\u0026rsquo;une mesure dans le temps).\nDans toutes ces solutions, il faut installer des agents sur les machines à surveiller afin de collecter les métriques et les envoyer à un serveur central :\n Affichage de graphique avec Grafana et une base :  avec InfluxDB / Telegraf : LA base spécialisée dans les timeseries avec l\u0026rsquo;agent de collecte Telegraf avec Prometheus : une alternative à la précédente   ELK : Elasticsearch / Kibana : cette base s\u0026rsquo;oriente depuis plusieurs années sur le monitoring (applicatif, infrastructure, logs\u0026hellip;) grâce à de nombreux connecteurs comme filebeat, metricbeat, heartbeat Datadog, Dynatrace, Splunk\u0026hellip;  Ces outils sont performants, mais dans mon contexte :\n Ce sont toujours des applications distribuées (même si elles peuvent tourner en mono-noeud) L\u0026rsquo;application centrale est souvent lourde en termes de resource (2GB de ram, 2 coeurs) De très nombreuses fonctionnalités que je n\u0026rsquo;utiliserai pas : création de dashboards, nombreux types de graphiques, alerting, analyse\u0026hellip; Plusieurs produits ne proposent pas de version gratuite\u0026hellip; rédhibitoire  Alors que choisir ? Je ne vais quand même pas coder cette solution moi-même\u0026hellip;et puis pourquoi pas ?\nAvant de coder une solution à la main, étudions le fonctionnement d\u0026rsquo;une base distribuée qui peut stocker des données temporelles, Cassandra.\nFonctionnement du stockage sur Cassandra Quand on utilise une base de données NoSQL, la seule question que l\u0026rsquo;on doit se poser est \u0026ldquo;comment est ce que je veux retrouver mes données ?\u0026rdquo;. Il ne faut pas s\u0026rsquo;imaginer stocker des données de manière désordonnée pour espérer plus tard les retrouver.\nDans une base distribuée, le fait d\u0026rsquo;interroger, lors d\u0026rsquo;une requête, l\u0026rsquo;ensemble des nœuds, produira de mauvaises performances :\n la durée de la requête dépendra de la machine la plus lente l\u0026rsquo;utilisation du réseau est plus conséquente l\u0026rsquo;agrégation de données des différents nœuds est coûteuse (elle s\u0026rsquo;effectue souvent sur une seule machine)  Si on veut avoir une lecture plus efficace, les données à lire doivent être colocalisées. En ayant les données à rechercher sur la même machine et contigües, on les lira en une seule fois. Cassandra illustre très bien ce mécanisme dans sa conception.\nCassandra est une base de données colonne, c\u0026rsquo;est-à-dire que pour une clé donnée, on va lire un certain nombre de colonnes.\nLa clé de partition (PK) permet de définir sur quelle machine se trouve les données tandis que la clé de cluster (clustering key / CK) est la clé d\u0026rsquo;unicité sur la machine. Les données sont également regroupées pour une même CK.\nSchéma de stockage\nsequenceDiagram participant User participant C* participant Memory participant CommitLog participant SSTables User-C*:Save info C*-Memory:Save info as structured C*-CommitLog: Append data in file Note right of CommitLog: Used to retrieve data after crash C*--User:It's saved Memory-SSTables:Flush files on disk Note right of SSTables: Data are copied frequently SSTables--CommitLog:Remove useless commits SSTables-SSTables:Compact tables Note right of SSTables: Clean files an optimize when necessary  mermaid.initialize({ startOnLoad: true, securityLevel: 'loose'}); Comment Cassandra stocke ses données ? Cela se fait en plusieurs étapes :\n Lors de la réception de nouvelles données sur un nœud :  Le driver détermine sur quelle(s) machine(s) / instance(s) (appelé replica) les données doivent être copiées, à partir de la clé de partition Sur chaque machine, les données sont copiées  En mémoire de manière structurée, afin de pouvoir les rechercher À la fin du commitLog (append) afin de pouvoir restaurer les données lors d\u0026rsquo;un crash.   À intervalle régulier les données sont \u0026ldquo;flushées\u0026rdquo; sur le disque dans les SSTables, structures de données organisées permettant une recherche rapide. Régulièrement, les tables sont compactées afin de supprimer les données \u0026ldquo;tombstone\u0026rdquo; (suppression en deux temps) et réorganiser les données pour les colocaliser. Il existe plusieurs stratégies qui vont être plus ou moins consommatrices en ressources : l\u0026rsquo;espace disque, la mémoire ou le cpu.    Cassandra est rapide en écriture car les nouvelles données sont stockées en mémoire et dans un fichier non structuré de manière séquentielle. Ce qui est intéressant, c\u0026rsquo;est que pour proposer une écriture efficace, il ne faut pas écrire les données de manière systématique et l\u0026rsquo;écriture contigüe est plus efficace que l\u0026rsquo;écriture aléatoire (surtout sur des disques mécaniques).\nEn fonction de notre besoin et de notre capacité à accepter la perte de données, nous choisirons à quelle fréquence il faut écrire les données.\nPour déterminer la fréquence, il faut calculer ce que l\u0026rsquo;on va stocker :\n 3 machines vont remonter des métriques 4 métriques de base sont prévues : cpu, mémoire, disque et température A raison d\u0026rsquo;une mesure par minute, on obtient par jour : 4 x 3 x 1440 = 17280 mesures par jour  Si l\u0026rsquo;on souhaite sauvegarder les données quand notre buffer atteint 100 valeurs, la perte maximum en cas de crash sera de 8 minutes. Il est possible, comme Cassandra, d\u0026rsquo;éviter de perdre des données en sauvegardant dans un fichier non structuré, utilisé uniquement pour la récupération, les métriques dès qu\u0026rsquo;elles arrivent.\nAu terme de cet article, nous avons vu ce qu\u0026rsquo;était le monitoring, défini quel était mon besoin et étudier le fonctionnement d\u0026rsquo;une base distribuée.\nDésormais, nous allons pouvoir passer aux choses en sérieuses en écrivant notre propre solution de monitoring dans la 2ème partie de cet article.\n","date":"Feb 28, 2022","href":"https://blog.talanlabs.com/write_monitoring/","kind":"page","labs":null,"tags":["Golang","Monitoring"],"title":"Écrire soit même son outil de monitoring, partie 1"},{"category":null,"content":"Cover Photo by Paul Esch-Laurent on Unsplash\nnpm Préambule : Contrairement aux croyances, npm ne veut pas dire Node Package Manager, c\u0026rsquo;est une rétroacronymie, même si concrètement, c\u0026rsquo;est exactement ce qu\u0026rsquo;il fait 😅. Il est développé et maintenu par la société npm (site officiel), qui s\u0026rsquo;occupe aussi du registry npm publique.\nVous ne comprenez pas ce qu\u0026rsquo;il se passe lorsque vous faites un npm install ?\nD\u0026rsquo;où sort le package-lock.json ?\nÀ quoi servent toutes les commandes autres que npm install ou npm run start ?\nEh bien moi non plus je ne comprenais pas ! Mais ne vous inquiétez pas, je vais vous aider à y voir plus clair.\nDans cette série d\u0026rsquo;articles, je vous propose de faire un tour sur les différentes fonctionnalités de base de npm.\nPartie 1 : Installer un projet et gérer les versions des dépendances Démarrage d\u0026rsquo;un projet La première chose que je fais quand je récupère un projet node, c\u0026rsquo;est de lancer la commande npm install.\nJe vois plein d\u0026rsquo;informations, et au début, voilà ce que je me disais :\n Peer dependencies ??? Bah c\u0026rsquo;est juste WARN osef Vulnerabilities ?? Allez, on va dire que j\u0026rsquo;ai pas vu Un dossier node_modules ??? Si c\u0026rsquo;est un dossier ça doit être important, je commit ! package-lock.json, deux versions possibles :  Un nouveau fichier ?? Je commit pas, on va me demander ce que c\u0026rsquo;est. Comment ça modifié ?? J\u0026rsquo;y ai pas touché ! Allez git reset.    Décrire son projet : package.json Pour essayer de mieux comprendre toutes ces informations et alertes, commençons par regarder le fichier qui défini notre projet, le package.json.\n{ \u0026#34;name\u0026#34;: \u0026#34;my-project\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;react-scripts start\u0026#34;, \u0026#34;build\u0026#34;: \u0026#34;react-scripts build\u0026#34;, \u0026#34;test\u0026#34;: \u0026#34;react-scripts test\u0026#34; }, \u0026#34;dependencies\u0026#34;: { \u0026#34;react\u0026#34;: \u0026#34;^17.0.2\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;^17.0.2\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;@testing-library/jest-dom\u0026#34;: \u0026#34;^5.15.1\u0026#34;, \u0026#34;@testing-library/react\u0026#34;: \u0026#34;^11.2.7\u0026#34;, \u0026#34;@testing-library/user-event\u0026#34;: \u0026#34;^12.8.3\u0026#34;, \u0026#34;react-scripts\u0026#34;: \u0026#34;^4.0.3\u0026#34; }, \u0026#34;eslintConfig\u0026#34;: { \u0026#34;extends\u0026#34;: [ \u0026#34;react-app\u0026#34;, \u0026#34;react-app/jest\u0026#34; ] } } Un exemple issu de la génération d\u0026rsquo;un nouveau projet react\nLes champs génériques Les premières entrées name et version donnent des informations générales sur votre projet. Il en existe plein d\u0026rsquo;autres que vous pouvez retrouver sur la documentation officielle.\nScripts Le block scripts contient une série de commandes à utiliser pour faciliter le développement dans votre projet. Chaque commande est définie par un nom et la commande à exécuter.\nPour utiliser un script, il suffit d\u0026rsquo;utiliser la commande npm run \u0026lt;nom du script\u0026gt;.\nDans tout projet de qualité, vous devriez retrouver la commande npm run test (ou le raccourci npm test).\nDépendances Les dépendances du runtime Le block dependencies contient l\u0026rsquo;ensemble des packages utilisés dans votre projet. Vous pouvez ajouter des dépendances via la commande npm install \u0026lt;package_name\u0026gt;.\nDans notre exemple, nous retrouvons le package react, mais vous pouvez aussi retrouver des packages de composants graphiques (antd, @mui/material, chartjs, \u0026hellip;) ou tout autre package utilisé au runtime de votre application (lodash, axios, \u0026hellip;).\nLes dépendances de développement Le block devDependencies contient l\u0026rsquo;ensemble des packages utilisés lors du cycle de développement de votre projet. Vous pouvez ajouter des dépendances via la commande npm install \u0026lt;package_name\u0026gt; -D.\nC\u0026rsquo;est une bonne pratique de séparer les dépendances entre dependencies et devDependencies. Dans le cas d\u0026rsquo;un package, seule les dependencies seront installées et le poids du package parent en sera allégé.\nDans notre exemple, nous retrouvons les packages react-scripts et @testing-library, qui vous permettent de compiler, tester ou exécuter votre projet. Vous pouvez aussi retrouver d\u0026rsquo;autres librairies de testing (jest, mocha, chai, cypress, \u0026hellip;), des librairies de types dans des projets en typescript (typescript, @types/node, @types/react, \u0026hellip;).\nD\u0026rsquo;autres dépendances Il existe aussi d\u0026rsquo;autres types de dépendances, que je n\u0026rsquo;aborderais pas ici par soucis de simplicité. Ces dernières sont peu utilisées dans le développement d\u0026rsquo;application, mais peuvent servir dans le développement de packages : peerDependencies, bundledDependencies, optionalDependencies. (plus d\u0026rsquo;info dans la doc officielle).\nVersioning Concernant le versioning des dépendances, c\u0026rsquo;est la norme semver qui est de rigueur. Ce système de notation vous permet de sélectionner de façon intelligente la version du package à utiliser.\nUne version s\u0026rsquo;écrit sous le format suivant : major.minor.patch\n major : est incrémenté en cas de breaking change; minor : nouvelle feature, rétrocompatible; patch : bug fix, rétrocompatible.  En général, il est possible de monter de version minor et patch sans risquer un changement de comportement, et en récupérant les correctifs de sécurité et bug, ainsi que les nouvelles fonctionnalités. Un changement dans la version major indique souvent un effort supplémentaire de réécriture de votre code pour s\u0026rsquo;adapter aux changements apportés.\nIl existe plusieurs façons d\u0026rsquo;écrire un ensemble de versions visées, je vous en détaille quelques-unes :\n 1.2.3: version figée, on souhaite avoir exactement cette version ^1.2.3: première version non 0 figée. Ici version major figée =\u0026gt; 1.2.3, 1.3.5\u0026hellip; ~1.2.3: version major et minor figées =\u0026gt; 1.2.3, 1.2.4, 1.2.5\u0026hellip; \u0026gt;=1.2.3: toute version supérieure ou égale =\u0026gt; 1.2.3, 1.2.5, 2.5.8 latest: la dernière version  De manière générale, j\u0026rsquo;utilise principalement la notation ~1.2.3, qui me permet de bénéficier des nouveautés et des correctifs sans introduire d\u0026rsquo;incompatibilité.\nJe vous invite à tester via ce petit site pour mieux appréhender les différentes notations et visualiser les versions qui correspondent à votre notation.\nPlus d\u0026rsquo;info dans la doc officielle\nLe fourzitout de la conf Il est courant de voir dans le package.json des blocs spécifiques permettant de configurer les outils de développement. En fonction des outils que vous utilisez, vous avez le choix d\u0026rsquo;un fichier spécifique à la racine, de variables d\u0026rsquo;environnement ou d\u0026rsquo;un bloc dans le package.json.\nUn exemple de config dans un package.json\nDans notre cas, la configuration d'eslint a été mise dans le package.json via le bloc eslintConfig, mais il était aussi possible d\u0026rsquo;avoir un fichier .eslintrc.js à la racine du projet.\nInstaller un projet : npm install Maintenant que nous avons éclairci le contenu du package.json, passons à la toute première étape lorsque l\u0026rsquo;on clone un nouveau projet : npm install.\nCette commande permet de télécharger toutes les dépendances et sous-dépendances du projet, en accord avec les versions définies dans le package.json.\nÀ chaque npm install, une résolution des versions des dépendances est calculée, les sources des dépendances sont téléchargées dans le dossier node_modules (plus de détail ici), et un fichier package-lock.json est généré/mis à jour avec l\u0026rsquo;arbre des dépendances et leur version résolue.\nUne fois cette commande lancée, vous pouvez démarrer votre projet et commencer à coder !\n⚠️ Si deux développeurs installent un même projet, il est possible de ne pas avoir le même arbre de dépendances (comme expliqué très brièvement dans la doc).\nIl est alors important de mettre en place un mécanisme afin de toujours obtenir le même résultat après installation. Ce qui nous permettra d\u0026rsquo;éviter des bugs aléatoires non reproductibles, et d\u0026rsquo;être plus serein quant au déploiement de notre projet.\nGarantir un environnement reproductible : package-lock.json Une petite minute, on parle de résolution de version, package-lock.json, mais ça correspond à quoi tout ça ?\nPour mieux comprendre, prenons l\u0026rsquo;exemple d\u0026rsquo;un petit projet, défini par ce package.json :\n{ \u0026#34;name\u0026#34;: \u0026#34;my-project\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;X\u0026#34;: \u0026#34;^1.0.1\u0026#34; } } Évolution de la version dans le temps 😖 Nous avons vu qu\u0026rsquo;à chaque npm install, les versions des dépendances sont résolues. On risque donc d\u0026rsquo;avoir des versions différentes en fonction de la date d\u0026rsquo;installation du projet :\n   Date Package X     D 1.0.2   D+5 1.0.5   D+X 1.2.1    Dans le monde parfait de semver, cela ne devrait pas poser de problèmes, mais comment éviter que 2 ans plus tard, on soit incapable d\u0026rsquo;installer le projet ?\nMauvaise idée : Et si on fige la version ? 😕 J\u0026rsquo;ai longtemps eu la superbe idée de figer la version dans le package.json en utilisant \u0026quot;X\u0026quot;: \u0026quot;1.0.2\u0026quot;. Après un npm install, j\u0026rsquo;aurai en effet toujours la même version :\n   Date Package X     D 1.0.2   D+5 1.0.2   D+X 1.0.2    Mais j\u0026rsquo;oublie quelque chose d\u0026rsquo;important ! Et les sous-dépendances alors ?\nEn considérant \u0026quot;X\u0026quot;: \u0026quot;1.0.2\u0026quot; et une sous-dépendance \u0026quot;Y\u0026quot;: \u0026quot;^2.0.3\u0026quot; définie dans le package.json de X, après de multiples npm install, on obtient le tableau de version suivant :\n   Date Package X Package Y     D 1.0.2 2.0.3   D+5 1.0.2 2.0.7   D+X 1.0.2 2.1.4    Bon on fait comment alors ? Je ne vais pas tout figer non plus, j\u0026rsquo;en aurais pour mille ans ! 🤨\nFiger les versions avec le package-lock.json Ce fichier permet de conserver l\u0026rsquo;arbre de résolution des dépendances et de leur version.\nEn considérant :\n Un package.json avec \u0026quot;X\u0026quot;: \u0026quot;^1.0.1\u0026quot;, X dépend de \u0026quot;Y\u0026quot;: \u0026quot;^2.0.0\u0026quot; Un package-lock.json qui a résolu les versions \u0026quot;X\u0026quot;: \u0026quot;1.0.2\u0026quot; et \u0026quot;Y\u0026quot;: \u0026quot;2.0.2\u0026quot;  { \u0026#34;name\u0026#34;: \u0026#34;my-project\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;lockfileVersion\u0026#34;: 2, \u0026#34;requires\u0026#34;: true, \u0026#34;packages\u0026#34;: { \u0026#34;\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my-project\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;X\u0026#34;: \u0026#34;^1.0.1\u0026#34; } }, \u0026#34;node_modules/X\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;X\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0.2\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;Y\u0026#34;: \u0026#34;^2.0.0\u0026#34; } }, \u0026#34;node_modules/Y\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Y\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2.0.2\u0026#34; } } } package-lock.json généré, plus d\u0026rsquo;informations sur la structure dans la doc officielle\nJe lance ma commande npm install et je me prépare à admirer la puissance de npm\n   Date Package X Package Y     D 1.0.2 2.0.3   D+5 1.0.5 2.0.7   D+X 1.2.1 2.1.4    Quoi ?? Toutes mes versions changent ?? Mais je ne comprends plus rien ça sert à rien ce fichier !!? En plus il est modifié à chaque npm install ! 😡\nLa commande npm ci Et oui, pour dire à npm d\u0026rsquo;installer les dépendances à partir du fichier package-lock.json, il faut utiliser une autre commande : npm ci.\n   Date Package X Package Y     D 1.0.2 2.0.3   D+5 1.0.2 2.0.3   D+X 1.0.2 2.0.3    Ouf ! On a enfin figé nos versions, si on doit faire un patch dans 2 ans on ne cassera pas tout ! 🤩\nRésumé de cette première partie  On utilise semver pour maitriser les versions de nos dépendances; npm install résout les versions et ajoute/met à jour le package-lock.json; On utilise npm ci :  Dans l\u0026rsquo;intégration continue pour garantir une pipeline reproductible; En local quand on ne veut pas modifier les versions résolues.   On commit le package-lock.json pour partager la résolution de version avec ses collègues et pour l\u0026rsquo;intégration continue On ne commit pas le node_modules, qui est construit par les commandes d\u0026rsquo;installation  Dans les articles suivants, nous découvrirons des fonctionnalités npm un peu plus poussées, et je vous présenterai des outils pour booster votre expérience développeur sur Node !\n","date":"Feb 17, 2022","href":"https://blog.talanlabs.com/npm-tout-ce-que-vous-n-avez-pas-compris-part1/","kind":"page","labs":null,"tags":["npm","Node"],"title":"npm - Tout ce que vous n'avez pas compris"},{"category":null,"content":"Une journée Ruche ? Tous les troisièmes jeudi du mois, les Labiens se retrouvent pour partager et échanger autour de sujets qu\u0026rsquo;ils choisissent. Technique ou organisationnel, sous forme de présentation ou d\u0026rsquo;atelier, c\u0026rsquo;est l\u0026rsquo;occasion de présenter des sujets qui leur tiennent à coeur ou tout simplement s\u0026rsquo;entraîner en vue d\u0026rsquo;une conférence.\nLa dernière journée Ruche s\u0026rsquo;est déroulée le 27 Janvier 2022\nVoici le résumé des différentes sessions proposées\nProjet de fin d\u0026rsquo;année SmartBuilding avec les étudiants de l\u0026rsquo;EPF Depuis la rentrée de Septembre 2021, TalanLabs a accompagné des élèves de l\u0026rsquo;EPF pour leur projet de fin d\u0026rsquo;année autour du Smart Building Talan\nCes derniers ont travaillé sur trois axes :\n détection et indication d’une pollution sonore amélioration de la gestion et occupation des salles de réunion automatisation, prédiction et amélioration la gestion du chauffage  Cette session était l\u0026rsquo;occasion de partager leur démarche, de l\u0026rsquo;idée jusqu\u0026rsquo;à la réalisation d\u0026rsquo;un prototype.\nMerci à Camille, Laurine, Julie, Elise, Thomas, Hans mais également tous les Labiens qui ont donné de leur temps\nRetour d\u0026rsquo;expérience sur le rôle de Product Owner et Scrum master Hélène témoigne de son intervention en temps que Product Owner au sein d\u0026rsquo;un grand groupe bancaire.\nAprès avoir longtemps endossé le rôle de Scrum Master, cette expérience lui a permis de se questionner sur le rôle de Product Owner vis à vis du Scrum Master. Cette session était l\u0026rsquo;occasion d\u0026rsquo;échanger sur les objectifs attendus par chacun de ces rôles au sein de l\u0026rsquo;équipe.\nS\u0026rsquo;en est suivi une riche session de débats en impliquant le speaker et tous les participants.\nEcriture d\u0026rsquo;un compilateur en Golang - seconde partie Guillaume continue de nous expliquer pas à pas l\u0026rsquo;implémentation d\u0026rsquo;un compilateur écrit en Golang.\nEn alternant présentation et live coding, la génération du bytecode a été enrichie au fur et à mesure de la session. En commençant par le stockage de littéral puis par l\u0026rsquo;ajout d\u0026rsquo;opérations arithmétiques, nous avons fini la session avec l\u0026rsquo;implémentation des conditions !\nQuelques pistes pour aller plus loin :\n gestion des variables d\u0026rsquo;une fonction en utilisant des scopes implémentation d\u0026rsquo;autres opérateurs phase d\u0026rsquo;optimisation du code (HIR)  Des souvenirs plus ou moins douloureux de cours d\u0026rsquo;assembleur pour certains.\nMerci à toutes et tous ! Les journées Ruche existent grâce à l\u0026rsquo;envie de toutes et tous les Labiens de partager et grandir ensemble.\nRendez-vous fin Février pour la 31ème journée Ruche\nSi certains sujets vous intéressent, contactez-nous\ncrédits photos: vecteezy\n","date":"Jan 31, 2022","href":"https://blog.talanlabs.com/recapitulatif-ruche-30/","kind":"page","labs":null,"tags":["ruche"],"title":"Résumé de la journée Ruche #30"},{"category":null,"content":"Récemment, j\u0026rsquo;ai proposé à une association de moderniser leur manière de réaliser leur commande groupée : fin des formulaires papiers à remplir et à déposer dans une boîte aux lettres pour passer à la dématérialisation.\nL\u0026rsquo;objectif est d\u0026rsquo;obtenir, à la fin, un fichier de type excel avec l\u0026rsquo;ensemble des éléments à commander afin de le fournir au commerçant, tout en ayant un coût nul.\nEn plus de ce fichier, contenant l\u0026rsquo;ensemble de la commande, je souhaiterais envoyer un récapitulatif à la personne ainsi que le montant de sa commande afin de limiter les erreurs de paiement (qui se fera toujours par chèque).\nFaire un formulaire en ligne, je sais faire Faire un formulaire gratuit, avec google forms, c\u0026rsquo;est très facile : je peux faire rapidement un bon de commande avec des tableaux et des boutons radios.\nForms peut également envoyer un e-mail avec le récapitulatif des réponses, c\u0026rsquo;est parti.\nPour le formulaire, je choisis :\n Champ texte pour le nom Grille à choix multiples pour les produits :  en ligne, le nom du produit. Le nom est suffixé par le prix, séparé par un - (10 chocolats - 5.3€) en colonne la quantité de 0 à 9, pour limiter les problèmes de saisie    C\u0026rsquo;est bien mais je ne peux pas personnaliser le contenu du récapitulatif ou calculer le montant de la commande.\nCe qui existe En fouillant, on trouve plusieurs extensions pour google forms permettant de gérer des bons de commande, certains directement avec paiement en ligne. Malheureusement, ces extensions sont lourdes à utiliser, peu intuitives et surtout\u0026hellip;payantes.\nEn dehors de google forms, un bon de commande gratuit avec une réponse personnalisée, impossible à trouver.\nIl n\u0026rsquo;y aurait donc pas de solution ?\nEn fait si : avec google scripts, il est possible de personnaliser l\u0026rsquo;usage de l\u0026rsquo;ensemble des outils de la suite google.\nQu\u0026rsquo;est ce que google script Dans tous les outils de la suite google, il est possible de développer ses propres scripts pour étendre les capacités des outils originaux. Ces scripts sont écrits en Javascript et exploitent la puissante API de google :\n Déclencheurs de fonction sur des événements Manipulation des fichiers du drive Accès à toutes les API google : accéder à l\u0026rsquo;agenda, à la boîte email, aux services google maps, translate\u0026hellip;  Pour accéder aux scripts, cliquer sur les 3 points verticaux -\u0026gt; éditeur de scripts\nL\u0026rsquo;écran qui s\u0026rsquo;affiche est celui de l\u0026rsquo;éditeur de fonction. A partir de cet écran, vous pouvez éxecuter une fonction et consulter toutes les exécutions. L\u0026rsquo;éditeur propose la coloration syntaxique et l\u0026rsquo;autocomplétion. Même sans typescript, il devine les types et propose les bonnes méthodes.\nSur la gauche, vous avez accès au paramétrage des déclencheurs, à la liste des executions\u0026hellip;\nIl est temps de coder L\u0026rsquo;algorithme que j\u0026rsquo;ai en tête est le suivant :\n Lorsque l\u0026rsquo;utilisateur valide le formulaire, une fonction est appelée On récupère le contenu de sa commande On récupère le prix des articles On calcule le prix total On prépare le contenu du mail qui liste la commande On envoie le mail  La documentation nous explique comment manipuler un formulaire :\nlet form = FormApp.openById(\u0026#39;mon_identifiant_de_form\u0026#39;); // Ouvre un formulaire à partir de l\u0026#39;id let lastResponse = form.getResponses()[form.getResponses().length-1]; // Dernière reponse Pour trouver votre id, il est dans l\u0026rsquo;url du formulaire : A partir de la réponse, on peut ensuite parcourir l\u0026rsquo;ensemble des questions avec :\n le nom de la question  getItem().getTitle()  le type de question  getItem().getType() Une enum permet de tester facilement le type de question :  les différentes réponses et leur valeur  getItem().asGridItem().getRows() Calcul du montant Attention : il n\u0026rsquo;y a pas de champ commentaire permettant d\u0026rsquo;enrichir le formulaire. Si l\u0026rsquo;on souhaite des infos supplémentaires, il faut les extraire du formulaire. Pour ma part, je l\u0026rsquo;extrais directement du titre de la réponse en respectant toujours le même format : \u0026lsquo;Produit ABC - 25€\u0026rsquo;.\nconst cost = parseFloat(title.split(\u0026#39; - \u0026#39;)[1].replace(\u0026#39;€\u0026#39;,\u0026#39;\u0026#39;)); Pour calculer le coût de la commande, on va parcourir les questions de type GRID et faire le calcul :\nfunction computeTotalAmount(lastResponse){ let total = 0; lastResponse.getItemResponses().forEach(ir=\u0026gt;{ // Parcours de toutes les questions  if(ir.getItem().getType() == FormApp.ItemType.GRID){ // getItem renvoie le détail de la question  let costs = ir.getItem().asGridItem().getRows().map(value=\u0026gt;parseFloat(value.split(\u0026#39; - \u0026#39;)[1].replace(\u0026#39;€\u0026#39;,\u0026#39;\u0026#39;))) ir.getResponse().forEach((val,pos)=\u0026gt;total+=costs[pos]*val) // Calcul à partir des réponses à la question  } }) return total.toFixed(2); // On garde deux décimales au prix } Envoi d\u0026rsquo;un email Le mail est envoyé à partir de la boîte de du compte de l\u0026rsquo;utilisateur. L\u0026rsquo;avantage est que chaque mail envoyé se retrouve dans les messages envoyés : une sorte d\u0026rsquo;accusé qu\u0026rsquo;un mail a bien été envoyé.\nLa documentation détaille plusieurs méthodes dont une pour envoyer du HTML et personnaliser le mail comme le nom de l\u0026rsquo;expéditeur, le noReply :\nconst messageHtml = `Un message avec mise en forme.\u0026lt;br/\u0026gt; Montant \u0026lt;b\u0026gt;${total}€\u0026lt;/b\u0026gt; à l\u0026#39;ordre de \u0026lt;b\u0026gt;moi\u0026lt;/b\u0026gt;.` + `\u0026lt;br/\u0026gt;Vous pouvez modifier votre commande avec \u0026lt;a href=\u0026#34;${lastResponse.getEditResponseUrl()}\u0026#34;\u0026gt;ce lien\u0026lt;/a\u0026gt;. \u0026lt;br/\u0026gt;${formatRecap(recap)}.\u0026lt;br/\u0026gt;\u0026lt;br/\u0026gt;L\u0026#39;équipe Talan Labs.\u0026lt;br/\u0026gt;\u0026lt;a href=\u0026#34;https://blog.talanlabs.com/\u0026#34;\u0026gt;Notre blog\u0026lt;/a\u0026gt;` MailApp.sendEmail(lastResponse.getRespondentEmail(),\u0026#39;Confirmation de commande\u0026#39;,message,{name:\u0026#39;Mon nom\u0026#39;,noReply:true,htmlBody:messageHtml}) Vous le voyez, il est également possible d\u0026rsquo;obtenir le lien de modification du formulaire, permettant de corriger sa commande.\nCet exemple donnera :\nMise en place du déclencheur La dernière étape consiste à connecter notre formulaire à notre script avec un déclencheur. L\u0026rsquo;ajout d\u0026rsquo;un déclencheur se fait dans le menu de gauche \u0026ldquo;Déclencheurs -\u0026gt; Ajouter un déclencheur\u0026rdquo;.\nVous devez ensuite choisir la fonction à appeler et le type de déclencheur (ouverture formulaire, envoi formulaire, horaire\u0026hellip;) Dans mon exemple, j\u0026rsquo;ai choisi le déclencheur formulaire / lors de l\u0026rsquo;envoi du formulaire.\nUn schéma pour résumer Pour aller plus loin Ce n\u0026rsquo;est parce qu\u0026rsquo;on développe un script qui devrait peu évoluer qu\u0026rsquo;il faut oublier les bases :\n Pensez à bien découper votre code avec des fonctions testables facilement (les plus pures possibles) Faites en sorte de pouvoir tester votre script sans avoir besoin de remplir le formulaire, par exemple en exécutant la fonction toujours sur la dernière ligne On peut imaginer modifier le fichier excel généré pour indiquer le prix final, s\u0026rsquo;il a été modifié\u0026hellip;  ","date":"Jan 26, 2022","href":"https://blog.talanlabs.com/google-form-script/","kind":"page","labs":null,"tags":["Javascript","Google Forms"],"title":"Un bon de commande gratuit avec un récapitulatif par mail"},{"category":null,"content":"Une journée Ruche ? Tous les troisièmes jeudi du mois, les Labiens se retrouvent pour partager et échanger autour de sujets qu\u0026rsquo;ils choisissent.\nTechnique ou organisationnel, sous forme de présentation ou d\u0026rsquo;atelier, c\u0026rsquo;est l\u0026rsquo;occasion de présenter des sujets qui leur tiennent à coeur ou tout simplement s\u0026rsquo;entraîner en vue d\u0026rsquo;une conférence.\nUn rendez-vous incontournable Déjà 29 journées passées ensemble depuis le premier rendez-vous.\nUn ojectif commun Partager et faire grandir !\nLes journées Ruche en 2021 Le mois de Janvier est synonyme de voeux mais c\u0026rsquo;est aussi l\u0026rsquo;occasion de partager tout ce qui a été fait l\u0026rsquo;année passée\nDes speakers motivés Une vingtaine de speakers ont présenté un ou parfois plusieurs sujets ou ateliers\nDu temps passé ensemble Plus de 45 heures de présentations ou ateliers\nPlus de 600 participants en cumulé\nDes sujets variés  De java 11 à java 17: Quoi de neuf entre ces deux LTS? Présentation SUN et travaux de thèse Unity 3d c\u0026rsquo;est pas compliqué Blockchain : Créez votre propre application de vote décentralisée NPM - Tout ce que vous n\u0026rsquo;avez pas compris​ Write your own programming language - episode 1: the interpreter Blockchain : focus sur Ethereum, Quorum, Corda et Hyperledger Fabric Pourquoi Webassembly Aka Wasm ? Petit guide pratique pour commencer un design system Initiation au sketchnoting Flutter - introduction et gestion des états. CupCarbon. Blockchain : introduction aux buzzwords Vendez-nous une licorne ! Ou comprendre les biais cognitifs 🦄 Notre expérience avec la programmation réactive REX d\u0026rsquo;un produit web en Agile : SellingProcess Création d\u0026rsquo;une application iPhone en swiftui Présentation : Comment démarrer un nouveau produit en Agile Scrum ? Kotlin - Un Java bien plus moderne RSA - Alice \u0026amp; Bob jouent avec des clefs Je suis une petite souris 🐭 ou l\u0026rsquo;eXpérience speakeuse Atelier: comprendre l\u0026rsquo;Infra as code avec Terraform + aws Fintech : sujet de thèse Design vers le futur : petite histoire d’une grande typographie incomprise Les structures de données classiques, qu\u0026rsquo;on retrouve dans tous les langages de programmation. AWS : Présentation et pratique REX POC api microsoft 365 Comment j\u0026rsquo;ai eu une PS5 / Comment je fais de l\u0026rsquo;achat-revente sur fifa 21 grâce à un bot en python Les outils collaboratifs pour travailler à distance Entreprise résiliente - Back to the basics of Agility (Les frameworks ne forment pas à l\u0026rsquo;agilité) Cloud : Le réseau et les bonnes pratiques  Si certains sujets vous intéressent, contactez-nous\nUn grand Merci ! Les journées Ruche existent grâce à l\u0026rsquo;envie de partager et grandir ensemble de tous les Labiens\nMerci à toutes et à tous pour cette belle année 2021 !\nRendez-vous fin Janvier pour la 30ème Ruche\ncrédits photos: vecteezy\n","date":"Jan 24, 2022","href":"https://blog.talanlabs.com/recapitulatif-ruches-2021/","kind":"page","labs":null,"tags":["ruche"],"title":"Récapitulatif des journées Ruche en 2021"},{"category":null,"content":"Depuis le mois de d\u0026rsquo;août 2020, Docker a instauré une limite de 100 pull que l\u0026rsquo;on peut faire gratuitement toutes les 6h. Comme chaque job Gitlab-CI nécessite le pull d\u0026rsquo;une image Docker, on peut vite se retrouver à dépasser cette limite, ce qui fera échouer vos jobs avec une erreur comme celle-ci :\nERROR: Preparation failed: failed to pull image \u0026quot;node:12.19\u0026quot; with specified policies [always]: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit (manager.go:205:1s) Afin de remédier à ce désagrément, il est possible de mettre à profit le Dependency Proxy de Gitlab. Il fonctionne comme un proxy local, qui permet de retourner une image en cache lorsqu\u0026rsquo;il reçoit une requête\nIl est utilisable depuis la version 11.11 et est activé par défaut.\nVoir la documentation ici\nDéfinir l\u0026rsquo;image d\u0026rsquo;un job Il suffit de préfixer l\u0026rsquo;image choisie pour le job avec l\u0026rsquo;url du proxy, par exemple :\n# .gitlab-ci.yml image: ${CI_DEPENDENCY_PROXY_GROUP_IMAGE_PREFIX}/alpine:latest Attention : les images doivent être présentes dans le Docker Hub pour que ça fonctionne\nUtiliser Docker dans le script d\u0026rsquo;un job Si vos jobs utilisent des commandes Docker dans leurs scripts, par exemple pour builder une image Docker ou faire un push vers un registry Docker, vous risquez de voir apparaître une erreur 403 lors de leur execution. Pour éviter cela il suffit de faire un docker login avant toute commande docker.\ndocker login ${CI_DEPENDENCY_PROXY_SERVER} --username ${CI_DEPENDENCY_PROXY_USER} --password ${CI_DEPENDENCY_PROXY_PASSWORD} Les variables d\u0026rsquo;environnement utilisées ici sont fournies au job par Gitlab CI, il n\u0026rsquo;est donc pas nécessaire de se créer un user ou de récupérer un token.\nIl est possible que vos scripts fonctionnent sans authentification. En effet, d\u0026rsquo;après la documentation Gitlab, l\u0026rsquo;authentification est nécessaire seulement depuis version 13.7.\nPhoto by Ian Taylor on Unsplash\n","date":"Nov 18, 2021","href":"https://blog.talanlabs.com/fix-docker-pull-limit/","kind":"page","labs":null,"tags":["Docker","Gitlab"],"title":"Gitlab Dependency Proxy vs Docker pull limit"},{"category":null,"content":"Après la présentation générale de Spring REST Docs, je vous propose d’aller un peu plus loin en nous penchant sur des petites améliorations qui feront toute la différence !\n Nous continuerons de nous baser sur mon projet de démonstration disponible sur GitHub.\n Exposer \u0026#34;automagiquement\u0026#34; la documentation Nous l’avons vu : nous sommes désormais capables de générer une page HTML contenant la documentation de notre API. Mais ce n’est pas le meilleur moyen de la rendre disponible aux consommateurs de l’API, nous pouvons faire mieux que ça !\n Et quel meilleur emplacement que l’application Spring Boot en elle-même ? Pour cela, il faut un peu de paramétrage, grâce au plugin Maven maven-resources-plugin.\n \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-resources-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;copy-resources\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;prepare-package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;copy-resources\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;outputDirectory\u0026gt;${project.build.outputDirectory}/static/docs/\u0026lt;/outputDirectory\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;directory\u0026gt;${project.build.directory}/generated-docs\u0026lt;/directory\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;   Ce code permet de copier le contenu du répertoire target/generated-docs dans un répertoire target/static-docs/docs. Autrement dit la page HTML générée dans un dossier un peu particulier, puisque static est automatiquement servi par Spring. Si vous voulez en savoir plus sur cette caractéristique, Baeldung a évidemment une page dédiée !\n Avec cette configuration, si vous lancez la construction de l’application (mvn package), vous obtenez un fichier JAR qu’il suffit de lancer (java -jar demo-spring-rest-docs-0.0.1-SNAPSHOT.jar). L’API est alors exposée, mais aussi la documentation, accessible sur http://localhost:8080/docs/index.html. Donc dès que l’application sera déployée quelque part, sa documentation sera présente !\n   Cacher par défaut les trop grands éléments Parfois nous devons documenter des endpoints qui renvoient des réponses relativement longues. Ce qui n’est pas sans alourdir la page de documentation. Heureusement, AsciiDoc a une balise pour résoudre le problème : [%collapsible].\n Tout ce qui est contenu dans un bloc ainsi annoté est caché par défaut et le lecteur doit cliquer dessus pour le déplier :\n  Élément trop grand (cliquez ici pour le déplier) [%collapsible] ==== Lorem ipsum dolor sit amet, consectetur adipiscing elit. Phasellus eget mollis neque. Etiam tempor lacinia lorem eget auctor. Quisque accumsan leo a tincidunt hendrerit. Nam eros ante, scelerisque eu tempus et, vestibulum luctus turpis. Donec id nisi risus. Nullam eu purus vulputate velit pharetra hendrerit. Donec varius, velit vitae aliquam interdum, dui sapien faucibus ipsum, et sollicitudin ligula magna quis velit. Donec luctus sed nisi ac blandit. Phasellus sodales mattis pharetra. Duis dignissim tellus nibh, quis imperdiet turpis pharetra et. Phasellus purus odio, pulvinar vel urna vel, consequat vulputate metus. Nunc elementum ornare eleifend. Pellentesque non dapibus ipsum. Nunc malesuada varius elit, auctor tristique nisl pellentesque sed. Vestibulum justo mauris, molestie ut tincidunt a, condimentum ac turpis. Aliquam eu interdum orci. ====     C’est ainsi que dans la documentation de notre application de démonstration nous retrouvons par exemple :\n .Response [%collapsible] ==== include::{snippets}/getAllCompanies/http-response.adoc[] ====     Alléger la description des requêtes/réponses Pour décrire la requête liée à un endpoint, rien de mieux que le snippet http-request. Celui-ci contient l’URL, le verbe HTTP, les headers et éventuellement le body.\n Mais bien souvent, les headers sont nombreux. Notamment ceux liés à la sécurité. Et il en va de même pour la réponse, avec de nombreux headers ajoutés par le framework et son outillage.\n Si ces headers ont un sens métier, tant mieux. Mais s’ils ne font que créer du \u0026#34;bruit\u0026#34;, alors autant les enlever de la description (de la requête comme de la réponse), elle n’en sera que plus claire.\n Je vous propose ainsi un petit utilitaire (ControllerTestUtils) pour éviter de répéter dans chaque test la même suppression de headers :\n public class ControllerTestUtils { static OperationRequestPreprocessor preprocessRequest() { return Preprocessors.preprocessRequest(removeHeaders(\u0026#34;Content-Length\u0026#34;, \u0026#34;X-CSRF-TOKEN\u0026#34;), prettyPrint()); } static OperationResponsePreprocessor preprocessResponse() { return Preprocessors.preprocessResponse(removeHeaders(\u0026#34;Content-Length\u0026#34;, \u0026#34;Pragma\u0026#34;, \u0026#34;X-XSS-Protection\u0026#34;, \u0026#34;Expires\u0026#34;, \u0026#34;X-Frame-Options\u0026#34;, \u0026#34;X-Content-Type-Options\u0026#34;, \u0026#34;Cache-Control\u0026#34;), prettyPrint()); } }   Libre à vous de supprimer tel ou tel header selon vos goûts ou vos besoins.\n Si l’on prend l’exemple du header Content-Type dans la requête de suppression d’une Company, il suffit de l’ajouter à la liste des headers à supprimer (cf. code ci-dessus).\n Avant : DELETE /companies/ID_1 HTTP/1.1 Content-Type: application/json;charset=UTF-8 Host: localhost:8080 { \u0026#34;id\u0026#34; : \u0026#34;ID_1\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;CoolCorp\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;Paris\u0026#34;, \u0026#34;creationDate\u0026#34; : \u0026#34;2021-11-06T11:03:53.066+00:00\u0026#34; }   Après : DELETE /companies/ID_1 HTTP/1.1 Host: localhost:8080 { \u0026#34;id\u0026#34; : \u0026#34;ID_1\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;CoolCorp\u0026#34;, \u0026#34;location\u0026#34; : \u0026#34;Paris\u0026#34;, \u0026#34;creationDate\u0026#34; : \u0026#34;2021-11-06T11:01:34.532+00:00\u0026#34; }     Générer un fichier OpenAPI  \u0026#34;Hé, c’est bien joli tout ça, mais dans mon équipe on a l’habitude d’utiliser Swagger, c’est bien plus complet comme outil, on peut même lancer des requêtes depuis la page de documentation !\u0026#34;  — Un développeur qui a ses petites habitudes   La remarque est intéressante ! En quittant Swagger, on passe d’une page interactive à une page HTML complètement statique. On perdrait donc au change ? Peut-être …​ mais ne parlons pas trop vite !\n Allons droit au but : il est tout à fait possible de générer un fichier décrivant l’API dans un format \u0026#34;Swagger-compatible\u0026#34;, autrement dit au format OpenAPI. Mais pas par défaut, nous devons ajouter une petite dépendance qui vient étendre les capacités de Spring REST Docs :\n \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.epages\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;restdocs-api-spec-mockmvc\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.14.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;   Et lorsque l’on documente un endpoint, il faut désormais importer la méthode document() de cette dépendance :\n import static com.epages.restdocs.apispec.MockMvcRestDocumentationWrapper.document;   En relançant les tests, on s’aperçoit alors qu’un nouveau \u0026#34;snippet\u0026#34; est généré : resource.json.\n   Il s’agit d’un snippet au format OpenAPI. Il ne reste donc plus qu’à rassembler ces snippets en un seul fichier, un peu comme nous savons déjà le faire pour la version AsciiDoc. Mais cette fois-ci, nul besoin d’un \u0026#34;fichier racine\u0026#34;, nous allons nous servir d’un plugin.\n Mais …​ problème en vue ! La dépendance ajoutée ne propose qu’un plugin Gradle et notre projet utilise Maven ! Pas de panique, la communauté est vaste et prévoyante, il existe un plugin pour générer la documentation finale : restdocs-spec-maven-plugin.\n En l’ajoutant dans la section \u0026#34;build\u0026#34; du fichier pom.xml, nous allons pouvoir générer un fichier au format OpenAPI en plus du fichier HTML précédent :\n \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;io.github.berkleytechnologyservices\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;restdocs-spec-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.21\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;generate\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;name\u0026gt;Demo Spring REST Docs\u0026lt;/name\u0026gt; \u0026lt;version\u0026gt;${project.version}\u0026lt;/version\u0026gt; \u0026lt;host\u0026gt;localhost:8080\u0026lt;/host\u0026gt; \u0026lt;outputDirectory\u0026gt;${project.build.directory}/generated-docs\u0026lt;/outputDirectory\u0026gt; \u0026lt;filename\u0026gt;openapi\u0026lt;/filename\u0026gt; \u0026lt;specification\u0026gt;OPENAPI_V3\u0026lt;/specification\u0026gt; \u0026lt;description\u0026gt;API description for Demo Spring REST Docs service\u0026lt;/description\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt;   De la même manière que pour le fichier HTML, comme ce fichier openapi.yml est déposé dans un répertoire exposé, il sera accessible une fois l’application lancée. Et à partir de ce moment-là, libre à vous de le fournir comme point d’entrée à une instance de Swagger UI.\n Pour cela, on peut tester rapidement la qualité du fichier généré en démarrant une instance de Swagger UI :\n docker run -p 80:8080 swaggerapi/swagger-ui   Une fois le container démarré, il suffit de se rendre sur http://localhost (Swagger UI fonctionnant sur le port 80 d’après notre commande ci-dessus), et de fournir l’adresse du fichier openapi.yml dans la barre de recherche en haut de la page. La documentation apparait …​ avec tout le fonctionnement habituel de Swagger. Alors, il est satisfait le développeur qui a ses petites habitudes ?\n    Alors, est-ce que vous commencez à être convaincu par Spring REST Docs ?\n C’est en tout cas le cas pour ma part et je vais continuer de le déployer sur les projets sur lesquels j’ai le plaisir de travailler !\n  Liens utiles :\n   Comparaison de Spring REST Docs et OpenAPI par Baeldung\n  Mon projet de démonstration\n    Les autres articles de cette série :\n   Partie 1 : Introduction à la notion de \u0026#34;Documentation as Code\u0026#34;\n  Partie 2 : Présentation de Spring REST Docs\n     ","date":"Nov 15, 2021","href":"https://blog.talanlabs.com/make-documentation-great-again-bonus/","kind":"page","labs":null,"tags":["AsciiDoc","Spring REST Docs","Java","Spring Boot"],"title":"Make documentation great again (Bonus)"},{"category":null,"content":"Nous avons vu précédemment une présentation du langage AsciiDoc et des avantages de la \u0026#34;Documentation as Code\u0026#34;.\n Il est néanmoins possible de faciliter encore plus la rédaction de la documentation ! En effet, il est de coutume de documenter l’API que l’on développe, pour faciliter sa maintenance et les interactions avec ses \u0026#34;consommateurs\u0026#34;.\n Même si l’une des 4 valeurs de l’agilité promeut \u0026#34;un logiciel fonctionnel plutôt qu’une documentation exhaustive\u0026#34;, nous allons voir que l’on peut proposer les deux en même temps grâce à un outil particulièrement pratique.\n Spring REST Docs Spring REST Docs permet d’alléger grandement la rédaction d’une documentation d’API, en combinant une rédaction manuelle et l’injection de sections autogénérées.\n Il s’appuie sur Spring MVC Test, largement utilisé pour les tests de la couche \u0026#34;web\u0026#34; d’une application Spring. Autrement dit, son ajout sur un projet déjà entamé ne demandera pas une refonte complète de ses tests …​ et c’est évidemment une bonne chose !\n   Un projet de démonstration De manière à nous baser sur un exemple concret, je vous propose un petit projet de démonstration disponible sur GitHub.\n Dans ces cas-là, inutile d’imaginer un cas d’usage très compliqué, j’ai donc mis en place un simple CRUD manipulant un objet Company. Une Company est définie par un ID, un nom, un lieu et une date de création.\n Pour ressembler à un vrai projet, j’ai tout de même ajouté une interaction avec une base MongoDB qu’il faut démarrer avant de lancer l’application. Pour ajouter une Company, on peut donc faire une requête POST avec comme body :\n { \u0026#34;name\u0026#34;: \u0026#34;CoolCorp\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;Paris\u0026#34;, \u0026#34;creationDate\u0026#34;: \u0026#34;2021-10-29\u0026#34; }     Ajouter la dépendance nécessaire Avant tout, il convient d’ajouter la dépendance \u0026#34;Spring REST Docs\u0026#34; au projet :\n Extrait du fichier pom.xml : \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.restdocs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-restdocs-mockmvc\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt;   Quant au reste des dépendances constituant cette application, inutile de les lister ici, elles sont très classiques et indépendantes de l’outil générant la documentation.\n   Modifier ses tests de Controller pour générer de la documentation Il est alors possible de regarder les tests et d’y ajouter des instructions dédiées à générer les éléments qui nous intéressent. Concentrons sur le test de la requête permettant de récupérer une Company à partir de son ID.\n À noter que je me suis contenté de mocker la couche de service pour ces tests de controller, il ne s’agit donc clairement pas de tests end-to-end.\n Extrait de CompanyControllerTest : @Test void getCompany() throws Exception { final String ID = \u0026#34;ID_1\u0026#34;; when(companyService.getCompany(ID)).thenReturn(company1); this.mockMvc.perform(RestDocumentationRequestBuilders.get(\u0026#34;/companies/{id}\u0026#34;, ID)) // (1) .andExpect(handler().handlerType(CompanyController.class)) // (2) .andExpect(handler().methodName(\u0026#34;getCompany\u0026#34;)) .andDo(print()) .andExpect(status().isOk()) .andExpect(content().json(objectMapper.writeValueAsString(company1))) .andDo(document( // (3) \u0026#34;getCompany\u0026#34;, ControllerTestUtils.preprocessRequest(), ControllerTestUtils.preprocessResponse(), pathParameters(parameterWithName(\u0026#34;id\u0026#34;).description(\u0026#34;The requested company id\u0026#34;)), // (4) responseFields( // (5) fieldWithPath(\u0026#34;id\u0026#34;).description(\u0026#34;The company unique ID\u0026#34;), fieldWithPath(\u0026#34;name\u0026#34;).description(\u0026#34;The company name\u0026#34;), fieldWithPath(\u0026#34;location\u0026#34;).description(\u0026#34;The company location\u0026#34;), fieldWithPath(\u0026#34;creationDate\u0026#34;).description(\u0026#34;The company creation date\u0026#34;)))); }     Première nouveauté : on utilise un RestDocumentationRequestBuilders pour émettre la requête plutôt qu’un traditionnel MockMvcRequestBuilders. La différence ? Il est chargé de capturer les informations de la requête pour la documenter.\n  On déroule les tests habituels : vérifier que la requête est traitée par la méthode getCompany() du CompanyController, que la réponse porte le code HTTP 200 et contient l’objet prévu.\n  Nouvelle instruction ! Ici on lance la génération de documentation, sous le nom de \u0026#34;getCompany\u0026#34; pour cette requête. Oublions les 2 lignes suivantes pour le moment.\n  Documentons la partie variable de l’URL : l’ID de la Company est passé en \u0026#34;path parameter\u0026#34;, nous pouvons le décrire (même si dans notre cas ce paramètre est assez facilement compréhensible).\n  Documentons la réponse de la requête : nous pouvons décrire chaque champ constituant la Company.\n   Désormais, si vous ajoutez un champ dans l’objet Company sans le documenter dans ce test, il ne passera plus. Idem si vous supprimez un champ, le test ne passera plus, car il cherchera à documenter un élément qui n’existe pas. Il s’agit donc d’une nouvelle sécurité : votre API ne pourra plus évoluer sans que vous en ayez totalement conscience !\n Nous avons donc un test qui documente le cas passant le plus évident : lorsqu’une Company correspond à l’ID demandé est trouvée. Lançons-nous dans un nouveau test pour le cas contraire.\n Extrait de CompanyControllerTest avec le test d’une erreur @Test void getCompanyNotFound() throws Exception { final String ID = \u0026#34;ID_3\u0026#34;; when(companyService.getCompany(ID)).thenThrow(new CompanyNotFoundException()); this.mockMvc.perform(RestDocumentationRequestBuilders.get(\u0026#34;/companies/{id}\u0026#34;, ID)) .andExpect(handler().handlerType(CompanyController.class)) .andExpect(handler().methodName(\u0026#34;getCompany\u0026#34;)) .andDo(print()) .andExpect(status().isNotFound()) .andDo(document( \u0026#34;getCompanyNotFound\u0026#34;, // (1) ControllerTestUtils.preprocessRequest(), ControllerTestUtils.preprocessResponse())); }     Comme dans le test précédent, on ajoute l’instruction document(…​), cette fois-ci avec un nouvel identifiant (\u0026#34;getCompanyNotFound\u0026#34;) pour différencier la documentation générée dans ce nouveau cas.\n   En revanche, il n’est pas ici utile de générer plus de documentation, dans la mesure où la description du \u0026#34;path parameter\u0026#34; a déjà été faite dans le test précédent, et où la requête ne renvoie rien d’autre qu’une erreur HTTP 404.\n En lançant les 2 tests que nous venons de voir, des fichiers (on parle de \u0026#34;snippets\u0026#34;) vont être générés sous target/generated-snippets. Et comme par hasard, il s’agit de fichiers .adoc !\n  Figure 1. Aperçu des fichiers générés  Si l’on ouvre par exemple getCompany/response-fields.adoc, on pourra y trouver :\n |=== |Path|Type|Description |`+id+` |`+String+` |The company unique ID |`+name+` |`+String+` |The company name |`+location+` |`+String+` |The company location |`+creationDate+` |`+String+` |The company creation date |===   Disons-le : même si l’on voit bien que l’on retrouve des éléments saisis dans le test, ce n’est guère lisible…​ Et puis il n’y a pas moins de 14 fichiers générés pour deux petits tests, qui lirait ça ?!\n   Il va donc être temps de générer une documentation plus lisible !\n   Le fichier source de la documentation Un fichier pour les gouverner tous. C’est en tout cas l’objectif que nous devons nous fixer pour rendre viable notre documentation.\n Nous allons donc ajouter un fichier sous source/asciidoctor : index.adoc. Et c’est là que nous injecterons nos \u0026#34;snippets\u0026#34; (et uniquement ceux qui nous intéressent), avec un peu de texte ajouté manuellement pour rendre l’ensemble plus digeste.\n Extrait du fichier index.adoc qui deviendra la documentation === Get one company // (1) .Request \\include::{snippets}/getCompany/http-request.adoc[] // (2) .Path parameters \\include::{snippets}/getCompany/path-parameters.adoc[] // (3) .Response \\include::{snippets}/getCompany/http-response.adoc[] // (4) .Response fields \\include::{snippets}/getCompany/response-fields.adoc[] // (5) .Response if the company was not found \\include::{snippets}/getCompanyNotFound/http-response.adoc[] // (6)     Donnons un petit titre à cette partie de la documentation pour décrire le endpoint documenté en dessous.\n  On injecte un snippet contenant la requête, c’est probablement la meilleure représentation de celle-ci.\n  On injecte un snippet contenant la description du \u0026#34;path parameter\u0026#34;.\n  Et le snippet qui illustre la réponse reçue.\n  N’oublions pas le snippet qui décrit les champs de la réponse.\n  Nous avions deux tests sur cet endpoint, pensons à montrer à quoi ressemble la réponse dans le cas où la Company n’est pas trouvée.\n   Pour le moment nous avons juste créé un squelette de documentation. Selon votre IDE, peut-être que vous apercevez déjà un rendu !\n De manière à améliorer la documentation, vous pouvez évidemment ajouter une table des matières, un titre principal, etc. Il s’agit là de fonctionnalités et annotations de base d’AsciiDoc, je vous propose de ne pas nous y attarder. Vous pouvez avoir une idée de ce que l’on peut faire en regardant le fichier index.adoc du projet de démonstration.\n   Générer la documentation Maintenant que nous savons générer des \u0026#34;snippets\u0026#34; et que nous pouvons les rassembler en un seul fichier, il est temps d’automatiser la génération de la page HTML qui contiendra toute la documentation.\n Pour cela, nous allons nous appuyer sur un plugin Maven : asciidoctor-maven-plugin.\n La configuration à ajouter dans le fichier pom.xml \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.asciidoctor\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;asciidoctor-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;generate-docs\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;prepare-package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;process-asciidoc\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;backend\u0026gt;html\u0026lt;/backend\u0026gt; \u0026lt;doctype\u0026gt;book\u0026lt;/doctype\u0026gt; \u0026lt;attributes\u0026gt; \u0026lt;snippets\u0026gt;${project.build.directory}/generated-snippets\u0026lt;/snippets\u0026gt; \u0026lt;/attributes\u0026gt; \u0026lt;logHandler\u0026gt; \u0026lt;outputToConsole\u0026gt;true\u0026lt;/outputToConsole\u0026gt; \u0026lt;failIf\u0026gt; \u0026lt;severity\u0026gt;DEBUG\u0026lt;/severity\u0026gt; \u0026lt;/failIf\u0026gt; \u0026lt;/logHandler\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.restdocs\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-restdocs-asciidoctor\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring-restdocs.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.jacoco\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jacoco-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${jacoco-maven-plugin.version}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;prepare-agent\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;prepare-agent\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;generate-report\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;verify\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;report\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt;   Ce code permet de générer une page HTML en piochant dans le répertoire target/generated-snippets, tout en bloquant le build en cas d’erreur rencontrée (snippet manquant, etc.).\n En exécutant une commande comme mvn package, les tests seront exécutés, donc les snippets générés et finalement la page index.html sera créée à partir du fichier index.adoc. Et on peut la retrouver dans target/generated-docs.\n  Figure 2. Aperçu de la page HTML générée   Nous avons donc vu comment générer une page HTML décrivant les endpoints d’une application Spring Boot grâce à Spring REST Docs, sans refactorer tous les tests de controller.\n L’outil est assez complet et il resterait encore plein de petits détails à montrer pour être plus exhaustif. Mais je vous propose d’attendre un article présentant des bonus !\n  Liens utiles :\n   Documentation de Spring REST Docs\n  Un projet d’exemple fourni par Spring\n  Mon projet de démonstration\n    Les autres articles de cette série :\n   Partie 1 : Introduction à la notion de \u0026#34;Documentation as Code\u0026#34;\n  Partie 3 : Aller plus loin avec Spring REST Docs\n     ","date":"Nov 8, 2021","href":"https://blog.talanlabs.com/make-documentation-great-again-part-2/","kind":"page","labs":null,"tags":["AsciiDoc","Spring REST Docs","Java","Spring Boot"],"title":"Make documentation great again (2/2)"},{"category":null,"content":"Vous aussi vous adorez les bandeaux de consentement que l’on trouve désormais sur (presque) tous les sites et vous rêvez d’en avoir un sur le vôtre ? C’est normal, ça fait tellement plus sérieux…​ Et puis, c’est aussi ça respecter les règles !\n RGPD, le vrai Cookie Monster Qu’est-ce qu’un cookie ? C’est un petit fichier assez simple déposé dans notre navigateur lorsque nous visitons certains sites. Notamment pour nous identifier lors de la prochaine visite. Ce qui peut parfois se révéler assez pratique : le site peut ainsi connaître à l’avance nos préférences d’affichage par exemple.\n Mais lorsqu’une régie publicitaire commune à plusieurs sites se permet de tracer nos habitudes d’un site à l’autre, par exemple pour nous proposer des publicités sur Amazon en rapport avec nos pages Facebook suivies …​ l’aspect pratique s’efface derrière l’aspect effrayant. Nos données personnelles ne sont plus personnelles, alors même que l’on ne nous a pas demandé notre avis.\n C’est pour combattre cette dérive que le RGPD demande de recueillir le consentement des utilisateurs avant d’exploiter leurs données personnelles.\n    RGPD : rappel rapide Le règlement général sur la protection des données est un texte du Parlement européen qui a pour but d’améliorer la protection des données des personnes, notamment en responsabilisant les acteurs du traitement de ces données.\n Il donne par ailleurs plus de pouvoirs aux autorités comme la CNIL, en charge de faire respecter les directives européennes transposées en droit français.\n  La conséquence Il en résulte une conséquence directe assez visible lorsque l’on navigue sur Internet : notre consentement autour des cookies est demandé à tout bout de champ.\n Et bien souvent la question est difficile à ignorer…​ Bandeau envahissant, popup au premier plan, tous les moyens sont bons pour détourner notre attention de l’information que l’on veut consulter. En améliorant la confidentialité de nos données, le RGPD a aussi considérablement détérioré notre expérience du web.\n   Le RGPD ne se résume pas qu’au consentement autour des cookies, mais l’objectif ici n’est pas de lister les tenants et les aboutissants d’un texte de loi de plus de 80 pages.\n    La difficulté de faire les choses proprement Mais avant de jeter la pierre aux designers ou aux développeurs qui n’ont pas eu le choix, essayons de comprendre les difficultés qui résident dans la mise en œuvre du recueil du consentement des utilisateurs.\n Lister les cookies La première étape consiste à lister les cookies utilisés par le site.\n Bien souvent on retrouve un (ou plusieurs !) cookie(s) permettant la mesure d’audience, comme ceux déposés par Google Analytics ou ses concurrents. Mais il faut aussi penser à tous les services tiers utilisés…​\n C’est ainsi que sur ce blog, nous utilisons par exemple Disqus pour gérer le système de commentaires.\n Il ne faut pas non plus oublier l’intégration de lecteurs vidéo comme YouTube qui apportent leur lot de cookies, l’utilisation de fonctionnalités de réseaux sociaux, etc.\n  Rendre les cookies optionnels Nous l’avons dit, de manière à être conforme aux recommandations de la CNIL, le site ne doit pas déposer de cookie sans le consentement du visiteur.\n Cela signifie donc qu’il faut gérer 3 états :\n  Le consentement n’a ni été donné, ni été refusé, donc les cookies ne doivent pas être déposés\n  Le consentement a été refusé, donc les cookies n’ont pas été déposés et le site doit tout de même fonctionner normalement\n  Le consentement a été accordé, donc les cookies peuvent être déposés\n   Il faut donc à tout moment savoir si le consentement a été accordé et être capable de désactiver des fonctionnalités le cas échéant. Ce qui ne manque pas d’avoir un impact direct sur le code source de la page concernée…​\n On se retrouve alors avec des petits morceaux de code retouchés qui ressemblent à cela :\n if (isGoogleConsentGiven) { // Add Google Analytics } if (isDisqusConsentGiven) { // Add Disqus }    Ne pas (trop) dégrader l’expérience utilisateur On l’a vu plus haut, même en ayant refusé les cookies (ou en tout cas certains), l’utilisateur doit pouvoir continuer sa navigation le plus normalement possible. Il convient donc de vérifier que les fonctionnalités au cœur de notre page ne sont pas impactées par l’absence d’un service.\n Par exemple, une page qui liste des vidéos YouTube sous forme de vignettes …​ sans YouTube …​ ça rend tout de suite moins bien. Il faut donc à minima prévoir un petit message pour expliquer l’apparence surprenante de la page.\n De plus, les bannières et autres encarts demandant le consentement viennent s’ajouter en superposition de pages souvent déjà bien remplies et complexifient parfois beaucoup trop la navigation. Même s’il y a du mieux ces derniers mois suite aux rappels à l’ordre de la CNIL, il est encore difficile de systématiquement refuser les cookies avec des boutons \u0026#34;ACCEPTER\u0026#34; bien trop mis en avant.\n Au-delà de l’aspect technique, il y a donc aussi des notions d’UX à prendre en compte pour éviter de retourner au web d’il y a 20 ans…​\n      La solution : tarteaucitron Nous l’avons vu, cette démarche de mise en conformité est donc relativement longue et complexe. Et heureusement, il est temps de présenter l’outil qui pourra grandement vous faciliter la tâche, Tarteaucitron !\n Sans le savoir, il est probable que vous avez déjà utilisé Tarteaucitron, utilisé sur de nombreux sites (plus de 20 000 revendiqués sur le site de l’éditeur).\n C’est un outil qui se veut le plus générique possible, avec une version gratuite open-source et une version payante proposant une mise à jour continue ainsi qu’un plugin WordPress. Il consiste en une librairie JavaScript disponible via npm, autrement dit très facilement installable.\n Il convient alors de l’intégrer à votre page :\n \u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;/tarteaucitron/tarteaucitron.js\u0026#34;/\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; tarteaucitron.init({ // Configuration }); \u0026lt;/script\u0026gt;   La configuration porte sur tous les aspects de l’outil : position de la bannière, affichage ou non des différents boutons de choix, etc. Il vous faudra vous retrousser les manches pour modifier l’apparence de la bannière, de l’icône et de l’écran de choix, mais rien de très compliqué.\n  Aperçu de la bannière sur ce site, en plein écran  Ensuite, service par service, vous allez pouvoir remplacer votre intégration actuelle par celle de Tarteaucitron. Par exemple, dans le cas de Google Analytics, vous aviez l’habitude de l’ajouter comme ceci :\n \u0026lt;!-- Global site tag (gtag.js) - Google Analytics --\u0026gt; \u0026lt;script async src=\u0026#34;https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXX\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag(\u0026#39;js\u0026#39;, new Date()); gtag(\u0026#39;config\u0026#39;, \u0026#39;UA-XXXXXXX\u0026#39;); \u0026lt;/script\u0026gt;   Il faut désormais remplacer ce morceau de code par :\n \u0026lt;!-- Cookies management for Google Analytics --\u0026gt; \u0026lt;script type=\u0026#34;text/javascript\u0026#34;\u0026gt; tarteaucitron.user.gtagUa = \u0026#39;UA-XXXXXXX\u0026#39;; (tarteaucitron.job = tarteaucitron.job || []).push(\u0026#39;gtag\u0026#39;); \u0026lt;/script\u0026gt;   À tout moment, l’utilisateur peut revenir sur ses choix, par défaut en cliquant sur l’icône située en bas à droite de l’écran comme sur ce site. Il accède alors à l’écran détaillé de choix :\n  Aperçu de l’écran de choix  De même, en naviguant sur le site, il peut se retrouver face à une fonctionnalité désactivée en l’absence de cookie, Tarteaucitron va automatiquement lui proposer d’accepter les cookies associés s’il veut s’en servir :\n  Aperçu d’une fonction de commentaire désactivée en l’absence de consentement autour de Disqus  En résumé, Tarteaucitron nous a permis de surmonter les difficultés listées plus haut :\n  En intégrant par défaut plus d’une centaine de services couramment utilisés, il permet de répondre à la grande majorité des cas que l’on peut lister sur son site\n  Il gère le cas du consentement accordé au non, service par service\n  Il évite de dégrader l’expérience utilisateur en proposant des bannières simples et en remplaçant proprement les encarts refusés\n   Par sa facilité d’installation et de configuration, nul doute que Tarteaucitron continuera de gagner de nombreux utilisateurs dans les prochaines années …​ pour le plus grand bien des internautes !\n   ","date":"Oct 27, 2021","href":"https://blog.talanlabs.com/mettez-facilement-site-conformite-rgpd/","kind":"page","labs":null,"tags":["RGPD","cookies","tarteaucitron"],"title":"Mettez (facilement) votre site en conformité avec RGPD !"},{"category":null,"content":"Qu\u0026rsquo;est-ce que Tailwind ? Tailwind est un framework CSS utilisant l\u0026rsquo;approche Utility first. C\u0026rsquo;est-à-dire l\u0026rsquo;isolement de chaque propriété css avec sa valeur dans une classe unique.\nPar exemple on aura la classe m-0 qui correspondra à la classe appliquant une marge de 0 :\n.m-0 { margin: 0; } Voici un exemple de comment utiliser tailwind pour créer un composant :\n\u0026lt;div class=\u0026#34;grid grid-cols-10 h-40 w-96 text-gray-700 shadow-md rounded-3xl\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col-span-3 bg-blue-100 rounded-l-3xl\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;col-span-7 flex flex-col justify-between p-2\u0026#34;\u0026gt; \u0026lt;span \u0026gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam tempor elit nisl. Vestibulum iaculis sit amet leo vitae pretium.\u0026lt;/span \u0026gt; \u0026lt;span class=\u0026#34;text-gray-900 font-bold\u0026#34;\u0026gt;John Doe\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; Et voici le résultat que l\u0026rsquo;on obtient :\nNous allons voir dans cet article en quoi Tailwind est intéressant, notamment dans la mise en place d\u0026rsquo;un Design System.\nEn quoi est-ce intéressant ? L\u0026rsquo;objectif de cette approche \u0026ldquo;Utilisty First\u0026rdquo; est de rendre chaque classe réutilisable. Cela a beaucoup d\u0026rsquo;avantages, par exemple :\n Alléger le code CSS produit : finie la re-déclaration des mêmes propriétés css partout dans les composants. Ici elle ne sera déclarée qu\u0026rsquo;une seule fois ! Par exemple la classe m-0 est déclarée une seule fois dans le css produit par Tailwind, et utilisable n\u0026rsquo;importe où. Arrêter les allers et retours entre code HTML et feuilles de style CSS : ici le style est appliqué en faisant appel à des classes CSS dans le code HTML, ce qui représente un certain gain de temps dans le développement de sites web. Apporter de la cohérence dans le code : cette approche facilite par exemple l\u0026rsquo;utilisation des bonnes couleurs et bonnes propriétés de fonts, sans sortir complètement des règles de design (utiliser une certaine palette de couleurs, utiliser des multiples de 8 pour tout ce qui est espacement, etc.). Maintenir plus facilement le design de l\u0026rsquo;application : changer une propriété CSS telle qu\u0026rsquo;une couleur ou une taille de police d\u0026rsquo;écriture devient facile et s\u0026rsquo;effectue à un seul et même endroit.  Pour parler plus spécifiquement de Tailwind, l\u0026rsquo;un de ses avantages est d\u0026rsquo;être hautement personnalisable. Que ce soit pour les couleurs, les tailles d\u0026rsquo;écran, les polices d\u0026rsquo;écriture, les espacements\u0026hellip; L\u0026rsquo;objectif est de pouvoir configurer un design cohérent et propre à l\u0026rsquo;application ou au service que l\u0026rsquo;on souhaite développer.\nDe plus, un autre avantage est qu\u0026rsquo;il permet la purge du css produit. Cela signifie que toutes les classes css générées par tailwind qui ne sont utilisées nulle part dans le code seront purgées (dans l\u0026rsquo;idéal, la purge est conseillée dans les envrionnements de production). Ce qui réduit considérablement la taille du fichier css produit. Il est également possible de manipuler la manière dont la purge est effectuée : des regex peuvent être appliquées lors de la purge pour créer une \u0026ldquo;safelist\u0026rdquo; de classes css à ne pas supprimer.\nTailwind permet aussi de préfixer ses classes css produites de sorte à ne pas causer de conflits avec d\u0026rsquo;autres librairies css ou tout simplement d\u0026rsquo;autres classes css. Ce sera très utile si l\u0026rsquo;on décide d\u0026rsquo;utiliser Tailwind en parallèle d\u0026rsquo;autres frameworks, librairies CSS, où les noms de classes css pourraient être les mêmes que celles générées par Tailwind.\nToutes ces règles sont définies dans le fichier de configuration de tailwind (tailwind.config.js) écrit en javascript, donc facilement manipulable. Cela donnera un fichier comme ci-dessous :\n// tailwind.config.js const colors = require(\u0026#34;./tailwind/colors\u0026#34;); // JSON de couleurs const purgeSafelist = require(\u0026#34;./tailwind/purge\u0026#34;); // Tableau de regex  module.exports = { purge: { content: [\u0026#34;./src/**/*.vue\u0026#34;], options: { safelist: purgeSafelist, }, }, darkMode: false, // or \u0026#39;media\u0026#39; or \u0026#39;class\u0026#39;  prefix: \u0026#34;myApp-\u0026#34;, theme: { colors, extend: { spacing: {}, borderRadius: {}, boxShadow: {}, }, }, variants: { extend: { backgroundColor: [\u0026#34;active\u0026#34;], }, }, plugins: [], }; On peut observer l\u0026rsquo;utilisation des champs extend, qui vont permettre d\u0026rsquo;ajouter des configurations aux configurations existantes. Par exemple, si l\u0026rsquo;on veut ajouter des possibilités de box-shadow (avec des valeurs et des couleurs différentes) tout en gardant les valeurs proposées initialement par Tailwind, ce sera dans le champ extend de theme qu\u0026rsquo;il faudra les ajouter. Si par contre on ne souhaite pas garder les valeurs par défaut de Tailwind, il faudra ajouter les nouvelles valeurs de box-shadow directement à la racine de theme.\nPour ce qui est des variants, il s\u0026rsquo;agit des circonstances durant lesquelles des propriétés et classes css vont être activées. Ces circonstances peuvent être des hover, active, focus, etc. Dans l\u0026rsquo;exemple ci-dessus, on peut voir qu\u0026rsquo;on étend le variant backgroundColor à l\u0026rsquo;état active. Cela va permettre de pouvoir appliquer des classes Tailwind sur le changement de couleur du background lors de l\u0026rsquo;état active (lorsque que le clic gauche de la souris est appuyé sur un bouton par exemple).\nPourquoi l\u0026rsquo;utiliser pour construire un Design System ? → Comme évoqué précédemment, Tailwind est un framework hautement personnalisable. Ce qui permet d\u0026rsquo;établir les règles du design system très facilement, qu\u0026rsquo;il s\u0026rsquo;agisse des règles d\u0026rsquo;espacement, des fonts à utiliser, des couleurs, etc.\n→ La configuration de Tailwind se faisant dans un fichier javascript, il est très simple de le manipuler. On peut donc le configurer depuis des données externes. Prenons l\u0026rsquo;exemple des \u0026ldquo;design tokens\u0026rdquo; : certains outils de design, comme Zeroheight (application permettant de documenter un design system), permettent de générer et d\u0026rsquo;exposer des fichiers JSON via des URL. Ces fichiers peuvent être générés depuis des extraits de données venant d\u0026rsquo;autres outils de Design tels que Figma (outil de maquettage d\u0026rsquo;écrans d\u0026rsquo;applications en collaboratif), où les designers peuvent spécifier leurs règles de design, telles que les couleurs, les box-shadows, les fonts, etc. Voyons cela à l\u0026rsquo;aide d\u0026rsquo;un schéma, qui sera plus parlant. → La cohérence qu\u0026rsquo;il permet d\u0026rsquo;apporter au design de l\u0026rsquo;application en fait une très bonne solution pour mettre en place un design system, en appliquant facilement les mêmes codes dans chacun des composants.\n→ Aucun thème par défaut n\u0026rsquo;est appliqué contrairement à d\u0026rsquo;autres librairies css. Il faudra donc définir soi-même tous les styles à appliquer.\n→ Tailwind fournit des outils pour traiter un grand nombre de problématiques, dont le dark mode. Il suffira alors de préfixer les classes de couleurs par dark:, et d\u0026rsquo;en définir les couleurs dans le design system.\n→ Enfin, il permet d\u0026rsquo;appliquer des règles pour rendre une application responsive, en donnant la possibilité de définir les comportements des composants développés selon les tailles d\u0026rsquo;écrans.\nQuelles sont les différences de ce framework par rapport à d\u0026rsquo;autres comme Bootstrap ? La première est que Tailwind, contrairement à Bootstrap, n\u0026rsquo;est pas un \u0026ldquo;UI kit\u0026rdquo; : il n\u0026rsquo;embarque ni de thème par défaut, ni de composants. Cela ne signifie pas que Tailwind ne propose rien pour autant. Des valeurs par défaut pour les palettes de couleurs ou les espacements sont à la disposition des développeurs, mais sont également très simples à écraser ou compléter. L\u0026rsquo;idée derrière cette logique est de ne pas imposer de règles de design.\nBootstrap va également apporter beaucoup sur le plan du responsive, tandis que Tailwind va plus s\u0026rsquo;axer autour d\u0026rsquo;une grande personnalisation.\nEn termes de chiffres, voici la comparaison des deux frameworks réalisée sur npm trends : Et pour ce qui est de l\u0026rsquo;utilisation des deux frameworks, Bootstrap est utilisé entre autres par Spotify, Twitter, et Lyft, tandis que Tailwind est utilisé par BlaBlaCar, überdosis et Superchat.\nÀ retenir Un grand nombre de solutions existent pour faciliter la mise en place d\u0026rsquo;un design dans une application, et Tailwind en fait partie. Il rendra le développement UI plus rapide et plus cohérent et permettra de configurer ses propres règles de design en toute simplicité.\nAinsi, son utilisation est tout à fait adaptée dans la mise en place d\u0026rsquo;un design system.\n","date":"Oct 15, 2021","href":"https://blog.talanlabs.com/tailwind-design-system/","kind":"page","labs":null,"tags":["design system","css","framework"],"title":"Tailwind - Design System"},{"category":null,"content":"La mise en place de tests de logiciels est une pratique très répandue. Cela ajoute un meilleur niveau de qualité sur le code d’une application.\n Choisir sa solution de tests peut être un exercice difficile. En effet, il existe de nombreux outils, frameworks, patterns permettant d’aborder ce sujet.\n Dans cet article, nous étudierons Testing Library, une bibliothèque permettant de réaliser des tests frontend du point de vue de l’utilisateur.\n La diversité de l’écosystème frontend L’écosystème frontend évolue fréquemment. Les paradigmes changent (templating côté serveur, spa…), de nouveaux frameworks sortent (Angular, React, Vue JS…), de nouvelles librairies continuent d’apparaître…​\n À ce jour, aucun des grands frameworks javascript n’a réussi à s’imposer comme standard de l’environnement frontend. Aussi, le code source d’une application Web peut fortement varier d’un projet à l’autre selon la stack choisie.\n Qu’en est-il des tests ? Chaque framework propose au moins une librairie pour tester le rendu de ses composants. Nous pouvons citer Enzyme pour React, la solution native pour Angular, etc. Toutefois, ces tests souffrent souvent du même défaut, à savoir que leur code source est très lié au framework javascript utilisé.\n Plusieurs frameworks, plusieurs manières d’écrire un composant Exemple avec Angular @Component({ selector: \u0026#39;todo-list\u0026#39;, template: ` \u0026lt;ul\u0026gt; \u0026lt;li *ngFor=\u0026#34;let todo of todos\u0026#34;\u0026gt;{{todo}}\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;input #newTodo/\u0026gt; \u0026lt;button (click)=\u0026#34;addTodo(newTodo.value)\u0026#34;\u0026gt;Add\u0026lt;/button\u0026gt; ` }) export class TodoListComponent { todos = []; addTodo(newTodo) { this.todos.push(newTodo); } }   Exemple avec React export default function TodoList() { const [todos, setTodos] = useState([]); const todoInput = useRef(null); const addTodo = newTodo =\u0026gt; setTodos([...todos, newTodo]); return \u0026lt;\u0026gt; \u0026lt;ul\u0026gt; {todos.map((todo, i) =\u0026gt; \u0026lt;li key={i}\u0026gt;{todo}\u0026lt;/li\u0026gt;)} \u0026lt;/ul\u0026gt; \u0026lt;input ref={todoInput}/\u0026gt; \u0026lt;button onClick={() =\u0026gt; addTodo(todoInput.current.value)}\u0026gt;Add\u0026lt;/button\u0026gt; \u0026lt;/\u0026gt;; }   Les deux codes sources sont très différents, bien que le résultat affiché dans le navigateur soit strictement identique.\n  Plusieurs frameworks, plusieurs manières de tester un composant Exemple avec Angular describe(\u0026#39;TodoListComponent\u0026#39;, () =\u0026gt; { it(\u0026#39;should add two todos\u0026#39;, () =\u0026gt; { // Arrange TestBed.configureTestingModule({declarations: [TodoListComponent]}); const fixture = TestBed.createComponent(TodoListComponent); const component = fixture.componentInstance; expect(component).toBeDefined(); const hostElement = fixture.nativeElement; const button = hostElement.querySelector(\u0026#39;button\u0026#39;) const input = hostElement.querySelector(\u0026#39;input\u0026#39;)!; // Act input.value = \u0026#39;First\u0026#39;; input.dispatchEvent(new Event(\u0026#39;change\u0026#39;)); button.dispatchEvent(new Event(\u0026#39;click\u0026#39;)); input.value = \u0026#39;Second\u0026#39;; input.dispatchEvent(new Event(\u0026#39;change\u0026#39;)); button.dispatchEvent(new Event(\u0026#39;click\u0026#39;)); fixture.detectChanges(); //Assert expect(hostElement.querySelectorAll(\u0026#39;li\u0026#39;).item(0)!.textContent).toEqual(\u0026#39;First\u0026#39;); expect(hostElement.querySelectorAll(\u0026#39;li\u0026#39;).item(1)!.textContent).toEqual(\u0026#39;Second\u0026#39;); }); });   Exemple avec Enzyme pour React describe(\u0026#39;TodoList\u0026#39;, () =\u0026gt; { test(\u0026#39;should add two todos\u0026#39;, () =\u0026gt; { // Arrange const wrapper = mount(\u0026lt;TodoList/\u0026gt;); // Act wrapper.find(\u0026#39;input\u0026#39;).instance().value = \u0026#39;First\u0026#39;; wrapper.find(\u0026#39;button\u0026#39;).simulate(\u0026#39;click\u0026#39;); wrapper.find(\u0026#39;input\u0026#39;).instance().value = \u0026#39;Second\u0026#39;; wrapper.find(\u0026#39;button\u0026#39;).simulate(\u0026#39;click\u0026#39;); //Assert expect(wrapper.find(\u0026#39;li\u0026#39;)).toHaveLength(2); expect(wrapper.find(\u0026#39;li\u0026#39;).at(0).text()).toBe(\u0026#39;First\u0026#39;); expect(wrapper.find(\u0026#39;li\u0026#39;).at(1).text()).toBe(\u0026#39;Second\u0026#39;); }); });   Les scénarios utilisateurs sont identiques, mais l’écriture des tests varie fortement d’un framework à l’autre.\n    Testing Library à la rescousse Testing library est une bibliothèque permettant de requêter et d’interagir avec un DOM. Cette librairie incite donc à rédiger des tests qui n’auront pas connaissance du détail d’implémentation des composants.\n Testing library est fournie avec de nombreuses bibliothèques tierces qui facilitent l’intégration de son API dans les principaux frameworks (Angular, React, Vue JS…).\n Utilisation de Testing Library avec Angular et React Exemple avec Angular describe(\u0026#39;TodoListComponent\u0026#39;, () =\u0026gt; { test(\u0026#39;should add two todos\u0026#39;, async () =\u0026gt; { // Arrange await render(TodoListComponent); // Act fireEvent.change(screen.getByRole(\u0026#39;textbox\u0026#39;), {target: {value: \u0026#39;First\u0026#39;}}); fireEvent.click(screen.getByRole(\u0026#39;button\u0026#39;)); fireEvent.change(screen.getByRole(\u0026#39;textbox\u0026#39;), {target: {value: \u0026#39;Se-cond\u0026#39;}}); fireEvent.click(screen.getByRole(\u0026#39;button\u0026#39;)); // Assert expect(screen.getAllByRole(\u0026#39;listitem\u0026#39;)).toHaveLength(2); expect(screen.getAllByRole(\u0026#39;listitem\u0026#39;)[0]).toHaveTextContent(\u0026#39;First\u0026#39;); expect(screen.getAllByRole(\u0026#39;listitem\u0026#39;)[1]).toHaveTextContent(\u0026#39;Second\u0026#39;); }); });   Exemple avec React describe(\u0026#39;TodoList\u0026#39;, () =\u0026gt; { test(\u0026#39;should add two todos\u0026#39;, () =\u0026gt; { // Arrange render(\u0026lt;TodoList /\u0026gt;); // Act fireEvent.change(screen.getByRole(\u0026#39;textbox\u0026#39;), {target: {value: \u0026#39;First\u0026#39;}}); fireEvent.click(screen.getByRole(\u0026#39;button\u0026#39;)); fireEvent.change(screen.getByRole(\u0026#39;textbox\u0026#39;), {target: {value: \u0026#39;Se-cond\u0026#39;}}); fireEvent.click(screen.getByRole(\u0026#39;button\u0026#39;)); //Assert expect(screen.getAllByRole(\u0026#39;listitem\u0026#39;)).toHaveLength(2); expect(screen.getAllByRole(\u0026#39;listitem\u0026#39;)[0]).toHaveTextContent(\u0026#39;First\u0026#39;); expect(screen.getAllByRole(\u0026#39;listitem\u0026#39;)[1]).toHaveTextContent(\u0026#39;Second\u0026#39;); }); });   Hormis la création des composants, les deux tests sont strictement identiques. L’expérience utilisateur étant la même quel que soit le framework utilisé, il est normal que cela se constate au niveau des tests.\n  Les principaux types de requête Testing Library permet de requêter le DOM de plusieurs manières, via différents types de requête. Parmi les plus intéressantes, nous avons :\n   getByRole : Requête des éléments selon leur représentation dans l’arbre d’accessibilité.\n  getByLabelText : Requête des éléments en passant par leur label. Très utile pour tester des formulaires.\n  getByPlaceholderText : Requête des éléments selon leur placeholder. Utile si on n’a pas de label.\n  getByText : Retrouve un élément selon son texte affiché à l’écran. Utile pour requêter une div, span, etc.\n  getByDisplayValue : Permet de récupérer des éléments de formulaire selon leur valeur.\n   En dernier recours, il existe également la requête getByTestId qui récupère des éléments selon leur attribut test-id.\n Il est possible de requêter des éléments de manière asynchrone en remplaçant le prefix get par le prefix find.\n  Testing Playground : un excellent plugin de navigateur Il existe un plugin navigateur qui retourne les requêtes correspondant à un élément donné. Les requêtes sont fournies par ordre de préconisation.\n Le plugin est disponible aux adresses suivantes :\n   Chrome: https://chrome.google.com/webstore/detail/testing-playground/hejbmebodbijjdhflfknehhcgaklhano\n  Firefox: https://addons.mozilla.org/en-US/firefox/addon/testing-playground/\n    Tester l’accessibilité de son application Testing Library permet de réaliser des tests qui simulent l’utilisation d’une interface d’accessibilité comme un lecteur d’écran. Les requêtes de types roles récupèrent les éléments du DOM via leurs rôles ARIA correspondant.\n La fonction isInaccessible détermine si un élément du DOM sera exclu de l’arbre d’accessibilité.\n  Utilitaire pour simuler les événements utilisateur Lorsqu’on utilise une application dans un navigateur, de nombreux événements sont générés. Par exemple, si un utilisateur souhaite cliquer sur un bouton à l’aide de sa souris, les événements suivants seront lancés :\n   mouseOver\n  mouseMove\n  mouseDown\n  focus\n  mouseUp\n  click\n   Pour rédiger un test similaire à ce qui se produit dans le navigateur, il faut générer toute cette suite d’événements. Pour nous simplifier la tâche, il existe une librairie satellite - @testing-library/user-event – qui fournit plusieurs cas classiques d’interactions entre l’utilisateur et le navigateur.\n La précédente suite d’évènements serait générée en appelant simplement userEvent.click.\n    Conclusion Comme nous avons pu le voir, les tests écrits avec Testing Library n’ont pas connaissance de l’implémentation des composants. Le découplage entre l’écriture des tests et l’implémentation des composants permet de facilement refactorer son code.\n Ensuite, cette librairie invite à tester l’accessibilité de l’application, chose qui est, malheureusement, trop souvent négligée.\n Je terminerai sur une remarque plus subjective. Je trouve qu’il est plus simple d’appliquer le TDD avec Testing Library. En effet, le comportement d’un composant est souvent bien anticipé, alors que son implémentation sera connue a posteriori. Il est donc pratique de commencer par rédiger un test qui répètera le scénario utilisateur bien identifié en amont.\n  Liens utiles :\n   https://testing-library.com/\n  https://github.com/testing-library\n     ","date":"Oct 14, 2021","href":"https://blog.talanlabs.com/2021-10-13-testing-library/","kind":"page","labs":null,"tags":["test","angular","react","testing-library"],"title":"Tester ses scénarios utilisateur avec Testing-Library"},{"category":null,"content":"Qui aime rédiger la documentation de son produit ou de son API ? Qui en a marre de trouver des documentations obsolètes en arrivant sur un projet ?\n Tu es développeur et tu te sens concerné ? Il est temps de passer à la \u0026#34;Documentation as Code\u0026#34; !\n La documentation que tout le monde connait L’exemple le plus trivial de \u0026#34;Documentation as Code\u0026#34; est le traditionnel fichier README.md à la racine de tous les projets. Dans l’idéal, ce fichier contient la documentation minimale du projet, les instructions d’installation et peut-être même un descriptif des principales fonctionnalités.\n Dans les faits, ce fichier est bien trop souvent négligé et rarement remis à jour. Et c’est bien dommage ! Comme il fait partie de votre projet, il est versionné et il n’y a pas besoin d’ouvrir un nouvel outil pour changer la documentation (Confluence ou SharePoint, on vous voit !).\n Et puis vous avez remarqué, par défaut, c’est un fichier .md, autrement dit en Markdown, un langage qui permet de formater du texte via des balises (définir un titre, mettre en gras, etc.).\n  Logo Markdown  L’énorme avantage : un humain peut le lire sans avoir le sentiment de lire du code (utile pour les parties prenantes non techniques d’un projet), mais en plus votre IDE ou GitHub peuvent l’afficher comme une page HTML bien présentée. Très pratique pour mettre à disposition du public votre documentation.\n   Un pas en avant : l’AsciiDoc Mais parfois la documentation dépasse le simple texte et on a besoin d’afficher des tableaux : ils sont très mal gérés en Markdown. Un autre désavantage du Markdown : il ne donne pas de sens (sémantique) au contenu généré, autrement dit il faut insérer manuellement du HTML pour bénéficier de classes CSS ou de balises spécifiques.\n Pour répondre à différents besoins qui ont émergé, des \u0026#34;flavors\u0026#34; de Markdown sont apparus. Autant de versions différentes, d’interpréteurs différents …​ et à la fin on a perdu le langage universel promis.\n C’est alors qu’intervient l’AsciiDoc. Comme le Markdown, c’est un langage de balises assez simple et lisible par un humain sans même être interprété. Mais il propose des balises pour afficher bien plus de types de données que le Markdown de base.\n  Logo Asciidoctor  Un autre avantage est la capacité à faire des \u0026#34;includes\u0026#34; d’un fichier dans un texte AsciiDoc. Par exemple, il est possible d’afficher le code présent dans un fichier du projet, et de voir ce morceau de code mis à jour dans la documentation à chaque modification du fichier source.\n Pour les plus curieux, Asciidoctor propose une comparaison poussée entre Asciidoc et Markdown.\n   Un exemple de document généré avec Asciidoctor Je l’évoquais l’an dernier dans un post, j’ai le plaisir d’encadrer des étudiants sur une série de cours autour des technologies Blockchain. Dans le cadre de ces cours, une grande place est laissée à la pratique, et même que l’on y parle de fromage…​\n  Extrait du support d’un TP en PDF  Les supports des TP sont régulièrement mis à jour, y compris pendant les séances lorsque les étudiants remarquent des erreurs ou ont besoin de plus de détails. Opération de mise à disposition qui serait bien plus lente dans le cadre d’un support rédigé sous Word par exemple, qu’il faudrait exporter en PDF manuellement, puis envoyer par mail, etc.\n Dans notre cas, en demandant une génération en PDF et en HTML du support, et via un simple job de Gitlab CI, nous mettons à disposition en quelques instants la nouvelle version du support aux étudiants.\n  Même extrait du support d’un TP, cette fois-ci en HTML    Et si on faisait mieux ? OK, nous venons de voir qu’il existe un langage de balises pour formater du texte à but de documentation (voire de rédaction de livres !), plus évolués que Markdown …​ et surtout que LaTeX qui a d’autres intérêts, plus éloignés du monde de l’informatique.\n Pour autant, il faut encore rédiger cette documentation, cela ne résout pas tous nos problèmes. Et si je vous disais que l’on peut aller plus loin ? La suite dans le prochain article !\n  Liens utiles :\n   Documentation du langage AsciiDoc\n  Résumé de la syntaxe\n  AsciiDocLIVE, un éditeur AsciiDoc en ligne\n    Les autres articles de cette série :\n   Partie 2 : Présentation de Spring REST Docs\n  Partie 3 : Aller plus loin avec Spring REST Docs\n     ","date":"Oct 1, 2021","href":"https://blog.talanlabs.com/make-documentation-great-again-part-1/","kind":"page","labs":null,"tags":["Documentation","AsciiDoc"],"title":"Make documentation great again (1/2)"},{"category":null,"content":"Cet article est le second d’une suite de trois articles, que vous pourrez retrouver ici quand ils seront sortis :\n   Contrat de coaching : agir POUR son client\n  La production du contrat de coaching et son entretien\n  Le suivi du contrat\n   Vous êtes désormais convaincu que le mandat de coaching est une nécessité et vous souhaitez produire un contrat ? Voici les étapes de production que je vous propose.\n Si vous n’êtes pas encore convaincu, je vous invite à lire ou relire mon article sur Pourquoi faire un mandat de coaching.\n Comment se passe la production du contrat Le coach a plusieurs choix : un contrat pour tous les coachés, ou bien un contrat par intéressé. Dans l’idéal, il fait les deux.\n Il m’a paru plus facile dans mes missions de faire un contrat avec le management, ainsi qu’un contrat pour le PO et un mandat pour l’équipe de développement. Ces derniers devront être partagés et transparents. Dans le cas où ils se contredisent, il convient alors de réunir les différentes parties. Toutefois, pour améliorer la communication et gagner du temps, il est préférable de faire un contrat pour toutes les personnes accompagnées. Le coach aura alors besoin d’organiser un temps de discussion avec chaque intéressé et de le faciliter afin de chercher des actions concrètes pour répondre aux problèmes.\n Le format que j’utilise habituellement est d\u0026#39;une heure, en suivant le modèle du contrat. En préparant ce type d’entretien, je choisis quelques \u0026#34;powerful questions\u0026#34; qui seront utiles pendant l’entretien.\n Je trouve pertinent de guider mon client en lui soumettant des idées d’améliorations quand je le souhaite, ou bien de lui préciser quand j’ai l’impression de repérer un problème, que lui ne voit pas forcément. Je n’entre pas dans un débat avec lui mais lui propose de prendre une action à ce sujet, s’il le souhaite.\n A la fin de l’entretien, un feedback est utile et bon à prendre, car il faut chercher à améliorer l’expérience de ces réunions, qui vont se produire à échéance régulières.\n   Mener l’entretien de production du contrat Durant cet entretien, votre objectif est de vous mettre d’accord avec le client sur les questions que vont toucher votre mission.\n En général, je commence le premier entretien en présentant le contrat, son utilité et comment il est construit.\n J’apprécie également fixer les échéances auxquelles nous souhaitons nous rencontrer, cela permet de déterminer la granularité des actions qui sortiront, même si ces échéances pourront changer plus tard.\n Ensuite, je reprend le modèle du contrat et nous commençons la scéance.\n Introduction : Contexte de la mission\n Je reprécise la situation de la mission et l’environnement dans lequel je dois intervenir. Cela permettra de définir les limites des équipes avec lesquelles je vais travailler et permettra de se mettre d’accord sur ce qui est attendu.\n J’en profite également pour évaluer les connaissances du coaching de mon client.\n Si c’est la première fois qu’il est face à un coach, je prendrais le temps de lui expliquer les bases de mon métier ainsi que mes moyens d’action.\n Nous abordons pour finir les problématiques générales pour lesquelles j’ai été convié.\n Partie I : Faire rêver\n Ensuite, je m’éloigne des problèmes et je m’intéresse à une vision long terme.\n Cette étape permet d\u0026#39;éloigner mon client des Problèmes pour se rapprocher des moyens d’action et de la situation qu’il souhaiterait avoir. Je l’invite alors à imaginer la situation idéale de son contexte et comment se passerait le projet si tout se passait bien.\n La question que j’en profite pour poser est \u0026#34;Si je sors de ma mission et que tu es extrêmement satisfait de mon travail, comment est le projet ? Qu’est-ce qui a changé ?\u0026#34; Nous nous concentrons alors sur l’impact que je peux provoquer et je comprends mieux ce qui est attendu de mon client. Je me permets également de lui dire si je ne suis pas d’accord avec lui, afin que nous nous adaptions l’un à l’autre. Une vision est créée.\n Nous pouvons alors passer à la partie II du contrat, plus complexe.\n Partie II : Le cours terme\n Désormais, nous nous sommes mis d’accord sur la vision idéale. Nous allons désormais essayer de déterminer ce que nous voulons voir changé d’ici notre prochaine rencontre.\n Dans cette partie, je guide un peu plus mon client sur ce que j’aimerais faire et les possibilités, afin de lui montrer ce qui est possible. Nous nous concentrons alors sur LE problème le plus important et comment le résoudre.\n J’essaie d’en faire sortir des actions concrètes, qui seront faisable par le client ou par moi.\n Lorsque nous avons avancé sur le premier problème, nous passons au deuxième et ainsi de suite.\n Fin de l’entretien\n Je reprend et note les actions identifiées au cours de la dernière étape. Ensuite, je fixe le prochain entretien et m’assure que ce dernier s’est bien passé.\n Il peut être intéressant d’aller vers quelque chose de moins formel, à la demande du client. Mais surtout, il ne faut pas être fermé à faire évoluer le format.\n   Conclusion : Clarifier son mandat de coaching auprès d’un client est nécessaire au bon déroulement d’une mission. Ainsi, l’entretien de production du contrat est un moment crucial dans les débuts d’une mission, car il permet de s’aligner avec son client sur le travail qui sera réalisé. Dès lors, le travail du coach sera clair et explicite et il pourra faire évoluer le contrat au fur et à mesure des entretiens. C’est ce que nous verrons dans le troisième article.\n   ","date":"May 10, 2021","href":"https://blog.talanlabs.com/mandat-de-coaching-part2/","kind":"page","labs":null,"tags":["Agilité","contrat de coaching","coaching"],"title":"Contrat de coaching : Production (partie 2)"},{"category":null,"content":"Mise en contexte Nous allons parler dans cet article de l\u0026rsquo;intégration des API Microsoft dans une application Web. Commençons d\u0026rsquo;abord par une mise en contexte :\nMicrosoft 365 est maintenant présent dans la majorité des entreprises, le télétravail tend à se démocratiser de plus en plus avec le contexte actuel, et Teams est de plus en plus utilisé. En effet, partager de l\u0026rsquo;information et chercher à améliorer nos manières de communiquer sont devenus de véritables enjeux.\nCertains grands groupes développent ainsi des portails clients (B2B ou B2C) pour répondre à ces problématiques. Il s\u0026rsquo;agit là de mettre à disposition des clients diverses informations et services de manière centralisée sur un même portail.\nLa question qui s\u0026rsquo;est posée dans le cadre de notre mission est donc la suivante : comment pourrait-on faciliter les communications entres les clients de l\u0026rsquo;entreprise et leurs contacts commerciaux, que ce soit par chat, audio, vidéo ou bien l\u0026rsquo;organisation de meetings, le tout depuis ce portail ?\n→ Et bien Microsoft met à disposition des outils répondant à ces besoins (API Microsoft Graph, deeplinks…)\nNotre objectif a donc été de réaliser un POC pour tester ces API, voir ce qu\u0026rsquo;il était possible de faire, de ne pas faire, en trouver les limites, etc. Pour ensuite éventuellement transmettre ce qu\u0026rsquo;on aura appris à d\u0026rsquo;autres équipes.\nEnvironnement de développement Tout d\u0026rsquo;abord, qu\u0026rsquo;est-ce que Microsoft Graph ? \u0026ldquo;Microsoft Graph is the gateway to data and intelligence in Microsoft 365. It provides a unified programmability model that you can use to access the tremendous amount of data in Microsoft 365, Windows 10, and Enterprise Mobility + Security. Use the wealth of data in Microsoft Graph to build apps for organizations and consumers that interact with millions of users.\u0026quot; (source)\nAvant de pouvoir utiliser les API Microsoft Graph, il va falloir quelques prérequis.\nTenant Commençons par la notion de tenant, qui en fait représente une organisation dans Office 365, par exemple Talan. Il sera nécessaire d\u0026rsquo;avoir accès au compte admin de notre tenant, ou bien de travailler avec les personnes qui y ont accès.\nDans le cadre de notre POC, on nous a mis à disposition un tenant de test (avec de faux utilisateurs, de faux groupes, etc.) afin de faciliter nos recherches.\nCréation de l\u0026rsquo;application L\u0026rsquo;étape suivante a été d\u0026rsquo;enregistrer une nouvelle application dans le portail azure, afin d\u0026rsquo;être capable d\u0026rsquo;utiliser les API Microsoft.\n→ Microsoft fournit un Quick Start pour configurer l\u0026rsquo;environnement de développement et ainsi lancer rapidement les développements dans le language que l\u0026rsquo;on souhaite. Dans notre cas, en Node.js.\nA l\u0026rsquo;issu de ces étapes, nous avons un ID et un secret pour notre application, qu\u0026rsquo;il faudra renseigner dans les variables d\u0026rsquo;environnement.\n Note : pour plus d\u0026rsquo;informations sur l\u0026rsquo;enregistrement d\u0026rsquo;applications dans le portail Azure, voici le lien !\n Authentification Afin d\u0026rsquo;utiliser les API Microsoft Graph, il faut disposer d\u0026rsquo;un authentication token en s\u0026rsquo;authentifiant en tant qu\u0026rsquo;utilisateur ou en tant que service.\n En tant qu\u0026rsquo;utilisateur (compte professionnel / étudiant, ou compte personnel): j\u0026rsquo;aurai accès principalement à mes informations (mails, messages, calendrier\u0026hellip;). En tant que service (backend, daemons): cette authentification peut être utilisée lorsqu\u0026rsquo;il s\u0026rsquo;agit de services d\u0026rsquo;arrière-plan ou bien lorsqu\u0026rsquo;il faut des privilèges plus élevés que ceux d\u0026rsquo;un simple utilisateur.   Note : Microsoft Graph utilise l\u0026rsquo;authentification standard OIDC / OAuth2\n Voici un exemple de requête de demande de token pour obtenir l\u0026rsquo;accès sans utilisateur :\nPOST: https://login.microsoftonline.com/{tenant}/oauth2/v2.0/token HTTP/1.1 Host: login.microsoftonline.com Content-Type: application/x-www-form-urlencoded client_id=535fb089-9ff3-47b6... \u0026amp;scope=https%3A%2F%2Fgraph.microsoft.com%2F.default \u0026amp;client_secret=qWgdYAmab0Y... \u0026amp;grant_type=client_credentials Les paramètres passés dans le body de la requête client_id \u0026amp; client_secret sont les App ID et secret définis lors de l\u0026rsquo;enregistrement de l\u0026rsquo;application dans le portail azure.\nPour le paramètre scope : \u0026ldquo;La valeur transmise pour le paramètre scope dans cette demande doit être l’identificateur (URI de l’ID d’application) de la ressource souhaitée, avec le suffixe .default. Pour Microsoft Graph, la valeur est https://graph.microsoft.com/.default. Cette valeur informe le point de terminaison de la Plateforme d’identités Microsoft que parmi toutes les autorisations que vous avez configurées pour votre application, un jeton doit être généré pour celles associées à la ressource que vous souhaitez utiliser.\u0026quot; (https://docs.microsoft.com/fr-fr/graph/auth-v2-service)\nEt enfin le paramètre grant_type doit prendre la valeur client_credentials lorsqu\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;une authentification en tant que service.\nVoici donc le token que nous recevons en réponse à la requête:\n{ \u0026#34;token_type\u0026#34;: \u0026#34;Bearer\u0026#34;, \u0026#34;expires_in\u0026#34;: 3599, \u0026#34;access_token\u0026#34;: \u0026#34;eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1Ni...\u0026#34; } Pour résumer les 2 manières de s\u0026rsquo;authentifier:\nPermissions - Consentement administrateur Le scope d\u0026rsquo;utilisation des API Microsoft Graph dans l\u0026rsquo;application est régulé par l\u0026rsquo;administrateur via le portail azure, où il va devoir donner son consentement, lorsque cela est nécessaire, afin d\u0026rsquo;utiliser tel ou tel endpoint, et ce quelque soit l\u0026rsquo;authentification.\nOn pourra ainsi observer des différences de privilèges selon si la requête est au nom d\u0026rsquo;un utilisateur ou en tant que service.\nLes permissions relatives à chaque endpoint de l\u0026rsquo;API sont décrites dans la documentation fournie par Microsoft. On y retrouve donc plusieurs types de permissions (déléguées et application). En voici un exemple sur les endpoints de lecture / écriture du calendrier :\nRésumé Une fois toutes ces notions de tenant, d\u0026rsquo;authentification, de permissions comprises, il est relativement simple d\u0026rsquo;utiliser les API Microsoft Graph. De plus, Microsoft fournit une documentation claire et complète, ainsi qu\u0026rsquo;un set de requêtes Postman pour s\u0026rsquo;y familiariser (voici le lien).\nMaintenant que nous avons vu comment utiliser les API Microsoft Graph, nous allons pouvoir parler du travail que nous avons réalisé dans le cadre du POC.\nCas d\u0026rsquo;utilisation Pour reprendre la problématique du projet, que mettent concrètement à disposition les API Microsoft Graph pour faciliter les communications ? Y a t-il des endpoints de lecture / écriture de messagerie ? Y a t-il d\u0026rsquo;autres solutions ? Que recommande Microsoft pour ces cas d\u0026rsquo;utilisation ?\nC\u0026rsquo;est ce à quoi nous allons tenter de répondre.\nCommuniquer par chat Pour commencer, nous avons fait quelques recherches sur la communication par chat et voici ce que nous en avons conclu :\n Il existe plusieurs endpoints pour récupérer des messages provenant de channels Teams, etc. Cependant, lors de nos recherches, il s\u0026rsquo;agissait de fonctionnalités de la version beta des API Microsoft Graph, et non pas des versions officielles (il semble qu\u0026rsquo;elles soient maintenant disponibles en version normale). Cette solution nécessite d\u0026rsquo;implémenter un système de messagerie instantanée. Cette solution nécessite que le client se connecte avec son compte Teams sur l\u0026rsquo;application. Une solution possible serait l\u0026rsquo;utilisation de bots, mais avait l\u0026rsquo;air trop complexe dans le cadre du projet.  → Nous avons été confrontés à beaucoup de freins, que ce soit par la faisabilité, la pertinence ou la complexité des solutions.\nComment alors faire plus simplement ?\n→ Azure Communication Services\nSuite à la discussion que nous avons eue avec les équipes Microsoft Teams, nous avons été redirigés vers cette solution qui n\u0026rsquo;allait pas tarder à être lancée (à l\u0026rsquo;époque en tout cas) et qui répondrait à nos besoins en matière de communication. Cela étant, nous n\u0026rsquo;avons pas creusé l\u0026rsquo;idée plus que ça, bien qu\u0026rsquo;elle avait l\u0026rsquo;air assez intéressante.\nLes équipes Microsoft Teams nous ont ensuite expliqué que leur vision de l\u0026rsquo;utilisation des services qu\u0026rsquo;ils proposent était différente de la nôtre. En effet, leur vision s\u0026rsquo;axe plus sur l\u0026rsquo;intégration d\u0026rsquo;applications externes dans l\u0026rsquo;environnement Teams que l\u0026rsquo;utilisation des API qu\u0026rsquo;ils mettent à disposition dans des applications externes.\n→ Les deeplinks\nLes deeplinks sont des liens intelligents capables de rediriger vers une url spécifique, inaccessible via un lien classique. Ce qui est le cas pour les urls de conversation Teams.\nLa structure des deeplinks vers une conversation est la suivante :https://teams.microsoft.com/l/call/0/0?users=\u0026lt;user1\u0026gt;,\u0026lt;user2\u0026gt;. Elle nécessite donc uniquement les identifiants (emails) des utilisateurs concernés.\nCette solution nécessite effectivement que le client ait un compte Teams, et que nous en connaissions l\u0026rsquo;email.\nDe plus, selon la configuration du tenant et les restrictions d\u0026rsquo;accès que cela peut engendrer, il est possible que des utilisateurs d\u0026rsquo;un certain tenant ne puissent pas discuter avec des utilisateurs Teams d\u0026rsquo;un tenant externe au leur. Une solution existe : inviter des utilisateurs externes au tenant dans son tenant : ils auront alors un statut guest, impliquant des restrictions sur l\u0026rsquo;utilisation de Teams, mais permettant de discuter avec les utilisateurs du tenant.\nAprès avoir accepté les conditions, le client pourra donc échanger avec son contact via l\u0026rsquo;interface Teams.\nNous avons finalement choisi l\u0026rsquo;utilisation des deeplinks, de par leur facilité d\u0026rsquo;utilisation et par la solution qu\u0026rsquo;ils apportent à notre besoin.\nCependant, nous avons vu là uniquement les solutions d\u0026rsquo;échanges instantanés. Mais les API Microsoft Graph permmettent aussi la planification de meetings, autrement dit la lecture / écriture dans les agendas des collaborateurs.\nOrganiser des meetings Nous avons découpé ce cas d\u0026rsquo;utilisation en 3 parties :\n Accéder aux disponibilités de notre contact  En voici la requête ainsi que la réponse :\nPOST https://graph.microsoft.com/v1.0/me/calendar/getSchedule Prefer: outlook.timezone=\u0026quot;Pacific Standard Time\u0026quot; Content-Type: application/json { \u0026quot;schedules\u0026quot;: [\u0026quot;adelev@contoso.onmicrosoft.com\u0026quot;], \u0026quot;startTime\u0026quot;: { \u0026quot;dateTime\u0026quot;: \u0026quot;2021-06-15T09:00:00\u0026quot;, \u0026quot;timeZone\u0026quot;: \u0026quot;Pacific Standard Time\u0026quot; }, \u0026quot;endTime\u0026quot;: { \u0026quot;dateTime\u0026quot;: \u0026quot;2021-06-15T18:00:00\u0026quot;, \u0026quot;timeZone\u0026quot;: \u0026quot;Pacific Standard Time\u0026quot; }, \u0026quot;availabilityViewInterval\u0026quot;: 60 } HTTP/1.1 200 OK Content-type: application/json { \u0026quot;value\u0026quot;: [ { \u0026quot;scheduleId\u0026quot;: \u0026quot;adelev@contoso.onmicrosoft.com\u0026quot;, \u0026quot;availabilityView\u0026quot;: \u0026quot;000220000\u0026quot;, \u0026quot;scheduleItems\u0026quot;: [...], \u0026quot;workingHours\u0026quot;: { \u0026quot;daysOfWeek\u0026quot;: [...], \u0026quot;startTime\u0026quot;: \u0026quot;08:00:00.0000000\u0026quot;, \u0026quot;endTime\u0026quot;: \u0026quot;17:00:00.0000000\u0026quot;, \u0026quot;timeZone\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;Pacific Standard Time\u0026quot; } } }, ] } Les éléments intéressants de la réponse à cette requête sont :\n le champs scheduleItems, qui \u0026ldquo;Contient les éléments qui décrivent la disponibilité de l’utilisateur\u0026rdquo;, autrement dit les réunions déjà planifiées de la personne ou de l\u0026rsquo;entité. le champs availabilityView, qui \u0026ldquo;Représente une vue fusionnée de la disponibilité de tous les éléments dans scheduleItems . L’affichage se compose de créneaux horaires. La disponibilité de chaque créneau horaire est indiquée par : 0 = libre, 1 = provisoire, 2 = occupé, 3 = absent du bureau, 4 = travail ailleurs.\u0026quot;  C\u0026rsquo;est donc ce dernier champs que l\u0026rsquo;on peut exploiter pour organiser des rendez-vous dans les créneaux disponibles du contact.\nCréer un meeting dans un agenda  En voici la requête ainsi que la réponse :\nPOST https://graph.microsoft.com/v1.0/me/events Prefer: outlook.timezone=\u0026quot;Pacific Standard Time\u0026quot; Content-type: application/json { \u0026quot;subject\u0026quot;: \u0026quot;Let's go for lunch\u0026quot;, \u0026quot;body\u0026quot;: { \u0026quot;contentType\u0026quot;: \u0026quot;HTML\u0026quot;, \u0026quot;content\u0026quot;: \u0026quot;Does noon work for you?\u0026quot; }, \u0026quot;start\u0026quot;: {...}, \u0026quot;end\u0026quot;: {...}, \u0026quot;location\u0026quot;:{...}, \u0026quot;attendees\u0026quot;: [...], \u0026quot;allowNewTimeProposals\u0026quot;: true, \u0026quot;isOnlineMeeting\u0026quot;: true, \u0026quot;onlineMeetingProvider\u0026quot;: \u0026quot;teamsForBusiness\u0026quot; } Comme on peut le voir, plusieurs champs sont à renseigner pour créer la réunion. Mais ce sont les 2 champs isOnlineMeeting et onlineMeetingProvider qui vont permettre de créer une réunion Teams en ligne (si onlineMeetingProvider vaut teamsForBusiness) associée à la réunion et en générer le lien en réponse.\nHTTP/1.1 201 Created Content-type: application/json { \u0026quot;subject\u0026quot;:\u0026quot;Let's go brunch\u0026quot;, \u0026quot;bodyPreview\u0026quot;:\u0026quot;Does noon work for you?\u0026quot;, \u0026quot;webLink\u0026quot;:\u0026quot;https://outlook.office365.com/...\u0026quot;, \u0026quot;isOnlineMeeting\u0026quot;: true, \u0026quot;onlineMeetingProvider\u0026quot;: \u0026quot;teamsForBusiness\u0026quot;, \u0026quot;body\u0026quot;:{...}, \u0026quot;start\u0026quot;:{...}, \u0026quot;end\u0026quot;:{...}, \u0026quot;attendees\u0026quot;:[...], \u0026quot;organizer\u0026quot;:{ \u0026quot;emailAddress\u0026quot;:{ \u0026quot;name\u0026quot;:\u0026quot;Dana Swope\u0026quot;, \u0026quot;address\u0026quot;:\u0026quot;danas@contoso.onmicrosoft.com\u0026quot; } }, \u0026quot;onlineMeeting\u0026quot;: { \u0026quot;joinUrl\u0026quot;: \u0026quot;https://teams.microsoft.com/l/meetup-join/...\u0026quot;, \u0026quot;conferenceId\u0026quot;: \u0026quot;conferenceId\u0026quot;, \u0026quot;tollNumber\u0026quot;: \u0026quot;tollNumber\u0026quot; } } Nous pouvons voir le champs onlineMeeting en réponse, qui fournit une url pour rejoindre le meeting ainsi que les informations nécessaires pour le rejoindre par téléphone.\nAccéder à nos prochains RDV  Cette requête renvoit en réponse plus ou moins les mêmes informations que la réponse à la création de meeting, dont le lien vers la réunion Teams s\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;une réunion en ligne, et toutes les informations dont on pourrait avoir besoin.\nCes fonctionnalités sont faciles à exploiter et peuvent apporter une vraie valeur à l\u0026rsquo;application.\nSeulement, utiliser les API Teams dans notre application n\u0026rsquo;est pas le seul moyen d\u0026rsquo;utiliser les outils Microsoft. Il est également possible d\u0026rsquo;exposer notre application dans l\u0026rsquo;application Teams.\nIntégrer une application dans Teams Une solution très intéressante est donc de créer une application et de l\u0026rsquo;intégrer dans l\u0026rsquo;environnement Teams. Il existe plusieurs types d\u0026rsquo;application que l\u0026rsquo;on peut ajouter : des bots, des webhooks, des outils de messagerie et des contenus web intégrés dans des onglets de Teams, comme le montre le schéma ci-dessous :\nPour notre POC, nous avons créé une application web intégrée dans un onglet de Teams en suivant ce tutoriel.\nL\u0026rsquo;application aura besoin entre autres d\u0026rsquo;un fichier manifest.json pour la décrire et indiquer à Teams où est exposée l\u0026rsquo;application (voir le champs staticTabs ci-dessous), ainsi qu\u0026rsquo;un grand nombre d\u0026rsquo;autres informations.\nVoici une partie du manifest à fournir :\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://developer.microsoft.com/en-us/json-schemas/teams/v1.8/MicrosoftTeams.schema.json\u0026#34;, \u0026#34;manifestVersion\u0026#34;: \u0026#34;1.8\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;{{APPLICATION_ID}}\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;{{VERSION}}\u0026#34;, \u0026#34;packageName\u0026#34;: \u0026#34;{{PACKAGE_NAME}}\u0026#34;, \u0026#34;developer\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;\u0026lt;app_name\u0026gt;\u0026#34;, \u0026#34;websiteUrl\u0026#34;: \u0026#34;https://{{HOSTNAME}}\u0026#34;, \u0026#34;privacyUrl\u0026#34;: \u0026#34;https://{{HOSTNAME}}/privacy.html\u0026#34;, \u0026#34;termsOfUseUrl\u0026#34;: \u0026#34;https://{{HOSTNAME}}/tou.html\u0026#34; }, \u0026#34;staticTabs\u0026#34;: [ { \u0026#34;entityId\u0026#34;: \u0026#34;\u0026lt;my_id\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;\u0026lt;display_name\u0026gt;\u0026#34;, \u0026#34;contentUrl\u0026#34;: \u0026#34;https://{{HOSTNAME}}/\u0026lt;my_tab\u0026gt;/\u0026#34;, \u0026#34;scopes\u0026#34;: [ \u0026#34;personal\u0026#34; ] } ] } Une fois que tout est prêt, et afin de permettre aux développeurs de tester leur application, il va falloir autoriser le chargement d\u0026rsquo;une application locale dans Teams. Seul un administrateur peut activer cette option. Il sera alors possible d\u0026rsquo;ajouter l\u0026rsquo;application en tant qu\u0026rsquo;utilisateur dans Teams, afin de faire profiter à tous les collaborateurs de la nouvelle application !\nBilan Il est maintenant temps de faire un bilan de tout ce travail :\n Microsoft offre un grand nombre de possibilités pour utiliser ses services, que ce soit par ses API Graph, les deeplinks, Azure Communication Services et l\u0026rsquo;intégration d\u0026rsquo;applications dans Teams. Il est intéressant de savoir utiliser ces outils, car ils peuvent sans nul doute susciter l\u0026rsquo;intérêt des clients. La documentation est exhaustive et très complète, à priori on peut y trouver tout ce dont on a besoin pour utiliser leurs API. L\u0026rsquo;utilisation de leurs API en environnement de développement n\u0026rsquo;était pas forcément très stable (trop sollicités), donc attention aux démos ! Pouvoir customiser son environnement Teams avec des petites applications utiles peut être bien cool et peut encourager à plus utiliser Teams.  ","date":"Apr 29, 2021","href":"https://blog.talanlabs.com/2021-04-29-rex-api-o365/","kind":"page","labs":null,"tags":["api","microsoft","rex"],"title":"REX : Intégration des API Microsoft"},{"category":null,"content":"Animez un sprint planning facilement Il y a parfois des contextes où le Coach de l’équipe n’est pas présent…​\n Si les objectifs de la réunion ne sont pas partagés, que l’équipe est récente et que les gens ont du mal à se comprendre, Alors le sprint planning peut devenir une réunion difficile à animer et très agaçante, voire interminable…​\n Que doit-on faire dans ce genre de cas ?\n Il m’est arrivé dans certaines missions de ne pas être disponible pour animer le sprint planning des équipes. J’ai alors créé un document pas-à-pas pour animer le Sprint Planning sans facilitateur de métier. En suivant ces étapes, le sprint planning devrait se dérouler correctement.\n  Présentation du déroulé et des objectifs de la réunion (obtenir un plan pour l’accomplissement de l’objectif de sprint) – 2min\n  Définir et mandater un facilitateur, ainsi que quelqu’un qui clôturera la réunion si elle est trop longue.\n  Optionnel : Repasser sur le dernier sprint backlog, passer les tickets dans le sprint backlog suivant, ou les clôturer.\n  Présentation de l’objectif de sprint, par le besoin – 5min\n  L’objectif semble-t-il réalisable ? Sinon, peut-on le découper ou allonger/réduire la taille du prochain sprint ? – 5 min\n  Commencer le découpage fonctionnel (ou présentation rapide du découpage fonctionnel réalisé en backlog refinement) : - 10min\n  Présentation de chaque US séparément, s’assurer que l’US est bien claire pour tout le monde. – 1h max\n  Présentation de chaque US de la part des développeurs sur la façon de réaliser l’US\n     Téléchargez le document Sprint Planning Facile\n Si vous avez également une rétrospective à faire, vous pouvez aussi utiliser mon format de retrospective Tour de France !\n   ","date":"Apr 19, 2021","href":"https://blog.talanlabs.com/sprint-planning-facile/","kind":"page","labs":null,"tags":["Sprint Planning","Scrum","Agilité","Auto-organisation"],"title":"Animez un Sprint Planning Sans Coach Agile"},{"category":null,"content":"Dédoublonner les messages d\u0026rsquo;un channel en go Les channels en Go sont des files threadsafe très utiles. Voyons un cas d\u0026rsquo;usage pour gérer des événements.\nContexte : Inotify et la détection d\u0026rsquo;événements sur un disque Afin de pouvoir détecter la création de nouveaux fichiers dans un répertoire, j\u0026rsquo;ai utilisé la commande Inotify sous Linux.\nInotify est un mécanisme sur linux permettant d\u0026rsquo;observer les événements sur un répertoire : création, suppression, modification de fichier ou répertoire.\nL\u0026rsquo;idée est de détecter ces nouveaux fichiers comme des événements pour ensuite effectuer un traitement, dans mon cas, les copier dans un autre répertoire.\nFs notify est une librairie Go qui encapsule ce mécanisme à l\u0026rsquo;identique en poussant les événements dans un channel. A noter que Fs notify est multi-plateforme et n\u0026rsquo;utilise Inotify que sur linux.\nDétection des événements Nous allons observer les événements qui se produisent sur un répertoire lorsque l\u0026rsquo;on copie des fichiers :\nfunc main(){ watcher,_ := fsnotify.NewWatcher() for { if value, hasMore := \u0026lt;- watcher.Events; hasMore { switch { case value.Op\u0026amp;fsnotify.Create == fsnotify.Create: log.Println(\u0026#34;CREATE\u0026#34;, value.Name) break case value.Op\u0026amp;fsnotify.Write == fsnotify.Write: log.Println(\u0026#34;WRITE\u0026#34;, value.Name) break case value.Op\u0026amp;fsnotify.Remove == fsnotify.Remove: log.Println(\u0026#34;DELETE\u0026#34;, value.Name) break } }else{ break } } } Après avoir lancé le code, on copie deux fichiers, file1.jpg et file2.jpg :\n2021/04/07 12:02:23 main.go: CREATE file1.jpg 2021/04/07 12:02:23 main.go: WRITE file1.jpg 2021/04/07 12:02:23 main.go: WRITE file1.jpg 2021/04/07 12:02:23 main.go: CREATE file2.jpg 2021/04/07 12:02:23 main.go: WRITE file2.jpg 2021/04/07 12:02:23 main.go: WRITE file2.jpg 2021/04/07 12:02:23 main.go: WRITE file2.jpg On s\u0026rsquo;attend à avoir 2 lignes pour chaque fichier, une pour la création et l\u0026rsquo;autre pour l\u0026rsquo;écriture. Or plusieurs événements sont lancés pour l\u0026rsquo;écriture (WRITE).\nInotify ne déclenche pas un événement WRITE lors de la fermeture du fichier mais apparemment lors de chaque flush. Sur des fichiers plus gros, on peut observer encore plus d\u0026rsquo;événements.\nEvénement et idempotence Le fait d\u0026rsquo;effectuer plusieurs fois le même traitement n\u0026rsquo;est problématique que si notre traitement n\u0026rsquo;est pas idempotent, c\u0026rsquo;est à dire que le résultat d\u0026rsquo;un même traitement est toujours le même. Dans l\u0026rsquo;objectif de surveiller un répertoire est de recopier les nouveaux fichiers, la copie est idempotente et on pourrait laisser ainsi.\nCependant, il n\u0026rsquo;est jamais bon de laisser des écritures inutiles se faire quand on peut l\u0026rsquo;éviter :\n hausse des I/O hausse de l\u0026rsquo;utilisation du cpu usure prématurée des disques problème de concurrence si les copies sont effectuées en parallèle  Comment garder le dernier événement ? Une solution serait de dédoublonner pour ne garder que le dernier événement d\u0026rsquo;un fichier, celui se produisant lors de la fermeture.\nEn reprenant l\u0026rsquo;exemple précédent, on voit que les événements reçus sont groupés par fichier ce qui est logique : la copie de fichiers n\u0026rsquo;est pas parallèle mais séquentielle.\n2021/04/07 12:02:23 main.go: CREATE file1.jpg 2021/04/07 12:02:23 main.go: WRITE file1.jpg 2021/04/07 12:02:23 main.go: WRITE file1.jpg \u0026lt;-- Bien 2021/04/07 12:02:23 main.go: CREATE file2.jpg 2021/04/07 12:02:23 main.go: WRITE file2.jpg 2021/04/07 12:02:23 main.go: WRITE file2.jpg 2021/04/07 12:02:23 main.go: WRITE file2.jpg \u0026lt;-- Bien Découpler la détection du traitement d\u0026rsquo;un événement avec les channels en Go Voici un algo qui va pouvoir corriger ce problème :\n Tant que le fichier sur lequel est lancé l\u0026rsquo;événement est le même, on l\u0026rsquo;ignore Quand le nom du fichier change, on le traite Si pendant un certain temps, il n\u0026rsquo;y a plus d\u0026rsquo;événement, on considère que le dernier reçu peut être traité  Il est possible de gérer facilement ce traitement avec un tableau et une analyse régulière des événéments.\nMais l\u0026rsquo;utilisation des channels apporte beaucoup plus de souplesse :\n Découplage entre la détection des événements et leur traitement Gestion du parallélisme Les événements sont traités de manière synchrone (à la queue leu leu) Les channels c\u0026rsquo;est cool :)  Implémentation Pour gérer ce mécanisme avec des channels, on va créer une structure avec deux channels :\n Le premier pour lire les événements de fs notify Le second pour écrire les événéments dédoublonnés  type Agregatechannel struct { inputChannel chan string // Lecture des événements \toutputChannel chan string // Ecriture des événements sans doublon \tpreviousValue string // Précédent fichier lu \ttimeout time.Duration // Temps au delà duquel on estime que le dernier événement peut être traité } Pour gérer la notion de timeout, on va utiliser le mécanisme de select avec un Timer.\nLe select permet d\u0026rsquo;écouter plusieurs channels et de traiter le premier dans lequel on lit un message. Un timer est un channel dans lequel l\u0026rsquo;heure est écrite au bout du temps défini, parfait pour gérer un timeout :\nfunc (ag Agregatechannel)readChanWithTimeout(){ select { case value := \u0026lt;-ag.inputChannel: ag.manageValue(value) break case \u0026lt;-time.NewTimer(ag.timeout).C: ag.manageTimeout() } } La méthode manageValue implémente l\u0026rsquo;algorithme décrit plus haut :\nfunc (ag *Agregatechannel)manageValue(value string){ if strings.EqualFold(\u0026#34;\u0026#34;, ag.previousValue) { ag.previousValue = value } else { // Si la valeur est différente de la précédente, on traite l\u0026#39;événement de la précédente valeur \tif !strings.EqualFold(ag.previousValue, value) { ag.outputChannel \u0026lt;- ag.previousValue ag.previousValue = value } } } En masquant l\u0026rsquo;implémentation à base de channel, avec une méthode Add et une méthode Get, on obtient une structure simple et légère.\nEt voilà\u0026hellip;en fixant un timeout de 2s, on obtient des résultats cohérents et je limite mes écritures au strict minimum.\nLa vraie force de l\u0026rsquo;algorithme présenté réside dans l\u0026rsquo;utilisation du select. Un cas d\u0026rsquo;usage intéressant est la sélection de la réponse la plus rapide lors de l\u0026rsquo;interrogation de plusieurs services / providers : celui qui a la réponse en premier sera choisi.\nVous pouvez retrouver le code source complet sur github.\nMerci beaucoup à Julien Rollin pour les conseils et la relecture.\n","date":"Apr 9, 2021","href":"https://blog.talanlabs.com/dedoublonner-messages-channel-go/","kind":"page","labs":["Lab 9"],"tags":["golang","go","channel","inotify"],"title":"Dédoublonner les messages d'un channel en Go"},{"category":null,"content":"Des individus et leurs interactions : le ROTI 🍖 ! Quoi de mieux qu\u0026rsquo;un repas convivial pour converser et échanger. Je vous propose de sauter l\u0026rsquo;entrée pour attaquer directement le ROTI !\nL\u0026rsquo;un des quatre principes Agile est de favoriser les individus et leurs interactions. Mais en utilisant le framework Scrum, j’entends souvent : “Il y a trop de réunions !”. Du coup, comment rendre ces échanges le plus efficace possible ? Et ainsi convaincre que chaque réunion est utile, ou non !\nPour répondre à cette question, je vous propose un outil simple : le R.O.T.I. C’est l\u0026rsquo;acronyme de “Return On Time Invested” : avez-vous bien dépensé votre temps ?\nEn voici le mode opératoire. Mais, ensuite, il y aura d\u0026rsquo;autres pistes pour aller plus loin.\nMode opératoire en 2 temps Étape 1 : La note Juste avant de terminer votre réunion, demandez à l’ensemble des participants de voter en même temps entre 1 et 5 afin que les uns n\u0026rsquo;influent pas les autres.\nSachant que :\n 5️ vaut \u0026lsquo;Excellent\u0026rsquo;, soit beaucoup plus que le temps passé : Haute valeur 4️⃣ vaut \u0026lsquo;Au-dessus de la moyenne\u0026rsquo;, soit plus de que le temps passé : Bonne valeur 3️⃣ vaut \u0026lsquo;Moyen\u0026rsquo;, soit autant que le temps passé : Valeur 2️⃣ vaut \u0026lsquo;Utile\u0026rsquo;, soit moins que le temps passé : Faible valeur 1️⃣ vaut \u0026lsquo;Inutile\u0026rsquo;, ne vaut rien, c\u0026rsquo;est une perte de temps : Aucune valeur  En présence physique, chaque participant peut utiliser une main levée. En télétravail, chaque participant envoie sa réponse en texte via votre outil favori.\nL’objectif est d’avoir un feedback instantané que tout le monde peut donner. Même les timides pourront s’exprimer.\nÉtape 2 : Le complément Une fois que tout le monde a voté, inviter toutes les personnes à compléter leur note d’un commentaire oral. Rien d’obligatoire mais l’objectif est de comprendre pourquoi la personne a mis sa note et favoriser l\u0026rsquo;échange.\nAlors je mets quelle note ? 😕 Les notes 1️⃣ et 2️⃣ sont très intéressantes car ce sont elles qui apporteront des améliorations à la réunion. Il faut donc tenter d’avoir systématiquement une explication sur ces notes.\nLes notes de 4️⃣ et 5️ ne sont pas à laisser de côté car elles apportent du positif. Elles peuvent dynamiser les autres participants. Et ça on prend !\nLa note de 3️⃣ est la note attendue. Même moyenne, ce n\u0026rsquo;est pas une mauvaise note car c\u0026rsquo;est ce vers quoi on tend.\nLes dérives 😩 “Allez je mets 3️⃣\u0026hellip; Comme cela je ne serai pas obligé de donner mon commentaire !”\nOn serait tenté de contourner ce genre de vote en retirant cette note neutre de 3️⃣. On ferait alors un ROTI sur 4️⃣ et non sur 5️ afin que les participants prennent plus position : soit sur une note positive, soit sur une note négative. Mais alors, dans leur logique, ces personnes mettent 4️⃣ au lieu de 3️⃣\u0026hellip; Je ne suis pas pour cette pratique car 3️⃣ n’est pas une mauvaise note ! C’est la note standard : Moyenne - Vaut autant que le temps passé : Valeur. Et j’aime conserver la super note de 5️ car quand elle est donnée, le message est vraiment impactant. Tout comme le 1️⃣, en symétrie. Ce sont des messages forts qui sont de super retour.\n⚠️ Attention au timing On souhaite avoir un retour rapide en un minimum de temps et sans déborder sur le temps qui suit. Aussi je préconise de prendre environ 5 minutes avant la fin de votre réunion, atelier ou événement et pas plus. On veut éviter de tomber dans de nouveaux débats. Stopper donc toutes les tentatives de réponses sur chaque commentaire de note si vous voulez libérer les participants à l’heure. Pour autant si des commentaires sont intéressants à débattre, ils peuvent être abordés à nouveau en dehors de la réunion : Soit juste après avec les personnes le souhaitant Soit plus tard lors d’une rétrospective ou un atelier dédié. A vous de voir.\nAllons plus loin Action ! Bien sûr, l\u0026rsquo;objectif premier de cet outil est d’avoir des échanges directs entre tous les participants. Mais c’est aussi de savoir ce qui a marché ou non afin de progresser dans l’efficacité de la réunion. Si des axes de progression sont identifiés, il faudra les mettre en place. C’est un peu comme une mini rétrospective.\nSatisfaction ? Le piège à éviter est de considérer que le ROTI est la satisfaction pour cette réunion. Nous sommes sur le temps investi ! Pour bien séparer, je préconise un feedback en 2 temps : une question sur la satisfaction puis le ROTI. Ainsi nous pourrons bien différencier les deux. Je m’en sers régulièrement pendant mes revues d’itération afin que les personnes nous remontent des axes d’améliorations sur notre support et sur le fait que l’équipe a bien avancé ou non.\nHistorisation Utiliser à toutes les réunions et avec une feuille Excel, on peut faire des statistiques de suivi sur les réunions. Et ainsi suivre la qualité de la réunion : Inspection !\nBon appétit à tous ! 😉\n","date":"Mar 19, 2021","href":"https://blog.talanlabs.com/commencons-par-le-roti/","kind":"page","labs":null,"tags":["agile","scrum","outil","roti"],"title":"Commençons par le ROTI"},{"category":null,"content":"Cet article est le premier d’une suite de trois articles, que vous pourrez retrouver ici quand ils seront sortis :\n   Contrat de coaching : agir POUR son client\n  La production du contrat de coaching et son entretien\n  Le suivi du contrat\n   Pourquoi devez vous passer du temps à faire un mandat ? Le mandat de coaching, c’est un outil très simple et très efficace.\n Pour le coach, c’est un moyen de clarifier sa mission et ce qui est attendu du client. Est-ce qu’il n’est pas préférable de commencer par agir sur ce qu’il veut, pour gagner sa confiance et ensuite proposer d’aller plus loin ?\n Expliciter les moyens et les résultats attendus d’une mission dès le début, est une base très solide.\n Pour les coachés, le mandat permet surtout à un ensemble de personnes de se réunir autour d’actions concrètes et à suivre leur avancement. C’est un moyen pertinent d’analyse critique d’une organisation.\n Dans cet article, je vous propose ma façon d’expliciter mon mandat. Ce n’est peut-être pas celle qui vous conviendra le mieux, l’approche étant personnelle.\n   Un contrat comme support Le modèle de mandat le plus simple pour moi est le contrat. C’est avant tout pour générer de la discussion, ce qui permet de faire le tour des questions et s’aligner avec le client. Il est aussi une trace écrite de la discussion.\n En effet, c’est un support objectif et c’est une bonne trace d’un accord. Il faut qu’il soit clair, transparent et partagé entre toutes les parties.\n Dans ce type de contrat, le coach pourra rencontrer son client assez peu, à raison d’une fois toutes les 3 semaines. Mais en général, on préfère une haute fréquence de rencontre et le niveau de détail fait foi de la confiance entre le coach et son client.\n Néanmoins, ce contrat ne doit pas être fixé, il sera amené à évoluer dans le temps, ces éléments devront être précisés dans l’introduction. Pour le réaliser au mieux, un template devrait être proposé par le coach afin de guider la conversation qui aura lieu.\n Il pourrait alors être réalisé de cette façon :\n Intro :\n   Expliciter le contexte de la mission du point de vue du coach, avec qui a lieu le contrat et pourquoi.\n  Décrire l’état actuel de la situation, qui est la personne/entité avec qui l’accord est conclu, quels sont les problèmes et pourquoi doit-on agir dessus.\n  Définir aussi la fréquence de révision du contrat.\n   Partie I : La vision idéale, faire rêver.\n   Après intervention du coach, comment serait le contexte ?\n  Qu’est-ce qui ferait dire que le coach a bien travaillé ?\n  Des éléments très concrets ne sont pas toujours nécessaires dans ce genre de cas.\n   Partie II : La vision court terme.\n   C’est le plus difficile et c’est la partie sur laquelle on passe le plus de temps.\n  Quelle doit être la première étape pour aller vers l’idéal ?\n  Comment doit-on s’y prendre pour l’atteindre ?\n  Quels sont les moyens objectifs de nous dire que cette étape sera passée ?\n   Conclusion :\n   Le coach et le client s’assurent que chaque action notée est pertinente et réalisable pour le prochain entretien.\n  L’issue du mandat est plutôt de chercher des actions pour le coach, mais des actions pour les intéressés sont envisageable également.\n   Conclusion Vous l’avez vu, avec le contrat, on fixe son intervention. Il existe d’autre façons de faire un mandat, qui sont plus souples.\n Ici, il est avant tout question d’échanger avec les accompagnés !\n Si vous souhaitez aller plus loin, je vous invite à voir la vidéo de Scrum Life avec Vincent Loisy.\n Au prochain article, nous verrons comment produire le contrat de coaching et mener l’entretien.\n    ","date":"Mar 12, 2021","href":"https://blog.talanlabs.com/mandat-de-coaching-part1/","kind":"page","labs":null,"tags":["Agilité","contrat de coaching","coaching"],"title":"Contrat de coaching : agir POUR son client?"},{"category":null,"content":"Identifié parmi le Top 10 des tendances technologiques stratégiques selon Gartner en 2018 [1], le concept de jumeau numérique - Digital Twin - suscite de plus en plus d’intérêt dans différents secteurs comme l’industrie, la santé, les smart cities ou encore le transport. Même si aujourd’hui, peu d’entreprises exploitent les jumeaux numériques en production, selon le cabinet Gartner, environ deux tiers des entreprises qui ont déployé de l’IoT devraient mettre en place au moins un jumeau numérique en production d’ici 2022 [2].\nLe concept de jumeau numérique n’est pas nouveau. Au début des années soixante-dix, la NASA a été la première à expérimenter l’ancêtre du jumeau numérique – Pairing technology – dans son programme Applo. Le jumeau numérique a été introduit en 2002, par le Dr. Michael Grieves, spécialiste de la gestion du cycle de vie des produits à l’université de Michigan. Le concept s’appelait alors « Conceptual Ideal for Product Lifecycle Management » [3] et comportait en lui les prémisses du jumeau numérique. En 2012, la NASA définie dans son papier « The Digital Twin Paradigm for Future NASA and U.S. Air Force Vehicles », les éléments clés pour mettre en œuvre un jumeau numérique [4]. Depuis, le jumeau numérique s’est installé dans différents secteurs et offre de nombreuses possibilités pour améliorer la prise de décision, renforcer la performance des produits, optimiser les opérations de maintenance, planifier les activités d’une chaîne de production toute entière, etc.\nQu’est qu’un jumeau numérique ? Le jumeau numérique est une représentation dynamique virtuelle d’un objet, d’un système (ensemble d’objets) ou d’un processus du monde réel. Le jumeau numérique se base sur les données collectées en temps réel, combinés à la data analytics, la simulation et le Machine Learning pour comprendre, anticiper et mieux décider. Le Digital Twin a un objectif fondamental : modéliser le comportement des systèmes du monde réel pour permettre de meilleures décisions.\nLe jumeau numérique n’existe que si les éléments suivants sont réunis :\n Un objet ou un système physique dans son environnement réel, Son double dans un espace virtuel, Une communication entre le monde physique et le monde virtuel.  Le Digital Twin permet :\n de comprendre : le double virtuel est alimenté en continu par les données du monde réel. Ces données sont remontées via les capteurs puis analysées et historisées offrant ainsi une vision précise du monde réel. de simuler : le jumeau numérique permet, grâce à la simulation, d’explorer de différents scénarios. Il peut prévoir les modifications auxquelles va être soumis le système physique en se basant sur une évaluation de l’état simulé, sur les modèles établis à partir des données collectées précédemment. d’agir : le jumeau numérique recommande des plans d’actions à mener à court ou moyen terme. L’opérateur peut décider de les mettre en place soit virtuellement (pour évaluer les impacts) soit physiquement.  Le jumeau numérique n’est pas à confondre avec :\n Le modèle numérique « Digital Model » : le Digital Model est une version numérique d\u0026rsquo;un objet physique préexistant ou planifié, exemple, la maquette 3D d’un réacteur ou le plan d’un bâtiment. Il n’y a aucun échange automatique de données entre le modèle physique et le modèle numérique. Ce que veut dire, qu’une fois le modèle numérique créé, toute modification apportée au l\u0026rsquo;objet n\u0026rsquo;a aucun impact sur le modèle numérique. L’ombre numérique « Digital Shadow » : le Digital Shadow est une représentation numérique d\u0026rsquo;un objet avec un flux de données à sens unique entre l\u0026rsquo;objet physique et numérique. Un changement d\u0026rsquo;état de l\u0026rsquo;objet physique entraîne un changement dans l\u0026rsquo;objet numérique mais le contraire est faux.  Dans le Digital Twin, les données circulent entre un objet physique et le double numérique dans les deux sens. Un changement sur l\u0026rsquo;objet physique entraîne automatiquement une modification sur le double numérique et vice versa. L’avantage principal de cette technologie est d’avoir une liaison en temps réel entre le monde physique et le monde virtuel. Le modèle physique alimente en continu son jumeau virtuel en données. Ces dernières sont recueillies par le biais d’un très grand nombre de capteurs situés sur l’objet ou le système physique. Ce qui va permettre d’obtenir une vision en temps réel de l’état du système.\nDes technologies qui ont permis le développement des jumeaux numériques Trois technologies émergentes ont permis le développement des jumeaux numériques :\n Les outils de conception assistée par ordinateur (CAO) et de modélisation 3D : le jumeau numérique trouve ses fondations dans la conception assistée par ordinateur (CAO), qui permet la conception et la représentation d’objets statiques en 3D. Aujourd\u0026rsquo;hui, ces outils sont devenus plus performants et permettent de représenter et simuler les performances des produits et des systèmes complexes. Le développement rapide de l’IoT : la maturité du secteur de l’IoT, le faible coût des capteurs et le développement des réseaux permettent de collecter, en temps réel, un grand volume de données à partir de n\u0026rsquo;importe quel ensemble d\u0026rsquo;actifs physiques. La puissance des algorithmes IA et des outils d’analyse prédictive : le développement des algorithmes de Machine Learning et des outils d’analyse de données nous offrent de nouvelles capacités pour mieux comprendre, mieux anticiper et mieux décider en analysant le grand volume des données remontées par les capteurs.  Le jumeau numérique est le fruit de la convergence de ces technologies et permet d’en tirer un maximum de valeur.\nQuels bénéfices aux jumeaux numériques ? Dans un environnement où les entreprises sont contraintes de réduire leurs coûts, de s’adapter toujours plus vite, de faire preuve d’une grande réactivité et capacité à anticiper les évolutions du marché, les applications du jumeau numérique sont multiples et offrent de nombreux bénéfices :\n  Des produits (et systèmes) mieux conçus : la visualisation de la progression de la construction d’un objet ou d’un système sur son jumeau numérique permet de comparer en temps réel les développements avec la cible finale. Le Digital Twin peut ainsi garantir que les exigences capturées au cours des premières phases du cycle de vie d\u0026rsquo;un produit sont maintenues, vérifiées et validées dès la conception et au fur et à mesure que le produit évolue.\n  Surveillance et contrôle à distance en temps réel : il est très difficile d’avoir une vue approfondie d’un grand système, physiquement et en temps réel. Par sa nature même, le jumeau numérique peut être accessible, n’importe quand et n\u0026rsquo;importe où, pour donner une vue précise et en temps réel d’un objet ou d’un système. Les performances du système peuvent non seulement être surveillées mais également contrôlées à distance.\n  Maintenance prédictive et planification : grâce aux quantités de données remontées par les capteurs, le jumeau numérique permet de mener des analyses et d’identifier en amont un risque de défaillance. Ce qui permet une meilleure planification de la maintenance.\n  Formation : alors que les systèmes deviennent de plus en plus complexes et que les experts se rapprochent de la retraite, l\u0026rsquo;utilité d\u0026rsquo;un jumeau numérique comme outil de formation prend de l\u0026rsquo;ampleur. Pour un nouvel utilisateur, toutes les informations sont disponibles dans le jumeau numérique (documentation technique, modèle 3D, processus, historiques, etc.). Il peut aussi simuler différentes situations, ce qui accélère le processus d’acquisition de connaissances.\n  Un système d\u0026rsquo;aide à la décision plus efficace et mieux informé : la disponibilité de données quantitatives et d\u0026rsquo;analyses avancées en temps réel permet aux organisations d’être mieux informées et de prendre rapidement les décisions.\n  Mieux collaborer : le Digital Twin ouvre de nouvelles possibilités pour la co-création et la collaboration intra et inter équipes. En effet, toutes les parties prenantes peuvent collaborer à distance et dans les mêmes conditions que dans le monde réel.\n  Meilleures documentation et communication : des informations disponibles facilement et en temps réel, combinées à du reporting automatique, permettent de garder toutes les parties prenantes bien informées améliorant ainsi la transparence.\n  Ces nouvelles possibilités, qu’offrent le Digital Twin, en termes d’optimisation des performances, ont des impacts positifs en matière de sécurité et de fiabilité, ce qui contribue à terme à renforcer la confiance des consommateurs dans l’entreprise, mais aussi en termes de réduction des coûts liés aux réparations ou défaillances des produits. Ces capacités, qui peuvent maintenant être appliquées à tout, des centrales électriques aux véhicules autonomes tout au long de leur cycle de vie, ont le potentiel de débloquer des milliards de dollars en valeur économique dans les décennies à venir.\nPour aller plus loin Dans notre prochain article, nous aborderons les jumeaux numériques d’un point de vue technique à travers une revue des quelques plateformes du marché ainsi que des recommandations pour la mise œuvre.\nRéférences  https://www.gartner.com/smarterwithgartner/gartner-top-10-strategic-technology-trends-for-2018/ https://www.gartner.com/smarterwithgartner/prepare-for-the-impact-of-digital-twins/ Michael Grieves, « Conceptual Ideal for PLM », Cours Université du Michigan, 2002 Glaessgen Edward H., Stargel, D. S., « The Digital Twin Paradigm for Future NASA and U.S. Air Force Vehicles », 53rd AIAA/ASME/ASCE/AHS/ASC Structures, Structural Dynamics and Materials Conference - Special Session on the Digital Twin, 2012.  ","date":"Mar 8, 2021","href":"https://blog.talanlabs.com/jumeaux-numeriques-au-dela-du-buzzword/","kind":"page","labs":null,"tags":["jumeau-numerique","digital-twin","iot"],"title":"Jumeaux numériques, au-delà du Buzzword"},{"category":null,"content":"Lorsque l’on veut livrer régulièrement et rapidement des fonctionnalités de qualité, on utilise une usine logicielle (Gitlab, Jenkins, Github, etc).\nQue ce soit pour une API REST développée en Go ou pour une application Front en Javascript, les grandes étapes restent les mêmes : builder, tester et déployer.\nAvant de déployer en production, il est vivement recommandé de tester l\u0026rsquo;application dans un ou plusieurs environnement(s) de recette.\nCela implique de pouvoir dynamiser certains paramètres en fonction de l’environnement.\nQuelques exemples de paramètres :\n URL de l’API utilisée car cette dernière n’est pas toujours sur le même domaine ou n’est tout simplement pas accessible hors production certains codes de mesure d’audience ou support client doivent être omis hors production afin de ne pas fausser les rapports les favicons peuvent varier pour différencier plus facilement la production de l’environnement de recette  Quelles sont les recommandations des librairies Javascript ? Avant de déployer sur votre serveur les fichiers générés, vous devez builder votre application en indiquant le fichier de configuration à utiliser.\nLes bonnes valeurs de configuration sont alors injectées dans votre code source et utilisées lors de la génération des fichiers.\n// organisation des configurations par environnement dans Angular └──myProject/src/environments/ └──environment.ts └──environment.prod.ts └──environment.stage.ts Les sites des projets Angular, Vue ou React indiquent les arguments à passer pour builder avec la bonne configuration.\n//angular ng build --prod //vuejs vue-cli-service build --mode development // react avec env-cmd env-cmd -f .env.staging npm run build Cependant, avec cette approche il est nécessaire de regénérer toute votre application autant de fois que vous avez d’environnements vers lesquels déployer.\nPourquoi ne pas injecter les paramètres au run ? Contrairement à une application en Go, NodeJs ou encore Java, le code HTML est statique.\nIl n’y a pas de processus qui fait tourner le code HTML, le code est interprété par votre navigateur. Il n’est donc pas possible d’utiliser des variables d’environnements dans le code HTML généré.\nCette notion est souvent rendue confuse car les frameworks et librairies sont souvent livrées avec beaucoup d’outils et configuration “out of the box” pour accélérer les développements\nCi-après quelques outils et optimisations qui peuvent prêter à confusion.\nLe serveur local et les outils de développement Un serveur HTTP en NodeJs (ex: webpack dev server) est souvent préconfiguré pour faciliter les développements en local (Hot reload configuration proxy, certificats SSL, etc).\nOn retrouve alors très souvent un fichier .env qui permet de surcharger le fonctionnement par défaut de ce serveur (ex: changer le port sur lequel écoute le serveur HTTP) ou encore définir des variables personnalisées.\nVotre serveur local peut lire les variables d’environnement depuis le fichier .env ou depuis votre terminal car c’est un processus qui est lancé sur votre machine.\nSite statique vs Server Side Rendering (SSR) La plupart des frameworks fournissent désormais une configuration qui permet de faire du Server Side Rendering.\nLe code Front peut alors être exécuté côté client mais également côté serveur avec Node JS.\nOn parle alors d’application isomorphique ou universelle.\nOn retrouve alors ce fichier “.env” car le site est servi par un process NodeJS et peut donc injecter des variables d’environnement.\nAttention, lors de chaque build, vous devrez toujours fournir les variables d’environnements à injecter à la commande.\nPourquoi vouloir builder une seule fois ? Les paquets NPM peuvent varier entre deux builds Les versions des paquets NPM sont souvent faiblement définies et le build d’une même application à deux moments différents peut engendrer une mise à jour non souhaitée d’un des paquets.\n// exemple de définition de dépendances \u0026#34;dependencies\u0026#34;: { \u0026#34;my_dep\u0026#34;: \u0026#34;^1.0.0\u0026#34;, \u0026#34;another_dep\u0026#34;: \u0026#34;~2.2.0\u0026#34; } Même si l’on utilise les “^” ou “~”, il est toujours possible qu’une dépendance d’une des librairies importées ne soit pas fixée…\nAu-delà d\u0026rsquo;éventuelles régressions ou incompatibilités entre certaines librairies utilisées, cela peut même introduire de nouvelles failles de sécurité.\nLa meilleure solution est de builder une seule fois votre application, analyser les fichiers utilisés et réutiliser ces mêmes fichiers dans tous vos environnements.\nFocus sécurité NPM Afin de vérifier qu’aucune dépendance requise n’est vulnérable, il est conseillé d’ajouter des vérifications dans la pipeline de votre usine logicielle.\nLa commande npm audit permet par exemple d’alerter si certaines vulnérabiltiés ont été détectées.\nnpm run audit Il est aussi possible de scanner le code à la recherche de secrets ou données sensibles.\nInutile de rappeler qu’il n’y a rien de secret dans votre navigateur et qu’il ne faut pas faire confiance au client !\nBonne pratique énoncée dans le manifeste “12 Factor App” Lors de l’apparition du Cloud, beaucoup d’applications web ont été déployées sans penser aux bonnes pratiques inhérentes à ce type d’infrastructure (élasticité, mise à l’échelle, création d’environnement à la volée, etc). Aussi, afin d’aider à l’adoption du cloud et à la mise en place de bonnes pratiques de conception, des développeurs (en particulier ceux d’Heroku) ont écrit en 2012 un manifeste qui énonce douze grands principes: 12 Factor App\nLa règle qui nous intéresse particulièrement est la suivante :\nDev/Prod parity\n Keep development, staging, and production as similar as possible\n Afin de livrer le plus rapidement possible les fonctionnalités à l’utilisateur final, l’idée est d’automatiser au maximum les process de livraison et réduire, voire supprimer, les interventions humaines.\nAussi, limiter au maximum les changements de code entre le poste du développeur et le code qui tourne sur les serveurs de production permet de détecter et corriger rapidement les anomalies.\nComment dynamiser mon application avec un seul build ? Chercher et remplacer les valeurs Une fois le code généré, la solution consiste à chercher toutes les valeurs de configuration et les remplacer par les bonnes valeurs.\nIl faudra donc utiliser un script qui détecte les motifs à trouver et les remplacer par les bonnes valeurs en fonction de l’environnement.\nUne solution consiste à utiliser la commande “envsubst”\nDOMAIN=”whatever.com” API_URL=”api.fake.com” envsubst $DOMAIN,$API_URL \u0026lt; \u0026#34;source.txt\u0026#34; \u0026gt; \u0026#34;source_replaced.txt\u0026#34; Attention aux erreurs de syntaxe si vous ne voulez pas remplacer toute une partie de votre applicatif par mégarde\u0026hellip;\nCharger la configuration depuis un fichier Javascript L\u0026rsquo;idée est de charger un fichier de configuration qui changera pour chaque environnement.\nSi nous définissons le chargement d’un script “env.js” suffisamment tôt dans le DOM HTML, le contenu sera alors disponible pour le code de notre application.\n// index.html \u0026lt;script src=\u0026#34;env.js\u0026#34; /\u0026gt; // env.js (function (window) { window.env = window.env || {}; window.env.apiUrl = \u0026#39;http://localhost:8080\u0026#39;; }(this)); // app.js const apiUrl = window.env.apiUrl C’est cette dernière solution que nous recommandons d’utiliser.\nElle permet de charger les bonnes variables sans risque d’écraser ou supprimer des lignes de code.\nLe fonctionnement est facilement compréhensible et déclinable par des ops si jamais vous n’avez pas accès à la production.\nConclusion A travers cet article, nous avons vu comment implémenter une logique de déploiement continu sans effet de bords grâce à un build unique.\nL’ajout d’un nouvel environnement de recette est très facile et la configuration devient explicite.\nA noter que cette technique n’est pas liée à un framework ou librairie, elle est déclinable pour toute application HTML qui contient du code Javascript.\nVous avez dit Web Component et Vanilla JS ?\nPour aller plus loin Dans de prochains articles, nous verrons des exemples d’implémentation de cette technique avec des frameworks comme Angular ou React.\n","date":"Feb 10, 2021","href":"https://blog.talanlabs.com/builder-une-seule-fois-son-application-javascript-et-la-deployer-a-l-infini/","kind":"page","labs":null,"tags":["craft","javascript"],"title":"Builder une seule fois son application JavaScript et la déployer à l’infini"},{"category":null,"content":"La crise sanitaire impose aux entreprises de passer à l’agilité en mode « remote » en transposant un certain nombre de rituels. Au-delà de l’outillage, il s’agit d’instaurer de bonnes pratiques organisationnelles et de revoir le mode de management.\n A première vue, les termes « agilité » et « distance » semblent antinomiques. La crise de la Covid-19 a permis de démontrer, par l’exemple, qu’il était possible de transposer à distance un grand nombre de rituels agiles sans renier les principes fondamentaux de l’agilité.\n  Retrouvez la suite de la tribune de Matthieu Riboulet sur Dirigeant.fr !\n ","date":"Jan 7, 2021","href":"https://blog.talanlabs.com/crise-agilite-distance/","kind":"page","labs":null,"tags":["Agilité","Crise","Distance"],"title":"En temps de crise, l’agilité à distance montre toute sa pertinence"},{"category":null,"content":"Quel que soit le produit développé ou le langage utilisé, il y a toujours des développeurs et des commits quelque part\u0026hellip;\nAu travers de l’historique, les commits permettent de retracer toutes les modifications effectuées sur un projet.\nNéanmoins, la qualité des messages peut beaucoup varier d’un développeur à l’autre, voire d’un jour à l’autre pour la même personne…\nQu’est-ce qu’un commit de qualité ? Quand nous regardons l’historique d’un projet versionné sous git, nous constatons parfois une suite de commits peu expressifs\u0026hellip;\ne60930a9 (HEAD -\u0026gt; my_branch, origin/mybranch) Modif de la nav 49ff1d07 Fix describe bis 49ff1d07 Fix describe 3672f204 Update conf 03d00e2e Types a5fdbc4b #261 suppress unecessary lines 886fd7af add logo and modify message for loading connections a06a1272 support logo and redirection 1dd84e9c fix reconnect insert #291 a62f7880 No comment 26f99ee0 ... d2be491c fix test bfb2f370 commit final with all fix Les développeurs arrivant sur le projet, ou pire ceux qui sont déjà sur le projet, peuvent difficilement exploiter un tel historique.\nPour citer quelques problématiques liées à un tel historique :\n difficile de distinguer les fonctionnalités des correctifs messages trog génériques dont on ne comprend pas les impacts éventuels structure des messages non homogène  Définir un format de message commun Les développeurs ont tendance à écrire des messages qui ne sont pas assez expressifs. Ils ne décrivent pas toujours les changements opérés ni l’objectif du code ajouté.\nChaque message de commit devrait pouvoir répondre aux questions suivantes :\n Quels types de changement suis-je en train de faire ?\n  Quel périmètre est affecté par mes changements ?\n  Quelles sont les modifications opérées ?\n La spécification “Conventional commit” Git ne définit pas de format de message. Cela reste du texte simple où chacun est libre de structurer son message comme il le souhaite.\nLa spécification “conventional Commit” propose d’ajouter une couche de vérification des messages avant de les écrire dans Git.\nL’objectif est de créer une convention commune au sein du projet et faciliter ainsi le partage du code ainsi que les résolutions de bugs.\nCette spécification est largement inspirée des bonnes pratiques de contribution du projet Angular\nStructure des messages Les messages sont composés comme suit :\n\u0026lt;type\u0026gt;[optional scope]: \u0026lt;subject\u0026gt; \u0026lt;BLANK LINE\u0026gt; [optional body] \u0026lt;BLANK LINE\u0026gt; [optional footer] Type\ndécrit ce que fait votre commit.\nVoici les valeurs couramment utilisées :\n feat: une nouvelle fonctionnalité fix: une correction de bug docs: un changement uniquement dans la documentation style: un changement qui n’affecte pas le sens du code (espace, reformattage, alignements dans le code…) refactor: un changement qui n’est ni une correction de bug ni une évolution perf: un changement qui améliore la performance test: un ajout de tests manquants chore: un changement sur le process de build ou des outils complémentaires  Scope\nDécrit le périmètre de la modification (ex: nom du composant, module, packages, etc)\nSubject\nDescription succincte des modifications\n impératif et temps présent pas de majuscule pas de point  Body (optionnel)\nDécrit plus en détail les objectifs et intentions qui ont motivé les changements apportés\nFooter (optionnel)\nLiens éventuels vers des outils de suivi de bugs\nA noter :\nEn cas de \u0026ldquo;Breaking change\u0026rdquo;, il est possible de l\u0026rsquo;indiquer via un \u0026lsquo;!\u0026rsquo; dans le type et dans le body en ajoutant \u0026ldquo;BREAKING CHANGE:\u0026rdquo; suivi de l\u0026rsquo;explication.\nExemples de messages attendus Fonctionnalité implémentée : Possiblité de passer commande via un bouton\nfeat(order): add purchase order button Mise à jour de la documentation pour expliquer les scripts d\u0026rsquo;initialisation du projet\ndocs(readme): document how to start project Arrêt du support Node 6\nrefactor!: drop support for Node 6 BREAKING CHANGE: refactor to use JavaScript features not available in Node 6. Vérifier la conformité des messages de commit Pour vérifier que tout le monde respecte bien le format défini par le projet, il convient de faire valider les messages.\nLa librairie “Commit Lint” est justement là pour nous aider !\nInstallation de la librairie npm install -g @commitlint/cli @commitlint/config-conventional Rq: il est également possible d’installer localement ou via npx la librairie.\nConfiguration des règles Création du fichier de config commitlint.config.js lue par la librairie\necho \u0026#34;module.exports = {extends: [\u0026#39;@commitlint/config-conventional\u0026#39;]}\u0026#34; \u0026gt; commitlint.config.js Les types autorisés par cette configuration sont : \u0026lsquo;build\u0026rsquo;, \u0026lsquo;ci\u0026rsquo;, \u0026lsquo;chore\u0026rsquo;, \u0026lsquo;docs\u0026rsquo;, \u0026lsquo;feat\u0026rsquo;, \u0026lsquo;fix\u0026rsquo;, \u0026lsquo;perf\u0026rsquo;, \u0026lsquo;refactor\u0026rsquo;, \u0026lsquo;revert\u0026rsquo;, \u0026lsquo;style\u0026rsquo;, \u0026lsquo;test\u0026rsquo;\nConsultez la documentation pour les autres règles prédéfinies\nExemple d’erreur lors de la validation d’un message # Lint from stdin echo \u0026#39;foo: bar\u0026#39; | commitlint ⧗ input: foo: bar ✖ type must be one of [build, chore, ci, docs, feat, fix, perf, refactor, revert, style,test] [type-enum] ✖ found 1 problems, 0 warnings ⓘ Get help: https://github.com/conventional-changelog/commitlint/#what-is-commitlint La personnalisation des règles de validation Il est par exemple possible de forcer l’usage des scopes et d’en restreindre le choix à une liste prédéfinie\n//commitlint.config.js module.exports = { extends: [\u0026#39;@commitlint/config-conventional\u0026#39;], rules: { \u0026#39;scope-empty\u0026#39;: [2, \u0026#39;never\u0026#39;], \u0026#39;scope-enum\u0026#39;: [2, \u0026#39;always\u0026#39;, [ \u0026#39;logging\u0026#39;, \u0026#39;services\u0026#39;, \u0026#39;docs\u0026#39;, \u0026#39;dependencies\u0026#39;, \u0026#39;profiles\u0026#39;, \u0026#39;users\u0026#39;, \u0026#39;api\u0026#39;, \u0026#39;segments\u0026#39;, \u0026#39;configuration\u0026#39; ]] } }; Pour aller plus loin, consultez la liste des règles de commitlint\nExemple d’historique git avec des commits structurés et expressifs 86f80528 (HEAD -\u0026gt; master, origin/master, origin/HEAD) fix(changelog): display scope in breaking change messages 6b578392 fix(CLI): show symlinked themes in `omz theme list` 64cb1530 chore: add Konfekt as universalarchive maintainer 492f712d feat(plugins): add `universalarchive` plugin to conveniently compress files (#6846) 2118d35e fix(vi-mode)!: add back edit-command-line key binding as \u0026#39;vv\u0026#39; (#9573) 79980b00 fix(vi-mode): hide cursor-change logic behind `VI_MODE_SET_CURSOR` setting 94ce46d4 docs(vi-mode): revamp README and document settings 66e0438d fix(archlinux): update URL and key server in `pacmanallkeys` (#9569) 9e5f280f feat(CLI): add `plugin info` subcommand (#9452) Logs extraits du projet OhMyzsh\nAutomatiser la vérification des messages Pour garantir la structure des messages de commit et ne pas ralentir le développeur, l’idéal est de vérifier automatiquement le format à la création du commit.\nGit permet justement d\u0026rsquo;exécuter des scripts personnalisés au déclenchement de certaines actions (documentation sur les Git Hooks)\nLe hook qui nous intéresse est “commit-msg”\nInstallation manuelle du hook Créer un fichier .git/hooks/commit-msg\n#!/bin/sh commitlint \u0026gt; $1 status=$? if [ $status -gt 0 ] then echo \u0026#34;✖ Commit message does not follow conventional commit message spec\u0026#34; echo \u0026#39;ⓘ Get help: https://github.com/conventional-changelog/commitlint/#what-is-commitlint\u0026#39; fi exit $status Installation automatique avec Husky (node project) Husky est une dépendance npm qui nous aide à facilement configurer les \u0026ldquo;Git Hooks\u0026rdquo;\nnpm install --save-dev husky Configuration du hook \u0026lsquo;commit-msg\u0026rsquo; pour exécuter le linter\n// package.json { \u0026#34;husky\u0026#34;: { \u0026#34;hooks\u0026#34;: { \u0026#34;commit-msg\u0026#34;: \u0026#34;commitlint -E HUSKY_GIT_PARAMS\u0026#34; } } } Exemple de vérification automatique des messages lors du commit $ git commit -m “foo” husky \u0026gt; commit-msg (node v12.19.0) ⧗ input: foo ✖ subject may not be empty [subject-empty] ✖ type may not be empty [type-empty] ✖ found 2 problems, 0 warnings ⓘ Get help: https://github.com/conventional-changelog/commitlint/#what-is-commitlint husky \u0026gt; commit-msg hook failed (add --no-verify to bypass) Conclusion Appliquer cette rigueur peut être perturbant au début mais cela devient très vite un réel atout.\nL’historique des commits n’est plus une liste inexploitable que personne ne consulte. Il devient une véritable aide à la compréhension des modifications opérées dans le projet.\nEn soignant ses messages de commit, le développeur se pose également plus de questions sur ce qu’il est en train de faire et si les changements apportés dans le commit sont bien adaptés (voir les bonnes pratiques des commits atomiques)\nEn cas de problème d’historique, ne pas oublier le “rebase interactif” :-)\nPour aller plus loin Avoir des messages expressifs et structurés permet également d’automatiser certaines tâches.\nDans un prochain article nous verrons comment automatiser la création d’un changelog lors de la livraison d’une nouvelle version.\nLiens utiles  Spécification Conventional Commits Commitlint Husky Git Hooks Git rebase Commits atomiques  Crédits Photo de couverture par Yancy Min sur Unsplash\n","date":"Jan 6, 2021","href":"https://blog.talanlabs.com/suivre-l-avancee-de-son-produit-avec-des-commits-de-qualite/","kind":"page","labs":null,"tags":["craft","tools","git"],"title":"Suivre l'avancée de son produit avec des commits de qualité"},{"category":null,"content":"Une nouvelle version du Scrum Guide a été dévoilée en novembre 2020. Un évènement ? Tout est relatif, mais voyons déjà ce qui change, pourquoi et quels impacts sur nous.\nPourquoi c’est important ? Eh bien\u0026hellip; ça n\u0026rsquo;est pas important ! Par contre, cette nouvelle version, je la trouve intéressante.\nIntéressante car, comme nous allons le voir, les modifications sont une véritable réponse aux pratiques que nous pouvons voir dans les implémentations de Scrum depuis des années. Les bonnes et surtout les mauvaises.\nQu’est-ce qui change – dans la forme ? Autant le dire, la version 2020 est une réécriture du Scrum Guide. Elle n\u0026rsquo;a plus grand-chose à voir avec la version 2017 dans sa forme.\n Elle est plus courte (13 pages) Elle est moins prescriptive Elle ne porte plus du tout sur l\u0026rsquo;informatique spécifiquement Le mot \u0026ldquo;agile\u0026rdquo; n\u0026rsquo;apparaît plus Par contre, \u0026ldquo;Lean\u0026rdquo; a été ajouté  Comme je vous le disais, rien que cette suppression (agile) et cet ajout (lean) sont intéressants. Si vous n\u0026rsquo;êtes pas sûr de la signification de \u0026ldquo;lean\u0026rdquo;, je vous invite à aller voir la page wikipedia sur le sujet.\nQu’est-ce qui change – dans le fond ? Rien. Juste quelques éclaircissements.\nVraiment\u0026hellip; Scrum, ne change pas. Par contre pour beaucoup, la compréhension de Scrum peut changer. Et c\u0026rsquo;est justement, à mon avis, la raison de ces changements sur la forme. Comme écrit plus haut, le Scrum Guide a été réécrit, alors c\u0026rsquo;est forcément pour une bonne raison, non ?\nJe vais vous donner mon avis sur les raisons de ces changements :\n Parce que 95 % des implémentations de Scrum que j\u0026rsquo;ai pu voir n’ont rien à voir avec Scrum Le Scrum Guide était donc clairement mal écrit et donc mal interprété Cette nouvelle version cible spécifiquement les plus mauvaises pratiques de Scrum, celles éloignant les équipes de l’efficacité  Où trouver le Scrum Guide ? Nous allons voir en détail les points les plus impactant pour les équipes (nous verrons dans un autre article ce qui change pour les parties prenantes et toutes les personnes en dehors de l\u0026rsquo;équipe), mais avant, au cas où, voici comment trouver les documents :\n Version Française Version Originale  Personnellement je vous conseille la version originale (anglaise) si vous le pouvez.\nScrum explique pourquoi utiliser Scrum  Essayez‐le tel qu\u0026rsquo;il est et, déterminez si sa philosophie, sa théorie et sa structure aident à atteindre les objectifs et à créer de la valeur. (page 4)\n Déjà c\u0026rsquo;est quelque chose qu\u0026rsquo;on dit souvent chez Talan Labs depuis des années et ça fait plaisir de le voir écrit dans les premières pages : ESSAYEZ-LE. Avant de dire que telle ou telle partie de Scrum ne va pas dans votre contexte : essayez et voyez si c\u0026rsquo;est vraiment le cas et pourquoi. Comme toujours, le plus important est de comprendre pourquoi vous prenez une décision.\nSi Scrum ou certaines de ses contraintes ne vous aident pas à atteindre vos objectifs, alors Scrum n\u0026rsquo;est pas adapté. Si c\u0026rsquo;est votre système ou votre fonctionnement qui vous empêche d\u0026rsquo;utiliser Scrum pour atteindre votre objectif\u0026hellip; alors c\u0026rsquo;est peut-être à vous de changer, qui sait ?\nScrum ne suffit pas et n’est pas une formule magique  Le cadre de travail Scrum est volontairement incomplet [\u0026hellip;] Divers processus, techniques et méthodes peuvent être employés dans ce cadre de travail. (page 4)\n Scrum ne suffit pas. Il faut d\u0026rsquo;autres choses, qui ne sont pas dites dans Scrum car elles dépendent entièrement de votre context et de votre domaine.\nUne clarification des responsabilités dans l’environnement  Un Product Owner ordonne le travail à faire pour résoudre un problème complexe dans le Product Backlog. La Scrum Team transforme une sélection de ce travail en un Incrément de valeur lors d\u0026rsquo;un Sprint. La Scrum Team et ses parties prenantes inspectent les résultats et s\u0026rsquo;adaptent pour le prochain Sprint. (page 4)\n Les piliers de Scrum forment un tout J\u0026rsquo;ai aussi apprécié dans cette version que les piliers de Scrum forment un tout. Nous comprenons alors mieux qu\u0026rsquo;il faut impérativement les 3 piliers pour ne pas avoir une équipe et un produit bancal.\n La transparence permet l\u0026rsquo;inspection [\u0026hellip;] L\u0026rsquo;inspection permet l\u0026rsquo;adaptation. Une inspection sans transparence est trompeuse et source de gaspillage. Une inspection sans adaptation est considérée comme infructueuse. L\u0026rsquo;adaptation devient plus difficile lorsque les personnes impliquées ne sont pas en possession de tous leurs moyens ou autogérées. (page 4 et 5)\n La Scrum Team Un détail, Scrum change la taille maximale recommandée pour une équipe en la fixant à 10 personnes maximum.\n Elles sont structurées et habilitées par l\u0026rsquo;organisation à gérer leur propre travail. Toute la Scrum Team est responsable de la création d\u0026rsquo;un incrément qui ait de la valeur et qui soit utile, à chaque Sprint. (page 6)\n Mais aussi, Scrum définit la Scrum Team comme une équipe auto-gérée là où dans les versions précédentes nous parlions d\u0026rsquo;auto-organisation. Le sens d\u0026rsquo;auto-gestion est à mon avis bien plus fort et devrait être le signal que l\u0026rsquo;équipe doit pouvoir prendre ses propres décisions sur son produit et la façon de le réaliser.\n Elles sont également autogérées, elles décident en interne qui fait quoi, quand et comment. (page 6)\n Attention néanmoins, comme j\u0026rsquo;aime bien à le dire, auto-organisée ou auto-gérée ne veut pas dire que l\u0026rsquo;équipe vit en autarcie. Elle existe dans un monde, dans un environnement et doit prendre en compte les règles et les contraintes de cet environnement, tout comme elle doit s\u0026rsquo;auto-gérer pour lui permettre de travailler avec les autres équipes de l\u0026rsquo;entreprise, les clients, les utilisateurs, etc. La gestion se faisant par rapport à tout cela, dans l\u0026rsquo;équipe.\nLe concept de rôles a laissé place aux responsabilités  Scrum définit trois responsabilités spécifiques au sein de la Scrum Team : les Developers, le Product Owner et le Scrum Master. (page 6)\n Les développeuses et développeurs “L’équipe de développement” a disparu pour “éliminer le concept d\u0026rsquo;équipe dans l’équipe, qui conduit à un comportement « proxy », du « nous et eux » entre le PO et l\u0026rsquo;équipe de développement.”\n Les Developers* sont les membres de la Scrum Team qui s\u0026rsquo;engagent à traiter tout ou partie utile d’un Increment à chaque Sprint. (page 6)\n  Attention, ici, l\u0026rsquo;anglais Developers est un faux-ami, il n\u0026rsquo;a pas du tout la même signification en français, je préfère donc parler de personnes réalisant l’incrément.  Responsabilités des “dev” :\n Créer un plan de Sprint, un Sprint Backlog Adapter leur plan chaque jour par rapport à l\u0026rsquo;Objectif de Sprint Se tenir mutuellement responsables en tant que professionnels Inculquer (Instilling) la notion de qualité en adhérant à une Definition of Done\n Ici, je ne trouve pas que la traduction française au jour où j\u0026rsquo;écris ces lignes ait traduit correctement instilling. En effet il existe un mot bien plus proche qu\u0026rsquo;inculquer en français : instiller dont la définition me parait plus juste par rapport à la version originale :\n Faire ressentir peu à peu un sentiment chez quelqu\u0026rsquo;un.\n Responsabilités de la ou du Product Owner Pas grand-chose ne change dans les responsabilités de la ou du Product Owner, sauf bien sûr, l\u0026rsquo;ajout de l\u0026rsquo;Objectif de Produit :\n Formuler et communiquer explicitement l\u0026rsquo;Objectif de Produit ; (page 6)\n Responsabilités de la ou du Scrum Master Le Scrum Master a subi pas mal d\u0026rsquo;éclaircissements.\n Faire en sorte qu’il n’y ait pas d’obstacles pouvant entraver la progression de la Scrum Team\n Il est bien explicite ici que la ou le Scrum Master n\u0026rsquo;est pas la personne qui s\u0026rsquo;occupe des problèmes mais bien, qui fait en sorte qu\u0026rsquo;il n\u0026rsquo;y en ait pas. Cela peut passer par des actions d\u0026rsquo;elle-même, ou d\u0026rsquo;autres.\n S\u0026rsquo;assurer que tous les événements Scrum ont bien lieu et sont efficients, productifs et respectent bien les [timebox]\n J\u0026rsquo;espère que cette formulation montrera bien que non, la ou le Scrum Master n\u0026rsquo;est pas un animateur de réunion.\n Encourager l’application de la planification produit empirique\n Ici, nous pouvons montrer que oui, l\u0026rsquo;équipe et ses parties prenantes doivent écouter l\u0026rsquo;avis du Scrum Master dans la planification. Non pas sur la pertinence de prioriser une chose, ou d\u0026rsquo;en retirer une autre. Mais bien de la manière de planifier une construction empirique. Et oui, ce n\u0026rsquo;est PAS en faisant un backlog de 50, 100 ou 200 éléments avant même de commencer à construire le produit. #empirisme\n La ou le Scrum Master est redevable (accountable) de l’efficacité de la Scrum Team\n Sûrement la phrase ayant fait couler le plus d\u0026rsquo;encre numérique depuis sa sortie. Non cette phrase ne change rien à l\u0026rsquo;approche d\u0026rsquo;un bon Scrum Master par rapport à la version 2017. Oui, le Scrum Master fait partie de l\u0026rsquo;équipe, elle ou il mouille sa chemise avec les autres et elle ou il est responsable de l\u0026rsquo;efficacité de l\u0026rsquo;équipe. Faire en sorte que l\u0026rsquo;équipe soit efficace est le but de sa présence, donc pas de dédouanement quand ça ne marche pas, c\u0026rsquo;est qu\u0026rsquo;on ne fait pas notre travail correctement. Être accountable, c\u0026rsquo;est aussi le moyen de se rendre compte qu\u0026rsquo;on n\u0026rsquo;est pas là où nous sommes attendu et donc de corriger notre façon de faire. Par contre, avec cette responsabilité, doit venir les pouvoirs qui vont avec.\nLes évènements Le Sprint Planning est maintenant clairement découpé en 3 phases (ou “topic”) : “Pourquoi ?”, “Quoi ?” et “Comment ?” (page 9)\n Pourquoi ce Sprint est‐il important ? Le Product Owner explique comment augmenter la valeur du produit et son utilité pour le Sprint en cours. L\u0026rsquo;ensemble de la Scrum Team collabore ensuite à définir un Objectif de Sprint qui énonce clairement aux parties prenantes l’utilité du Sprint. L\u0026rsquo;Objectif de Sprint doit être finalisé avant la fin du Sprint Planning.\n  Que peut‐on faire durant ce Sprint ? En discutant avec le PO, les Developers [personnes réalisant l’incrément] sélectionnent les éléments du Product Backlog à inclure dans le Sprint en cours. Au fur et à mesure de la discussion, la Scrum Team affine ces éléments, améliorant ainsi leur compréhension et leur confiance dans leur capacité à les développer.\n  Comment le travail choisi sera‐t‐il réalisé ? Pour chaque élément sélectionné du Product Backlog, les Developers [personnes réalisant l’incrément] planifient le travail nécessaire pour créer un Increment qui réponde à la Definition of Done. Cela se fait souvent en décomposant les éléments du Product Backlog en éléments de travail d\u0026rsquo;une journée ou moins.\n Concernant le daily, beaucoup ont remarqué que l\u0026rsquo;exemple des \u0026ldquo;3 questions\u0026rdquo; a disparu. Tant mieux. Trop d\u0026rsquo;équipes déroulaient ces questions bêtement, sans les comprendre et très souvent même en oubliant les 3/4 de ces questions qui portaient toutes sur l\u0026rsquo;atteinte de l\u0026rsquo;objectif de sprint.\nMoi ce que j\u0026rsquo;ai remarqué aussi, c\u0026rsquo;est la précision du fait que ce n\u0026rsquo;est pas le seul moment de discussion et de planification :\n Le Daily Scrum n\u0026rsquo;est pas le seul moment où les Developers [personnes réalisant l’incrément] sont autorisés à ajuster leur plan. Ils se réunissent souvent tout au long de la journée pour des discussions plus détaillées sur l’adaptation ou la re‐planification du reste du travail du Sprint. (page 10)\n La revue de sprint (Sprint Review) a subi, à mon sens, les plus gros éclaircissements et simplifications. Et elle en avait besoin !\nVoici la nouvelle première phrase :\n L\u0026rsquo;objectif de la Sprint Review est d\u0026rsquo;inspecter le résultat du Sprint et de déterminer les adaptations futures. (page 10)\n Il y a moins la notion de “on a fait un truc, on passe à un autre truc”.\n Un Incrément peut être livré aux parties prenantes avant la fin du Sprint. La Sprint Review ne doit jamais être considérée comme le seul moment pour délivrer de la valeur. Afin de fournir une valeur, l\u0026rsquo;Incrément doit être utilisable. La Sprint Review est une session de travail et la Scrum Team doit éviter de la limiter à une session de présentation.\n Clairement ici, Scrum veut casser le rythme des équipes faisant juste une démo en lieu de la revue.\nJ\u0026rsquo;aime.\nLes artefacts Ils sont maintenant plus clairs et servent un objectif à chaque échelle (page 11) :\n Chaque artefact contient un engagement qui apporte l’information nécessaire à la transparence et au focus rendant possible la mesure de la progression : Pour le Product Backlog, il s\u0026rsquo;agit de l\u0026rsquo;Objectif de Produit Pour le Sprint Backlog, c\u0026rsquo;est l\u0026rsquo;Objectif de Sprint Pour l\u0026rsquo;Incrément, c\u0026rsquo;est la Definition of Done\n  L\u0026rsquo;Objectif de Produit décrit un état futur du produit qui peut servir de cible à la Scrum Team pour planifier. L\u0026rsquo;Objectif de Produit est dans le Product Backlog. Le reste du Product Backlog émerge pour définir « ce qui » permettra d\u0026rsquo;atteindre l\u0026rsquo;Objectif de Produit.\n  L\u0026rsquo;Objectif de Produit est l\u0026rsquo;objectif à long terme de la Scrum Team. Ils doivent atteindre (ou abandonner) un objectif avant de s\u0026rsquo;attaquer au suivant (page 12)\n Et donc ? Tout ça pour quoi ?\nPour les équipes qui faisaient preuve de bon sens, de recul, de transparence et d’inspection, c’est un non-évènement. Elles devraient même plus se retrouver dans cette version de Scrum.\nPour les autres… Eh bien le rôle “d\u0026rsquo;exécutant” des codeuses/codeurs, designer et autres experts devrait être plus flagrant. Dans Scrum, plus que jamais, celles et ceux qui réalisent le produit sont des créateurs, plutôt que des exécutants. Une équipe Scrum conçoit, réalise un produit, en tant qu’équipe. Codeuses/codeurs, designer et autres experts aident le produit à émerger, à se développer. Le rôle de “PO-Passe-Plat” devraient donc être aussi plus visible, comme étant peu efficace ou valorisant.\nPour ces équipes, le Scrum Guide peut être utilisé comme une carte d’un chemin à prendre pour être plus efficace et pour construire le bon produit, plutôt que le produit que vous voulez faire.\nRépétons-le encore une fois : Scrum n’est pas un but. Les clients et les utilisateurs s’en fichent de ce que vous voulez faire, ce qui leur importe, c’est ce qu’ils veulent faire eux.\n","date":"Jan 6, 2021","href":"https://blog.talanlabs.com/scrum-guide-2020-quelles-implications-pratiques-dev-designers-po-scrummaster/","kind":"page","labs":null,"tags":["scrum","scrumguide","dans-la-vraie-vie"],"title":"Scrum Guide 2020 : Quelles implications pratiques pour les dev, les designers, les PO et les Scrum Master ?"},{"category":null,"content":"Dans cet article, nous allons explorer l\u0026rsquo;installation de python embarqué (MicroPython) sur une carte microcontrôleur ESP32, jusqu\u0026rsquo;à pouvoir exécuter du Python en étant directement connecté à la carte en question.\nESP32, c\u0026rsquo;est quoi ? L\u0026rsquo;ESP32 est une série de microcontrôleurs de type système sur une puce (SOC pour \u0026lsquo;System On a Chip\u0026rsquo;). Les SOCs sont des systèmes complets embarqués sur un seul circuit intégré, dont la mémoire, les microprocesseurs et les différents périphériques d\u0026rsquo;interface. L\u0026rsquo;ESP32 est le successeur et une version plus musclée de l\u0026rsquo;ESP8266, pour les bricoleurs qui l\u0026rsquo;auront déjà utilisée.\nUn carte basée ESP32 est un candidat idéal pour l\u0026rsquo;utilisation dans le domaine du développement des objets connectés, car\n son coût est très bas, je vous invite à aller regarder les prix sur AliExpress en cherchant \u0026lsquo;esp32\u0026rsquo; elle dispose de connectivité Wifi et Bluetooth intégrés, donc aucun besoin de rajouter du matériel supplémentaire pour avoir de la connectivité outre son aspect connecté elle dispose d\u0026rsquo;une pléthore de possibilités pour l\u0026rsquo;interfaçage de composants électroniques, les références sont disponibles sur esp32.net par exemple elle est adaptée aux problématiques temps réel (pour l\u0026rsquo;utilisation des bibliothèques NeoPixel par exemple), et possède une RTC (Real Time Clock) intégrée son intégration est très facile pour ceux qui aiment bricoler sur de l\u0026rsquo;IOT, il suffit de voir les différents projets qu\u0026rsquo;on peut réaliser avec ces cartes sur Hackaday  En résumé, l\u0026rsquo;ESP32 se situe en milieu de chemin entre les besoins basiques de contrôle sur de l\u0026rsquo;électronique non connectée (famille Arduino par exemple) et les besoins d\u0026rsquo;un contrôleur basé sur un OS complet nécessitant des applications plus proches du monde logiciel OpenSource classique (famille des Raspberry Pis et dérivés).\nMicropython, c\u0026rsquo;est quoi ? Micropython est un interpréteur Python réduit OpenSource, développé pour être exécuté sur de l\u0026rsquo;embarqué, basé sur du Python 3. Vous pouvez donc utiliser du code python pour contrôler du hardware embarqué à la place des langages classiques plus complexes et bas niveau basés sur du C/C++ comme ceux utilisés pour le développement sur Arduino par exemple.\nLa simplicité du Python permet une approche plus facile pour ceux qui sont moins familiers au domaine du développement, et MicroPython garde une grande partie des capacités du Python classique\nEn plus de sa facilité de développement, micropython a quelques features qui le démarquent du développement embarqué traditionnel :\n un REPL (ou Read-Evaluate-Print-Loop) interactif, qui permet de se connecter à une carte et d\u0026rsquo;exécuter des lignes de code à la volée sans compilation ou upload, ce qui est pratique pour tester des branchements de composants et capteurs rapidement une bibliothèque large permettant de faciliter des tâches associées aux traitements des messages sur les systèmes distribués, et facilitant l\u0026rsquo;utilisation de composants électroniques branchés sur une carte MicroPython est aussi extensible avec des fonctions bas niveau C/C++ pour des utilisations plus avancées  Pré-requis Si vous voulez appliquer cet article sur une carte ESP32, vous aurez besoin de :\n un cable usb (s\u0026rsquo;assurer que c\u0026rsquo;est bien un cable data et pas seulement alimentation) une carte ESP32 (dans notre cas c\u0026rsquo;est une DevKit V1 de DOIT basée sur le module ESP-WROOM-32) un pc sous Linux (ici un pc sous Ubuntu 20.04) python version 3.4+ recommandé  Configuration de votre environnement 1. Installer picocom Picocom est un émulateur de terminal pour les connections série sur les tty Linux, nous en aurons besoin pour accéder au REPL de micropython lorsqu\u0026rsquo;il sera installé sur l\u0026rsquo;ESP32 afin de vérifier que nous avons réussi notre flashage.\nsudo apt install picocom 2. Disposer des droits d\u0026rsquo;accès à /dev/ttyUSB0 Lorsque vous brancherez votre ESP32 à votre PC, vous aurez besoin de communiquer via le port série USB, qui sera typiquement /dev/ttyUSB0 si vous n\u0026rsquo;avez aucun autre composant USB connecté.\nDépendant du Linux utilisé, il se peut que vous ayez besoin d\u0026rsquo;appartenir aux groupes tty et dialout pour disposer des droits d\u0026rsquo;écriture sur /dev/ttyUSB0.\nsudo usermod -a -G tty \u0026lt;username\u0026gt; sudo usermod -a -G dialout \u0026lt;username\u0026gt; Si vous avez toujours des \u0026ldquo;permission denied\u0026rdquo; sur les étapes suivantes pour écrire sur /dev/ttyUSB0, vous pourrez toujours changer l\u0026rsquo;owner (à refaire à chaque fois que vous rebrancherez l\u0026rsquo;ESP32 à votre PC) :\nsudo chown \u0026lt;username\u0026gt; /dev/ttyUSB0 3. Installation des outils pour flasher Nous aurons enfin besoin de télécharger les outils pour pouvoir flasher correctement la carte ESP32. Je préfère personnellement créer un environnement virtuel python dès que je dois utiliser des bibliothèques dans un contexte spécifique, permettant ainsi d\u0026rsquo;isoler les dépendances nécessaires à chaque projet.\nPour créer son environnement virtuel, il suffit de lancer la commande suivante dans un répertoire de son choix :\npython3 -m venv --system-site-packages ./venv_micropython Une fois l\u0026rsquo;environnement virtuel installé, on l\u0026rsquo;active avec la commande suivante :\nsource ./venv_micropython/bin/activate Nous pouvons maintenant installer les outils nécessaires au flashage (esptools) dans notre environnement virtuel :\npip install esptool Pour désactiver son environnement virtuel, il suffira d\u0026rsquo;exécuter :\ndeactivate Déployer le firmware micropython Avant de pouvoir flasher notre carte ESP32 avec MicroPython, il nous manque un dernier élément : le firmware. Vous pouvez le télécharger sur https://micropython.org/download/esp32/\nJ\u0026rsquo;ai récupéré la dernière version stable de septembre 2020 : esp32-idf3-20200902-v1.13.bin.\nUne fois le firmware téléchargé, le flashage s\u0026rsquo;effectuera en deux étapes recommandées, notamment si c\u0026rsquo;est le premier flash MicroPython sur la carte :\n1. Effacer le flash existant sur la carte ESP32 esptool.py --chip esp32 --port /dev/ttyUSB0 erase_flash Si l\u0026rsquo;effaçage du flash s\u0026rsquo;est bien déroulé, vous devriez voir une sortie console d\u0026rsquo;esptool indiquant qu\u0026rsquo;il a correctement détecté votre carte et que l\u0026rsquo;effaçage a été effectué :\n2. Installer le firmware micropython esptool.py --chip esp32 --port /dev/ttyUSB0 --baud 460800 write_flash -z 0x1000 esp32-idf3-20200902-v1.13.bin Idem que pour la première commande, une sortie console d\u0026rsquo;esptool vous indiquera le flashage effectué avec le firmware que nous avons installé et que la carte a fait l\u0026rsquo;objet d\u0026rsquo;un hard reboot.\n3. Tester micropython via le Serial Lancer picocom sur le terminal série approprié avec un baud rate à 115200 pour la communication avec la carte :\npicocom /dev/ttyUSB0 -b115200 Si vous avez pu vous connecter correctement, vous devriez voir un message d\u0026rsquo;accueil indiquant que l\u0026rsquo;ESP32 est connecté et prêt à recevoir vos inputs, il suffit d\u0026rsquo;appuyer sur [Enter] pour avoir accès au REPL MicroPython :\nNous pouvons maintenant vérifier la version de MicroPython installée :\n\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; sys.version_info (3, 4, 0) Et tester avec un petit hello world :\n\u0026gt;\u0026gt;\u0026gt; print(\u0026#34;hello world\u0026#34;) hello world \u0026gt;\u0026gt;\u0026gt; Nous pouvons aussi tester le hello world façon Maker, qui consiste à allumer la led on-board de la carte testée (dépendant du modèle le pin utilisé pourra être différent) :\n\u0026gt;\u0026gt;\u0026gt; from machine import Pin \u0026gt;\u0026gt;\u0026gt; p2 = Pin(2,Pin.OUT) \u0026gt;\u0026gt;\u0026gt; p2.on() \u0026gt;\u0026gt;\u0026gt; Conclusion Dans cet article, nous avons pu aborder en quelques lignes de commande le flashage et le test via le REPL Python d\u0026rsquo;une carte ESP32 sous micropython.\nNous pourrons, dans un article suivant, aborder le développement et l\u0026rsquo;intégration avec un IDE Python (PyCharm) d\u0026rsquo;une carte ESP32 sur une problématique d\u0026rsquo;objet connecté.\nRéférences  Site de référence sur ESP32 : http://esp32.net/ Projets sur l\u0026rsquo;ESP32 : https://hackaday.io/projects?tag=ESP32 Téléchargement du firmware micropython pour ESP32 : https://micropython.org/download/esp32/ Installation de micropython sur ESP32 : https://docs.micropython.org/en/latest/esp32/quickref.html#installing-micropython  ","date":"Jan 5, 2021","href":"https://blog.talanlabs.com/flasher-esp32-micropython/","kind":"page","labs":null,"tags":["IoT","microcontroleurs","micropython","ESP32"],"title":"Flasher un ESP32 avec micropython sous Linux"},{"category":null,"content":"Martin Scher a rejoint Talan Labs en reconversion. En effet, dans sa vie précédente, il était cuisinier ! Rien à voir avec le monde de TalanLabs dans l\u0026rsquo;informatique\u0026hellip; rien à voir ? Et si le passage d\u0026rsquo;un monde à l\u0026rsquo;autre était un atout ? Une chance de voir le monde informatique différemment, avec un œil neuf et d\u0026rsquo;apporter de la fraîcheur ?\nInterview sur Scrum Life Retrouvez l\u0026rsquo;interview de Martin que j\u0026rsquo;ai pu faire sur la chaîne Youtube Scrum Life et aussi le LIVE qui a suivi l\u0026rsquo;interview et dans lequel Martin a répondu aux questions de la communauté :\n   Comment Devenir SCRUM MASTER ? Martin Scher partage son EXPÉRIENCE de RECONVERSION ♻\n   Le LIVE avec Martin\nN\u0026rsquo;hésitez pas à contacter Martin pour avoir plus d\u0026rsquo;informations sur son parcours et si vous aussi, vous cherchez à vous reconvertir.\n","date":"Jan 4, 2021","href":"https://blog.talanlabs.com/reconversion-en-scrummaster-temoignage/","kind":"page","labs":null,"tags":["scrummaster","reconversion","job","carrière"],"title":"Scrum Master après une reconversion : le témoignage de Martin Scher"},{"category":null,"content":"A la croisée des défis de notre temps, le bâtiment intelligent permet de gagner en efficience énergétique tout en améliorant le confort de ses occupants. Ou comment associer conformité réglementaire, stratégie RSE, réduction des coûts d’exploitation et marque employeur.\n Il y a actuellement un alignement des planètes en faveur du smart building. Dans une approche systémique, tout concourt à la généralisation du concept de bâtiment intelligent. L’urgence du moment, c’est bien sûr la crise sanitaire qui rappelle aux organisations l’importance de gagner en résilience. Un bâtiment intelligent permet d’encadrer le protocole sanitaire en aidant, par le contrôle des accès et les indicateurs de présence, à faire respecter la distanciation sociale et le système de \u0026#34;jauge\u0026#34; des effectifs.\n  Retrouvez la suite de la tribune de Sami Khalfaoui sur InfoDSI !\n ","date":"Dec 3, 2020","href":"https://blog.talanlabs.com/smart-building-coeur-enjeux/","kind":"page","labs":null,"tags":["Smart Building","IoT","IA"],"title":"Le smart building, au cœur de tous les enjeux"},{"category":null,"content":"Qu’est-ce que l’Atomic design ? L’Atomic design est une méthode de conception modulaire. Pour faire simple, lorsque l’on conçoit des interfaces avec cette méthode, on part du plus petit des éléments pour construire un ensemble. De l’atome à la page…​ Si comme pour moi les cours de Physiques/Chimie ne sont qu’un vague souvenir ou que vous ne voyez pas le rapport entre les atomes et les interfaces…​ne bougez pas, nous allons détailler l’ensemble.\n D’après Brad Frost, l’esprit ingénieux qui se cache derrière cette méthode, elle se compose de cinq phases, qui sont :\n   Les Atomes\n  Les Molécules\n  Les Organismes\n  Les Templates\n  Les Pages\n   Les Atomes, sur lesquels on s’attarde dans la première phase, sont des éléments qui sont irréductibles et qui n’ont aucun but fonctionnel à eux seuls. Ils serviront de base à tous les éléments graphiques qui composeront notre interface. Par exemple : un logo, un bouton, une couleur, une icône ou un label…​\n Les Molécules, qui constituent la deuxième phase, sont des groupes d’atomes relativement simples qui forment des composants d’interfaces. Elles doivent être pensées \u0026#34;responsive\u0026#34; pour encourager la réutilisation des composants et favoriser la cohérence dans notre interface. Par exemple : un label + une icône loupe + un champ de saisie = Un champ de recherche.\n Pour la troisième phase, Les Organismes se composent à leur tour de molécules et d’atomes. Ce sont des combinaisons de composants un peu plus complexes, qui forment des sections de 2 l’interface finale. Par exemple : Un champ de recherche + Logo + Nav = Un Header.\n Selon Brad Frost, l’analogie qui a permis d’établir la hiérarchie des éléments s’arrête ici, pour redonner place à un langage plus approprié et éviter de perdre davantage les parties prenantes.\n Maintenant, en fonction de nos différents supports, on se servira des Organismes pour constituer Les templates, on entre ainsi dans la quatrième phase. C’est le moment d’apporter un peu de contexte aux éléments jusqu’ici abstraits et de réfléchir à la mise en page, en tenant compte de la structure du contenu. Par exemple : Un Header + Image + Un Bloc texte + CTA + Footer = Une page d’accueil.\n Enfin, la cinquième et dernière phase, nous permettra de mettre le contenu en action. C’est l’occasion, d’ajouter du contenu réel sans pour autant aller jusqu’à la réalisation de maquettes. Par exemple : On ajoutera à notre page d’accueil du texte, des images ou médias représentatifs de notre contenu.\n C’est bien tout ça, mais que nous apporte la mise en application de cette méthode ?\n   Quel est l’objectif de l’Atomic design ? C’est une démarche utile si l’on souhaite remettre en question son workflow en tant que designer UI/UX, gagner en efficacité et rapidité sans pour autant devoir sacrifier à la cohérence et la qualité du projet. (Ndlr : Parce que c’est notre projet !!!) On gagne du temps et ça, ça n’a pas de prix.\n C’est aussi un moyen d’évoluer au même rythme que nos interfaces qui changent elles aussi pour s’adapter aux nouveaux supports qui sortent du cadre habituel (Hololens, Oculus Rift). Ces supports peuvent être aussi petits (montres connectées…​) que gigantesques (projections murales, salles immersives…​).\n Ainsi le design par page n’a plus lieu d’être, car quand on y pense la page est un concept datant du livre et que l’on a transposé au web, (Ndlr : …​mais ça c’est un autre sujet ). Enfin, les développeurs utilisent cette méthode depuis assez longtemps, cela permet donc d’accorder notre manière de concevoir une interface à la manière de la développer et par la même occasion, de parler le même langage que nos chers développeurs. Une des manières pour rétablir nos atomes crochus (Ndlr : Envoyer un don à l’association devs et designers sans frontières).\n (Beaucoup d’avantages mais pas seulement…​) Mais alors, si tout est si parfait, me direz-vous, pourquoi tout le monde n’applique pas cette méthode ?\n   Comment s’approprier la méthode ? Pour beaucoup, cette méthode soulève la question de l’uniformisation au détriment de la singularité de chaque projet. Quand on parle de “factorisation” et de “réutilisation”, malheureusement beaucoup comprennent “uniformisation”,\u0026#34;censure des idées\u0026#34; et “castration créative”. Il faut savoir identifier les parties du projet pour lesquelles on recherche avant tout un sursaut créatif dans le but de créer une émotion particulière et celles où on souhaite une forte cohérence et beaucoup de réutilisation. On privilégiera les composants plus aboutis et concrets comme les templates et organismes si on souhaite de la réutilisation alors que l’on se tournera plus vers les molécules qui favoriseront notre liberté d’interprétation et de ce fait, notre créativité.\n En lisant différents articles à ce sujet et en appliquant cette méthode, on se rend compte que la méthode en elle-même n’est pas applicable à tous les projets et ne convient pas à toutes les structures. Notamment les plus grosses, ayant plusieurs équipes de design/développement qui appliquent généralement la même méthode de conception pour donner l’impression que leurs produits viennent d’une seule et même équipe. Donc proposer aux 4 équipes une méthode qui va miser d’abord sur la liberté créative pour ensuite essayer d’uniformiser les patterns, amènerait beaucoup trop de divergences parmi les différentes propositions des équipes de design, ce qui aurait pour effet d’empêcher ou de ralentir le process de conception.\n Et puis, soyons honnêtes, certains termes peuvent créer de la confusion. J’ai par exemple eu du mal à savoir ce que doivent être concrètement les templates versus les pages, d’autres établissent mal la différence entre les molécules et les organismes. Que l’on prenne un angle scientifique avec l’analogie des atomes et molécules ou l’exemple des LEGOS, l’intérêt est de réellement comprendre les principes derrière cette méthode pour l’adapter en fonction de son projet et de la taille de son équipe.\n Alors on achète ? Cette approche nous permet de mieux comprendre ce que l’on souhaite créer et surtout comment on va le créer. Cela incite enfin à réfléchir autant au détail, qu’à l’ensemble. La méthodologie aide aussi à avoir donc une vision globale du produit ou de la marque. Enfin, cela permet aux designers de se rapprocher de la façon de travailler des développeurs. On doit se rappeler que comme pour toutes les méthodes de design, il faut se l’approprier et l’adapter au contexte de travail. Au final, instaurer ce type de méthodologie de travail au sein de son équipe doit servir à gagner du temps sur des tâches répétitives et ingrates pour lesquelles nous n’avons aucune valeur ajoutée en tant que designer, telles que répercuter la même modification sur 20 écrans différents, remplacer des millions de fois le même wording, refaire le même composant un nombre incalculable de fois… Gagner du temps oui mais si cela permet de travailler sur d’autres éléments beaucoup plus intéressants pour notre utilisateur final et/ou notre client. On pourra alors se concentrer sur ce qui nous importe le plus, à savoir le plaisir d’usage et l’émotion !\n Sources :\n   @Audrey Hack\n  @Brad Frost\n  Image d’illustration : Uxdesign.cc\n     ","date":"Dec 1, 2020","href":"https://blog.talanlabs.com/presentation-atomic-design/","kind":"page","labs":null,"tags":["UX","Design Thinking"],"title":"Presentation de l'Atomic Design"},{"category":null,"content":"Quel coach agile n’a pas entendu cette question : \u0026#34;Euh…​ on arrive à la fin du sprint, et on n’a rien à montrer. Est-ce qu’on peut reporter la date de fin ?\u0026#34; Pas toujours facile d’y répondre correctement ! Il faut comprendre que cela indique plusieurs problèmes et empêche l’émergence de solutions. Voici quelques armes pour répondre à cette question.\n  En théorie : Selon notre cher cadre SCRUM, le sprint possède une durée fixe et il n’est pas envisageable de le rallonger parce qu’on ne va pas le terminer.\n La récurrence des divers évenements permet à tous les acteurs de s’y retrouver et d’être prédictibles. Les parties prenantes sauront à quelle date a lieu la Revue, tant que l’équipe existe, et pourront bloquer le point.\n Pour l’équipe, cela permet d’avoir le temps de s’inspecter. S’il est posé la question du report de la fin du sprint, il apparait très clairement que l’équipe n’est pas forcément familière avec le droit à l’erreur, mais aussi qu’elle a vraiment besoin de s’inspecter ! Finalement, ne pas avoir de démo à présenter est plutôt une bonne façon de mesurer la qualité de votre revue ! Il viendra peut-être un article sur le sujet prochainement…​\n    En pratique : Cet article fait suite à une expérience lors d’un atelier : Lors de l’animation d’un Minecraft4Scrum (équivalent à Lego4scrum, sur Minecraft), récemment, une réflexion assez pertinente s’est posée devant moi. La timebox de l’itération était fixée à 45min et j’incitais les étudiants à poser un maximum de questions au PO en sprint planning.\n Après 40 minutes de sprint planning, ils commencent ENFIN à produire ! Malheureusement, 5 minutes plus tard le timer de fin de sprint sonne. La décision fut dure à prendre : Dois-je risquer de frustrer tout le monde et arrêter le sprint ? C’est mon collègue Constantin Guay qui m’a incité à le faire. A vrai dire, c’était plutôt une réussite !\n Les participants étaient frustrés, certes, mais la revue a été utilisée pour mettre en évidence les problèmes qui avaient empêché le bon déroulement de la production. La rétrospective a permis d’identifier divers points qui permettraient d’accélérer le sprint planning et de se lancer plus vite dans la production.\n Nous sommes repartis sur le 3ème sprint avec un sprint planning très court et nous avons doublé notre vélocité !\n Par ailleurs, cela m’a permis d’introduire la notion de droit à l’erreur, plutôt contre-intuitive. Cela ne pose pas de problème de faire des erreurs, si on est capable d’en apprendre et de ne pas les reproduire !\n    Quelle conclusion en tirer ? Bien sûr, ça fait peur quand on arrive au bout d’un sprint et qu’on n’est pas prêt pour finir. Mais ce n’est pas si grave !\n En tant que coach, il est important de profiter de ces moments pour entrer dans des réflexions profondes et fondamentales.\n   ","date":"Oct 5, 2020","href":"https://blog.talanlabs.com/minecraft4scrum/","kind":"page","labs":null,"tags":["Agilité"],"title":"Pourquoi ne jamais reporter la fin d'un sprint ?"},{"category":null,"content":"Comme je l’ai déjà évoqué dans un article précédent, le partenariat entre Talan et l’EPF m’a fait découvrir le rôle du formateur. Mais nous sommes allés plus loin, en proposant un projet sur le long terme à un groupe d’étudiants de 5ème année.\n Des attentes qui se rejoignent Avant le grand saut dans la vie active en passant par le stage de fin d’études, les étudiants doivent se confronter au monde de l’entreprise au travers d’un projet de 5 mois. Entre le projet scolaire et le projet réel, il s’agit là de mettre en pratique les connaissances acquises, mais aussi de découvrir les « vraies » méthodes de travail de l’entreprise.\n  Promo 2020 dans sa salle @ Talan — 16/10/2019  Évidemment, ces attentes pédagogiques ne pouvaient que nous intéresser, puisque cela nous permet de nous frotter nous-mêmes aux problématiques soulevées par un projet blockchain tel que nous n’avons pas toujours le temps de mettre en place le reste du temps.\n Pour cela, il nous fallait nous éloigner d’un projet trop scolaire, autrement dit « jetable », pour nous concentrer sur un sujet à même de mobiliser aussi bien nos équipes que les étudiants.\n   Le projet Share2Gether Contrairement au TP de création d’un système de vote réalisé sur une journée tel que nous avons pu le mettre en place lors de la formation initiale, nous voulions un projet beaucoup plus complet et long terme. Et parce qu’utiliser une technologie comme la blockchain juste pour l’utiliser n’a pas vraiment de sens, il nous fallait imaginer un cas d’usage plausible.\n C’est ainsi qu’est né le projet Share2Gether, visant à proposer une solution d’organisation d’événements, à l’image de ce que propose Meetup.com. En effet, lors de l’accueil de différents meetups dans nos locaux, nous avons été régulièrement confrontés au « no-show », autrement dit à des inscrits qui ne viennent pas à l’événement, sans prendre la peine de libérer leur place.\n  Meetup blockchain @ Talan — 29/05/2019  Si ce comportement est connu des organisateurs, il n’en est pas moins frustrant, soulevant notamment des problématiques de gâchis alimentaire lors la commande de buffet ou empêchant des personnes qui le souhaite réellement de venir et trouvant des événements notés complets à tort. Avec un absentéisme de 20 à 50 %, tous les types d’événements sont concernés, y compris les meetups orientés blockchain.\n Au travers d’un système de réputation, et donc de bonus / malus, nous souhaitons encourager les inscrits à venir ou tout au moins à favoriser les désinscriptions pour donner une lecture claire à l’organisateur du nombre de participants.\n   Une organisation à imaginer À partir de l’idée du produit, on imagine très vite une implémentation technique, les technologies à utiliser, etc. Mais encore faut-il faire passer ces idées aux étudiants et surtout leur donner envie de s’approprier le projet.\n Convaincus des valeurs défendues par Talan Labs quant aux méthodes de travail (solidarité, amélioration continue, etc.), il était naturel d’en profiter pour transmettre les préceptes de l’agilité à l’équipe qui se formait tout juste.\n Mais la méthode ne fait pas tout ! Lors de la première année du projet nous avons découvert au fur et à mesure que certaines notions fondamentales n’étaient pas abordées dans le cursus académique des étudiants. Design thinking, gestion du code source, tests automatisés, les sujets étaient nombreux et variés, et ont nécessité des formations ciblées en cours de projet, principalement à la demande des étudiants eux-mêmes.\n  Promo 2020 dans sa salle @ Talan — 16/10/2019  De manière à anticiper ces questions lors de la deuxième année du projet, nous avons proposé aux étudiants de la promo 2020 de suivre ces formations dès le début. Évidement, de la théorie de « il faut écrire des tests » à la mise en pratique pendant le développement, il y a un pas important à franchir. Pour autant, l’idée même de l’aspect indispensable des tests était ancrée dès le commencement.\n Une fois les bases acquises, la routine peut se mettre en place, rythmée par les réunions agiles. Tous les mercredis, les étudiants bénéficient d’une salle qui leur est réservée, mais peuvent facilement solliciter les collaborateurs Talan Labs présents sur les plateaux autour. C’est cette proximité qui permet aussi d’apercevoir le quotidien d’un développeur et la vie d’une équipe projet.\n   De la « galère » à la maîtrise Ces deux années de projet ont partagé des caractéristiques, à commencer par une première période compliquée pour les étudiants. Des nouveaux langages, des technologies nouvelles, une méthode de travail à laquelle on n’est pas habitués …​ autant d’obstacles à surmonter.\n  Promo 2020 dans sa salle @ Talan — 16/10/2019  C’est sûrement là la valeur principale que nous avons voulu transmettre : même en manquant de confiance en eux, même en doutant des enseignements de l’école, les étudiants sont capables de travailler « pour de vrai » et de créer de la valeur. En fin de projet, les retours des étudiants étaient sans équivoque :\n  Ils étaient heureux du résultat obtenu et quasiment étonnés d’avoir réussi à autant produire.\n   Avec le recul, nous pouvons dire que nous ne nous attendions pas à ce retour. Nous comptions former des étudiants en leur apportant des compétences nouvelles, mais ce qui a le plus fait la différence est bien la confiance transmise. Avant de maîtriser les nouvelles technologies comme la blockchain, l’ingénieur de demain doit avoir confiance en ses capacités (et notamment ses capacités à apprendre et découvrir).\n   Une présentation réussie …​ et le tour est joué ! Que serait un projet scolaire sans sa soutenance finale ? Il nous paraissait important de cultiver ces compétences oratoires au même titre que les compétences techniques, c’est pour cela que nous avons proposé aux étudiants plusieurs présentations « à blanc » de leur soutenance.\n  Promo 2020 lors d’une présentation “à blanc” @ Talan — 15/01/2020  D’abord entre nous pour identifier les points attendus par l’école et les principaux messages à faire passer. Puis, face à quelques collègues qui n’ont pas suivi le projet dans ses détails, mais qui vont pouvoir identifier les messages trop survolés tandis que les designers vont se charger de conseiller sur le support en lui-même.\n Nouveauté cette année comme nous avions du temps en fin de projet : une présentation à des Directeurs Talan. Notre DRH, notre Directrice de la Communication ou encore le Directeur Général de Talan Labs ont pu assister à la présentation des étudiants. Et quelle présentation ! Si l’ingénieur généraliste EPF a bien une force, c’est celle de savoir transmettre ses messages avec conviction et clarté, parfois même avec humour.\n  Promo 2020 lors de la présentation finale @ Talan — 22/01/2020  Forts de cette expérience qui était de leur propre aveu plus stressante que les présentations habituelles, les étudiants ont pu délivrer une soutenance finale de grande qualité à leurs professeurs et encadrants. Deux années de suite j’ai eu le plaisir de voir les groupes encadrés réussir leur soutenance et montrer une joie non feinte en évoquant le travail fourni.\n   Une suite prometteuse Entre les formations délivrées en 4e et 5e années (évoquées dans un article précédent) et le projet Share2Gether, l’investissement réalisé par Talan Labs est important, mais évidemment pas dénué d’intérêt. En montrant nos compétences et notre état d’esprit, en apportant de la confiance et des connaissances aux étudiants, nous souhaitions aussi leur donner envie de nous rejoindre.\n C’est donc avec plaisir que nous comptons désormais dans nos rangs de nouveaux ingénieurs EPF (du stage de fin d’études au CDI). Et nous ne comptons pas nous arrêter là : les années à venir seront tout aussi chargées !\n Retrouvez l’article présentant la formation Blockchain délivrée à l’EPF depuis 2018 et l’ensemble des articles #blockchain sur le blog des Labs de Talan !\n \n   Cet article a initialement été publié sur Medium.\n   ","date":"Mar 4, 2020","href":"https://blog.talanlabs.com/blockchain-projet-scolaire-monde-entreprise/","kind":"page","labs":null,"tags":["blockchain","EPF","formation"],"title":"Blockchain : du projet scolaire au monde de l’entreprise"},{"category":null,"content":"L’écriture des US est souvent au coeur des discussions dans une équipe et dans la réflexion Agile en général. Est-elle bien écrite ? comment la tester ? Apporte t-elle réellement de la valeur ? Pourquoi faisons-nous cette US ?\n Toutes ces questions nous nous les posons dans chacune des équipes que nous accompagnons.\n Trop souvent le but de l’US est négligé et on les voient très souvent écrites avec les 2 premiers volets (En tant que …​, je souhaite …​), mais où est passé le \u0026#34;afin de\u0026#34; ????\n Ce fameux “afin de” qui est délaissé, devrait être au coeur de nos réflexions, car il nous dit pourquoi nous travaillons, dans quel but. Cette valeur que nous apportons au produit est censée être déterminée par ce “afin de”.\n Alors pourquoi est-il mis de côté ? Par oublie ? Fainéantise ? Par manque de visibilité ? Manque de compréhension ?\n Il faudrait toujours répondre à cette question afin de simplifier la prise de décision du PO sur la priorisation de ces US et pour éclairer les développeurs sur ce qu’ils font. Cela pourrait les inciter challenger davantage l’US et ainsi travailler main dans la main avec le PO.\n Comprendre ce que nous sommes en train de faire et quel besoin nous devons adresser doit nous rendre meilleur et nous poser les bonnes questions. Cet échange entre PO et équipe qui devrait se faire naturellement au lieu de ne faire que ce qui est écrit dans une US, peut être quotidien si tout le monde sait pourquoi il fait ce qu’il est en train de faire.\n L’idée du jour serait de commencer par ce “afin de”, tant mis à l’écart (puisque le reste semble acquis) pour redonner tout son semble à nos chères US.\n Afin de mieux comprendre le besoin Je souhaite les écrire différemment En tant que PO avec l’aide du SM\n Et vous comment sont vos US ? avez-vous à chaque fois ce fameux afin de ?\n ","date":"Feb 20, 2020","href":"https://blog.talanlabs.com/afin-de-quoi/","kind":"page","labs":null,"tags":["User story"],"title":"Afin De Quoi"},{"category":null,"content":"Depuis deux ans j’ai la chance et le plaisir d’intervenir auprès des étudiants de l’EPF, école d’ingénieurs généraliste dont j’ai été diplômé en 2015. Passer de l’autre côté de l’enseignement n’est pas une mince affaire, surtout sans expérience, mais revenir dans «son» école facilite largement le changement de posture.\n   Une école d’ingénieurs a la lourde responsabilité de maintenir sa formation à jour, pour préparer les étudiants du mieux possible à leur entrée dans le monde du travail. C’est ainsi que Talan a développé un partenariat avec l’EPF, pour un cycle de formations sur le thème de la Blockchain pour ses 4ème et 5ème années de la filière informatique.\n Faire passer ses idées Notre premier challenge était de créer une formation théorique. Travailler sur cette technologie relativement jeune et aux applications disruptives apporte des convictions et des idées fortes sur ce que la blockchain peut faire, mais aussi et surtout sur ce qu’elle ne peut pas faire. Les implications de la mise en place d’un tel système distribué ont plus d’impacts sur les organisations et les personnes que la technologie en elle-même sur les systèmes d’information. Pour autant, face à un public étudiant, il convient de regarder de près les composantes techniques de la blockchain pour en saisir l’intérêt.\n De nombreuses idées à faire passer, sur des thèmes très vastes et variés, de la plus petite brique technique à la conduite du changement, en passant par des concepts complexes, la gouvernance…​\n Formaliser et organiser ces idées en thèmes, les articuler de manière logique et fluide, en détaillant les concepts sans forcément entrer trop en détails dans l’implémentation, s’adapter aux connaissances supposées d’étudiants que l’on ne connaît pas…​ Autant de challenges à relever, mais non sans intérêt : c’est bel et bien le meilleur moyen de se rendre compte des concepts que l’on maitrise et de ceux qui méritent que l’on s’y attarde.\n  Définition des Blockchain \u0026amp; DLT en 5 mots  Grâce à de nombreuses présentations « à blanc » du support face aux collègues, on fait émerger les questions qui subsistent, les passages trop rapides sur des points complexes et, surtout, il devient possible de roder une première version du discours qui se doit de rester interactif et illustré pour intéresser.\n   Du baptême du feu au discours rodé Après une première édition de la formation en conditions réelles, face à des étudiants qui semblent intéressés et posent des dizaines de questions, nous ne comptions pas en rester là. L’objectif était de passer d’une formation basique à un parcours complet couvrant plusieurs technologies de Blockchain et DLT, à destination de nos clients.\n Il est clair qu’une formation sur un sujet aussi mouvant doit rester vivante et évolutive, même si cela suppose un investissement constant.\n  Formation théorique @ EPF — 04/06/2019  C’est ainsi que les étudiants de la promotion 2020 ont bénéficié d’une formation largement revue, telle que nous la délivrons à nos clients ou lors de BBL.\n Pour éviter la perte de spontanéité et la routine, j’essaye de changer au moins un slide par présentation, de toujours adapter le contenu au public ou à la durée, voire de prendre des angles très différents, comme à l’https://www.esgi.fr/[ESGI] (une école d’informatique) où le thème était « Blockchain : des métiers pour tous ».\n   Adapter son discours au public est indispensable : face à un public très haut niveau et fonctionnel on survole les aspects techniques, tandis que face à des développeurs il convient souvent de descendre dans les entrailles de la blockchain, pour évoquer les algorithmes de consensus, etc.\n   De la théorie à la pratique, il n’y a que la création d’un TP Sans en avoir la consigne directe de la part de l’EPF, nous souhaitions limiter le temps de présentation théorique pour accorder du temps à la pratique.\n De manière à rester relativement abordables et en accord avec une idée de réelle décentralisation, nous avons choisi dans un premier temps de nous concentrer sur Ethereum, probablement la blockchain la plus accessible.\n Mais par où commencer ?\n CryptoZombies Nos premières expérimentations sur Ethereum remontent à fin 2016, avant qu’un écosystème ne se mette parfaitement en place. Mais en gardant un œil sur les nouveautés nous ne pouvions rater la franche réussite de CryptoZombies, sûrement le meilleur moyen de découvrir le langage Solidity des smart contracts Ethereum.\n   CryptoZombies a été créé par Loom Network afin de rendre plus facile l’apprentissage de Solidity. Persuadés de leur pertinence, nous estimons qu’il faut faire profiter de ces excellents exercices à nos étudiants. Au-delà de la facilité que cela représente pour nous, il faut reconnaitre que CryptoZombies est bien pensé et surtout très ergonomique, il serait dommage de créer une formation dédiée alors que la communauté s’accorde à l’utiliser.\n En créant une armée de zombies, les étudiants acquièrent les bases techniques du développement de smart contracts, il est ensuite temps de passer aux choses encore plus sérieuses !\n  Vote décentralisé Pour le saut dans le grand bain, nous avons choisi de mettre en place un TP visant à développer une application de vote décentralisée.\n De manière à ne laisser personne en chemin, mais aussi pour faciliter l’encadrement de ce TP, nous avons choisi de rédiger un support le plus complet possible, ainsi que de proposer un jeu de tests unitaires qui valident étape par étape les développements.\n  Page de garde du support de TP  Cette préparation en amont est loin d’être anodine, et s’est déroulée sur plusieurs semaines :\n   Réalisation du système de vote minimal (possibilité de créer une élection avec des candidats et de voter pour l’un d’entre eux à chaque élection) et validation des développements par des tests unitaires\n  Rédaction d’un premier jet du support, basé sur des souvenirs des développements qui venaient d’avoir lieu\n  Réalisation du TP par 10 de nos collègues, sur la base du support uniquement, afin d’obtenir un maximum de retours\n  Correction et amélioration du support, mais aussi des exercices à réaliser (trouver la bonne balance entre le guidage total et l’incompréhension des consignes, établir la liste exhaustive des pré-requis, etc.)\n  Nouveaux tests en conditions quasi-réelles, etc.\n   Et puis le jour J arrive. Une soixantaine d’étudiants doivent suivre nos instructions, à commencer par la mise en place du poste de développement. Et c’est bien là que les premières difficultés apparaissent.\n  TP @ EPF — 04/06/2019  Évidement, sur le PC d’un développeur, la majorité des pré-requis est déjà présente (variables d’environnement maîtrisées, outils installés et maîtrisés, etc.). Mais sur des postes aux performances et systèmes d’exploitation variables, avec des niveaux de maîtrise très hétérogènes et des consignes parfois survolées, les premières salves de questions surprennent. Tout ce qui nous paraît évident ne l’est pas pour tout le monde, c’est bien là notre première leçon. Ce qui nous paraît complexe ne l’est pas pour tout le monde non plus, et c’est là notre meilleure surprise.\n Si nous attendions des difficultés dans la compréhension et la rédaction des smart contracts par exemple, le gros des problématiques rencontrées est finalement au niveau de leurs appels depuis une page web. Ces notions de smart contracts et leur langage (Solidity) ont beau être relativement novatrices, pour des débutants le développement web l’est tout autant.\n    Retour d’expérience Nous étions venus pour transmettre des connaissances, mais aussi pour bénéficier des enseignements que des étudiants ont à transmettre. En mettant en place un climat de confiance (facilité lorsque l’on sort de la même école), il devient possible de récolter des avis francs et des idées d’amélioration au plus près du terrain.\n Sur ces journées de formation, si la balance théorie / pratique a paru bonne, les difficultés rencontrées dans le développement web ont plombé le ressenti des étudiants.\n Nul doute que nous prenons en compte ces problèmes pour tenter de les effacer lors de la prochaine session. Par exemple en fournissant plus de tests unitaires qui guident de manière plus stricte, ou une interface web attrayante plus satisfaisante pour tous.\n \n  \n Retrouvez la vidéo de présentation de ces formations auprès des étudiants de l’EPF réalisée par Talan :\n    \n  \n Au-delà de ces formations ponctuelles de quelques jours en 4e et 5e années, Talan Labs s’est aussi lancé dans l’accompagnement d’un groupe de projet plus restreint (5 en 2018/2019 puis 8 en 2019/2020), toujours sur la thématique de la blockchain.\n C’est l’objet d’un autre article !\n \n   Cet article a initialement été publié sur Medium.\n   ","date":"Jan 18, 2020","href":"https://blog.talanlabs.com/blockchain-former-les-ingenieurs-de-demain/","kind":"page","labs":null,"tags":["blockchain","EPF","formation"],"title":"Blockchain : former les ingénieurs de demain"},{"category":null,"content":"Vous cherchez un moyen de vous améliorer en équipe avec la rétrospective agile ? Vous avez perdu le sens de vos rétrospectives et vous trouvez que c’est un temps de réunion inutile ?\n La rétrospective \u0026#34;Tour de France\u0026#34; est un format simple et assez classique adapté des formats habituels. Similaire au speedboat, il est facile à préparer.\n LE BESOIN D’UNE APPROCHE LUDIQUE Il est peut-être temps de réfléchir différemment, voire même de s’amuser ! La rétrospective étant un moment d’introspection et de remise en question, elle est nécessaire à l’amélioration de l’équipe.\n Lors de ma nouvelle mission, nous avons commencé par faire des rétros très formelles (Keep, Drop, Start) et je sentais l’équipe s’en lasser doucement, surtout dans un contexte où nous faisions 3h de cérémonies de fin de sprint d’affilée.\n Il fallait adopter une approche plus ludique !\n Cet atelier de rétrospective agile aura beaucoup plus d’impact pendant le Tour de France.\n   SET THE STAGE Pour commencer, récupérez le profil/parcours des étapes du Tour de France. J’ai choisi de le recopier sur une feuille de paperboard à l’horizontale car je trouve le format A3 trop petit.\n  Profil de la 18ème étape du TDF 2019 ©ASO  Arrivé en rétro, avant de présenter le document, demandez à chaque participant de prendre un post-it et d’y dessiner un vélo. Ensuite, accrochez le graphe et présentez-le :\n  Aujourd’hui, vous êtes des cyclistes. Vous avez votre vélo devant vous et vous allez le placer à l’endroit du graphe qui correspond le plus au point où on en est par rapport à la durée de vie du projet.\n   Lorsque l’équipe a choisi la position de leur vélo, vous pouvez demander à chacun pourquoi il a choisi cette position et pas une autre.\n  Ici, l’équipe a montré qu’elle était arrivée au bout de plusieurs difficultés mais elle s’attend à en voir de plus importantes plus tard…​    GATHER DATA A la suite de cette étape, intéressez-vous au prochain pic descendant de la courbe. Redessinez-le à côté, sur un tableau et demandez à l’équipe de répondre à ces questions sur des post-it :\n   Comment voyez-vous la suite immédiate (prochain sprint) ? Une côte ? Une descente ?\n  Pourquoi ?\n  Que peut-on faire pour s’y préparer et faciliter la montée ?\n  Quel a été le dernier pic que l’on a surmonté ?\n       GENERATE INSIGHTS / DECIDE WHAT TO DO Ensuite, l’équipe vote pour les sujets dont elle veut discuter (il est important de limiter ces sujets à 2 ou 3).\n L’équipe choisit des solutions et des personnes s’engagent à les appliquer.\n L’équipe à laquelle j’ai présenté ce format a été très motivée. D’après leur retour, cette approche les a fait réfléchir différemment de d’habitude et les a amené à des solutions concrètes.\n En espérant que ce format pourra motiver vos équipes !\n   ","date":"Jul 30, 2019","href":"https://blog.talanlabs.com/retrospective-tour-de-france/","kind":"page","labs":null,"tags":["Retro","Agile"],"title":"Retrospective Tour De France"},{"category":null,"content":"Si vous avez un peu de mal à tirer le meilleur de vos daily, que vous soyez développeuse, développeur, ou Scrum Master, vous pouvez agir en proposant une base de plan pour que l’équipe se l’approprie et la fasse évoluer.\nLe plan du daily, toujours visible à côté du board (ou de l\u0026rsquo;écran, au pire du pire\nJe vous propose d’expliquer mon approche et ce que je mets en place généralement avec les équipes que j’accompagne.\nC’est quoi, déjà un daily ? Le Daily Scrum, c’est un événement de Scrum.\nD’après le Scrum Guide, le daily est un moment dédié à l’équipe de développement de planification et d’inspection. Et oui, chaque jour, on planifie et on s’améliore, avec Scrum ! On attends pas pour ça.\nLe daily n’est donc pas un rapport à une tierce personne qui ne fait pas partie de l’équipe de développement (voir en fin d’article un lien vers une explication de pourquoi, le PO ne devrait pas y participer). C’est un moment de collaboration.\nUn plan, ou à l’arrache ? Le mieux je pense, est bien sûr que le daily se passe de façon presque informelle; car cela veut dire que c’est devenu complètement naturel pour l’équipe de communiquer.\nMais avant d’en arriver là, il faut parfois se faire aider pour acquérir les bons réflexes.\nLe Scrum Guide propose une approche en trois questions :\n   Qu’est-ce que j’ai fait hier qui a aidé l’équipe de développement à atteindre l’objectif du Sprint ?\n    Que ferai-je aujourd’hui pour aider l’équipe de développement à atteindre l’objectif du Sprint ?\n    Est-ce que je vois des obstacles qui m’empêchent ou empêchent l’équipe de développement de respecter l’objectif du Sprint ?\n   Le plan du daily Afin d’aider une équipe qui a du mal à trouver son chemin dans le daily ou qui se démotive pour le faire, je leur propose généralement de commencer à suivre une chose.\nJuste une chose. On peut choisir cette chose en fonction du contexte et de l’équipe, mais par exemple, commencez par un élément simple (en plus des 3 questions de base) :\n Posez-vous la question en début de daily : l’objectif du sprint est-il en danger ? Posez-vous les trois questions  Nous allons ensuite agrémenter notre plan de façon empirique, en observant le déroulé du daily, on va savoir ce qui nous manque et ce à quoi nous devons faire attention\nExemple  Le burndown  On en apprend quoi ? L’objectif est-il en danger ?   Walk the board  Pour chaque élément “non-terminé”, en commençant par le plus prioritaire :  On en parle Un problème ? (oui ? on en discute après alors)   Comment maximiser le “done” ?   Confiance dans l’objectif ?  Roti de 1 à 4 (pour se positionner) et on note la réponse On met le burndown à jour   Fin du daily, une discussion à entamer ?  Vous pouvez-voir sur la photo ci-dessus que sur un grand post-it vert, nous affichons l’objectif du sprint, le numéro de celui-ci et, dans le coin haut-droit, on trace la moyenne de confiance dans la tenue de l’objectif.\n(en bas du post-it, on note combien d’action d’amélioration nous avons pris ce sprint).\nAttention à la complexité Comme je l’ai dit plus haut, le but n’est pas de suivre le plan. Le plan est une aide pour ne rien oublier dans notre recherche d’amélioration. Je vous conseille de faire attention à garder le plan assez simple pour qu’il puisse facilement devenir un réflexe. Il pourra toujours servir en cas de nouveaux venus.\nPensez aussi à le faire évoluer !! Essayez de nouvelles choses, changer l’ordre des éléments, etc. Et dites-moi comment vous faites vos dailies, vous.\nAnecdote : Vous savez pourquoi on fait (souvent) les dailies debout ? Venez le découvrir dans mon court article sur le sujet.\nPour des daily sains et une bonne vie d’équipe, je vous recommande de lire mon article sur la participation néfaste du PO au daily et à quel point ce n’est pas nécessaire quand le PO est au rythme de l’équipe.\n","date":"Apr 16, 2019","href":"https://blog.talanlabs.com/comment-bien-mieux-faire-son-daily/","kind":"page","labs":null,"tags":["daily","Scrum"],"title":"Comment bien (mieux) faire son daily"},{"category":null,"content":"Nous avons vu dans un article précédent la présentation des grands principes de Corda, le DLT (Distributed Ledger Technology) développé par le consortium R3. Utilisé en très grande majorité par des banques, Corda semble répondre à de nombreux besoins non assouvis par les blockchains publiques. Nous allons désormais tâcher d’explorer les limites de Corda.\n   Est-ce que c’est une blockchain ? À question directe …​ réponse directe : non. Corda n’est pas une blockchain.\n Ok, mais, pourquoi ? Tout simplement parce que le registre Corda n’est pas stocké sous forme de blocs, donc pas de chaîne de blocs possible.\n Mais alors, si ce n’est pas une blockchain, ce n’est pas intéressant ? C’est beaucoup plus compliqué que ça…​\n Certes, ces deux dernières années, le mot ‘blockchain’ fait le buzz, y compris dans les médias généralistes, notamment suite à l’envol de la valeur de certaines crypto-monnaies. Pour autant, pour l’immense majorité des entreprises et tout particulièrement des banques, tout ce qui touche aux crypto-monnaies est bien loin de leurs intérêts premiers, et on parle plus volontiers de DLT (Distributed Ledger Technologies). Les DLT forment un ensemble de technologies qui englobent les blockchains, bien plus large que simplement Bitcoin ou Ethereum.\n Pour autant, la communication autour de Corda, principalement par le consortium R3, a entretenu un certain flou sur la question depuis les origines. Et ce flou perdure ! Lorsque l’on se rend sur le site officiel de Corda, la baseline du logo est claire :\n   Pour autant, la polémique ne date pas d’hier, puisque déjà en 2017 R3 publiait un article sur son blog pour expliquer que le problème est plus de l’ordre de la sémantique que de l’architecture. Il semblerait donc que le terme de ‘blockchain’ soit ici utilisé comme un raccourci pour ‘DLT’, avec une composante marketing indéniable. #buzzword\n   Est-ce que Corda est décentralisé ? Comme nous l’avons vu dans l’article de présentation, pour rejoindre un réseau Corda il faut fournir son identité à un doorman qui autorise l’accès et fournit les certificats nécessaires. Ce doorman peut révoquer une identité (dans le cas du retrait d’un participant au sein d’un consortium par exemple). Mais ne serait-ce pas là une porte ouverte à la censure ?\n Tout dépend de qui gère ce service central. R3 a récemment annoncé la création de la “Corda Network Foundation”, une entité à but non lucratif en charge de gérer le réseau mondial Corda. Donc une seule entité va être chargée de gérer un réseau soi-disant décentralisé…​ Heureusement, l’aspect non lucratif garantit une certaine résilience à la corruption par exemple.\n Pour autant, si l’on craint une certaine centralisation du ‘pouvoir’, il est toujours possible de créer son propre réseau privé, avec sa propre gouvernance et ses propres règles. Donc inutile de paniquer, la solution existe.\n   Est-ce que Corda est réellement open-source ? Il existe deux versions de Corda : la version Community, totalement open-source et dont le code est disponible sur GitHub, et une version Enterprise, pour laquelle il faut faire une demande à R3 pour obtenir les exécutables.\n Il s’agit là d’une différence majeure : on récupère des exécutables et non pas du code source. Même en considérant que la majorité du code de “Corda core” est similaire dans les deux versions, la brique “firewall”, argument principal justifiant la valeur de la version Enterprise, est totalement opaque. Difficile de savoir comment est assurée la sécurité via ce firewall…​\n   A quoi servent les notaires de Corda ? Actuellement, les nœuds de type ‘notaire’ d’un réseau Corda proposent deux modes : *validating *et non-validating. Lorsqu’ils sont non-validating, ils ne procèdent qu’à une seule vérification : la double dépense. En revanche, en mode validating, le notaire va inspecter le contenu de la transaction pour le valider, selon le protocole de consensus choisi (RAFT ou BFT disponibles).\n Toutefois, sur le réseau d’UAT fourni par R3, seul le mode non-validating est actuellement disponible, ce qui diminue l’intérêt des notaires. Cela s’explique par des besoins de performances qui ne semblent donc pas encore au rendez-vous avec le mode de fonctionnement cible.\n À noter que les notaires jouent tout de même un rôle intéressant en étant des témoins supplémentaires des transactions, notamment en permettant d’horodater de manière univoque celles-ci.\n   Intégration dans un système d’information legacy Corda étant vendue comme une solution tournée vers les entreprises, tout laisse à penser que son intégration a été prévue pour s’imbriquer sans souci dans le SI des grandes banques qui constituent le consortium R3. Malheureusement, la réalité n’est pas aussi rose…​\n Alors évidemment, la notion même de peer-to-peer est antinomique de ce que font les banques. Par exemple, pour convaincre les équipes de sécurité d’accepter de nouveaux flux considérés comme ‘exotiques’ dans leur SI, il est nécessaire de travailler de concert dès le lancement des initiatives Corda.\n Une fois les validations obtenues via les équipes d’architecture technique et de sécurité, encore faut-il s’attaquer à la réalisation technique. Malheureusement, la documentation de Corda Enterprise (légèrement différente de celle de Corda open-source) n’est pas parfaitement auto-porteuse. De nombreux allers-retours sont nécessaires avec les équipes de support et de déploiement de R3.\n Par exemple, si un flux n’est pas documenté par R3, l’impact temporel sur un déploiement peut être assez conséquent : toutes les étapes de documentation, de validation et de mise en pratique sont parfois longues.\n Pour autant, Corda reste une solution très tournée vers le monde des entreprises sécurisées. Évoquons notamment Hyperledger Fabric qui utilise des flux gRPC (Remote Procedure Call, développé initialement par Google), alors qu’il n’y a actuellement pas de WAF (Web Application Firewall, chargé de protéger le serveur applicatif) en mesure d’analyser ces flux qui contiennent de surcroit de nombreuses failles CVE identifiées. Idem par rapport à Quorum qui ne comporte pas de brique firewall, donc avec tout le traitement dans la même zone réseau.\n   Conclusion(s) et ouverture(s) En guise de conclusion, précisons tout de même que le but n’était pas de critiquer unilatéralement cette solution, mais bien d’examiner des axes d’amélioration ou des contradictions dans la communication officielle.\n Si Corda est une solution DLT encore jeune et prometteuse, elle n’en a pas moins de nombreux challenges à relever. Améliorer son intégration aux SI existants, avoir une documentation à jour, clarifier sa nature (blockchain vs. DLT) …​ les pistes sont nombreuses !\n De plus, pourquoi ne pas aller un cran plus loin et imaginer une solution interopérable avec d’autres DLT, pour devenir une brique incontournable d’un système décentralisé ? Si les blockchains actuelles ne sont pas interopérables, Corda aurait tout à y gagner, pour se démarquer plus fermement sur un marché (déjà) concurrentiel.\n La documentation a été pointée comme lacunaire, toutefois il faut noter un effort certain de transparence avec la page “Tradeoffs” qui liste les points sur lesquels des concessions ont été faites. Parmi ces concessions, on relèvera un point : R3 encourage à doubler les contrats en code par des contrats sur papier. Un comble pour une solution censée désintermédier les échanges de ses participants ! En opposant “Code is law” et “Existing legal systems”, R3 trahit en fait une réalité : les entreprises et les réglementations en vigueur ne sont pas encore prêtes à reposer uniquement sur du code.\n Si le produit n’est pas fini, avec une roadmap encore floue, il n’en est pas moins en constante (r)évolution, réagissant aux demandes de ses clients, grâce à une équipe de développeurs très investis, réactifs et heureux d’échanger avec les utilisateurs finaux.\n Au-delà des difficultés initiales, Corda devrait réussir à repousser ses limites et s’assurer un avenir plus radieux dans le monde bouillonnant des blockchains et DLT !\n Cet article fait partie d’une série sur Corda, commencée par R3 Corda : Présentation.\n \n   Cet article a initialement été publié sur Medium.\n   ","date":"Apr 15, 2019","href":"https://blog.talanlabs.com/r3-corda-limites/","kind":"page","labs":null,"tags":["corda","R3","DLT"],"title":"R3 Corda : Limites"},{"category":null,"content":" Parce qu’il n’y a pas que les coachs agiles qui ont le droit de vous faire jouer au Lego  — François - Coach DevOps    Le Lego4Devops, c’est un jeu ludique qui repousse les frontières entre les \u0026#34;Dev\u0026#34; et les \u0026#34;Ops\u0026#34;. Une sorte de team-building léger qui prend soin d’explorer les dysfonctionnements entre ces 2 populations. Voilà pourquoi vous devriez regarder ce \u0026#34;serious-game\u0026#34;.\n À porte de la Villette, je coache des équipes IT dans leurs transformations DevOps, et je constate un problème récurent, avec la sensation d’avoir manqué quelque chose.\n Mes tentatives avaient toutes échouées, l’absence de collaboration entre les dev et les ops persistait…​ je manquais d’idées et commençais à perdre espoir.\n C’est en revenant d’un atelier de sensibilisation au TDD qu’une idée m’est venue : rechercher un atelier pour aborder mes problématiques.\n Lego4DevOps ! Là, comme des enfants (avec les autres coachs de mon équipe), ce fut l’illumination ! On a traversé la rue pour acheter une boite de Lego dans le magasin de jouets. Et c’était parti !\n   À peine revenus, on a imprimé les règles https://lego4devops.github.io/#supports et trouvé une grande table pour expérimenter ce qui allait devenir notre atelier le plus populaire.\n   Lego4DevOps, qu’est ce que c’est déjà ? Le Lego4DevOps, c’est simple : un atelier ludique à base de Lego, un peu conceptuel, créé par Sébastien Fauvel, Cécile Especel et Didier Drouin, et mis à disposition selon les termes de la licence Creative Commons Attribution ­ Partage. L’initiative était sympa, on se doit de les en remercier.\n Sauf qu’en réalité, Lego4Devops s’inspire (aussi) pour le nom et pour le matériel du jeu Lego4Scrum, créé par Alexey Krivitsky http://www.lego4scrum.com/.\n Mais comme nous sommes de grands enfants, c’est anecdotique dans l’univers des \u0026#34;serious game\u0026#34;, ce qui importe c’est de partager.\n   De quoi parle Lego4DevOps ? Une première équipe prénommée \u0026#34;Les devs\u0026#34; a pour mission de construire un produit à base de Lego durant cinq itérations (aka sprints). Lorsqu’elle reçoit des specs, elle s’y attèle puis envoie un messager pour délivrer ses features à la seconde équipe nommée \u0026#34;les ops\u0026#34;, qui a pour mission de mettre en production et de s’assurer que la plateforme soit toujours disponible.\n   Qu’y a-t-il comme intérêt ? C’est que les règles évolues. Mais tous sont loin de se douter que le client donne une note à ce qui a été livré…\n Le point commun de ces Devs et ces Ops c’est qu’ils opèrent tous sur le même produit, mais ne prennent pas en considération les problématiques de l’autre. Ils ont tous des objectifs d’équipes, non partagés et surtout des contraintes différentes.\n Sur le papier, Lego4DevOps est une série d’itération qui amène à réfléchir progressivement à comment travailler plus efficacement en respectant les contraintes de chacun.\n Lego4DevOps, c’est un jeu \u0026#34;brillantissime\u0026#34;, hyper bien pensé, qui mérite d’être connu par le plus grand nombre possible.\n   Lego4DevOps, amusement ou sérieux ? Bien sûr, les créateurs ne se sont pas contentés de donner une liste de features à assembler, le réalisme est entier avec des managers d’équipes en charge du reporting. Ce jeu flirte avec un \u0026#34;ice-breaker\u0026#34; pour devenir un réel atelier d’amélioration continue. C’est l’histoire de deux équipes, qui pour évoluer d’une situation à l’autre, doivent… collaborer (si je simplifie à l’extrême).\n   Ce qui devait arriver, arriva. L’atelier, répété minutieusement par notre équipe de coachs, plait énormément aux équipes qui se laissent prendre au jeu du Lego4DevOps.\n Chaque itération du Lego4DevOps fait naître une situation différente, raconte à elle seule une problématique concrète et réaliste que vivent nos équipes au quotidien. C’est un enchainement de situations, tellement réaliste qui joue sur les émotions pour sensibiliser les équipes à partager leurs objectifs et leurs contraintes.\n   Quel bilan pour Lego4DevOps ?   Rétrospectivement, c’est unanime, les avis sont à chaque fois identiques. Une équipe pluridisciplinaire et autonome, il n’y a rien de mieux. Devs et Ops, dont la mission est devenue de satisfaire le client (et oui), repartent travailler avec l’idée de trouver le compromis idéal pour enfin collaborer dans la vrai vie !\n Ensemble, ils ont réussi à percer deux mystères : la collaboration, malgré les contraintes, et la satisfaction du client par la livraison de valeurs métiers.\n Lego4DevOps se fait analyste des problématiques organisationnelles du monde de l’IT en général. C’est probablement pour cette raison qu’ils ont mis si longtemps à délivrer de la valeur ;)\n En Bref, on a adoré et j’espère que vous allez aussi adorer le Lego4DevOps\n   ","date":"Mar 27, 2019","href":"https://blog.talanlabs.com/lego4devops/","kind":"page","labs":null,"tags":["devops","lego","formation"],"title":"Lego4DevOps"},{"category":null,"content":"Les technologies Blockchain \u0026amp; DLT (Distributed Ledger Technology) sont nombreuses. Nous allons ici nous concentrer sur Corda, un DLT développé par le consortium R3.\n   Genèse du projet À l’origine de Corda : cinq problématiques des blockchains publiques les rendent inadaptées à l’utilisation en entreprise. Nous allons commencer par détailler ces problématiques.\n Manque de confidentialité Les blockchains classiques telles qu’Ethereum proposent un registre complet distribué entre tous les participants (les ‘nœuds’). Or, pour des raisons de compétition voire de régulation, il est difficilement envisageable de donner accès à toutes les transactions réalisées par une entreprise à ses concurrents directs.\n  Risque de non-finalité R3 considère que le protocole PoW (Proof of Work) utilisé par Bitcoin ne répond pas au besoin de ‘finalité’ requis par les entreprises. En effet, pour entériner une transaction dans le registre, il faut attendre le travail des mineurs, et même si la transaction est ajoutée à un bloc, rien ne permet de s’assurer définitivement qu’un fork n’aura pas lieu.\n  Pseudonymat des participants Si l’inscription sur un réseau tel qu’Ethereum est très facile, par exemple en créant son portefeuille via MyEtherWallet, à aucun moment dans le processus il n’est demandé de fournir son identité. Alors, anonymat complet ? Non, pas complètement. À travers des pratiques de crawling on peut assez aisément relier de nombreuses adresses de portefeuilles à des individus précis. On parle de ‘pseudonymat’.\n Lorsque l’on parle d’échanges entre des entreprises, il est évident qu’elles doivent savoir à tout moment à qui elles ont affaire. Plutôt que de masquer l’identité des participants, il faut bien au contraire identifier fortement les entités qui traitent sur le réseau.\n  Faible scalabilité On recense plusieurs milliards de transactions de type ‘paiement’ dans le monde chaque jour. À l’inverse, Bitcoin ne traite qu’environ 300 000 transactions dans une journée. Pour qu’un système actuel puisse être remplacé par un système blockchain, il faut que ce dernier puisse absorber autant de transactions. Ce critère est d’autant plus important en entreprise que la rapidité de transfert est parfois indispensable (par opposition de la moyenne actuelle de 19 secondes par bloc sur Ethereum).\n  Faible productivité et lenteur d’intégration Si les technologies et langages exotiques comme Solidity (pour Ethereum) sont loin de rebuter les startups, il est clair qu’un grand compte (une grande banque, un assureur, etc.) aura plus de mal à accepter l’ajout d’une brique d’un nouveau format dans son système d’information. De plus, il est aujourd’hui plus facile de recruter des développeurs Java que Solidity…​\n Pénurie des talents et standardisation du SI poussent les entreprises à privilégier des technologies déjà éprouvées plutôt que les nouveautés encore mal maîtrisées voire interdites par les experts de la sécurité.\n    Historique rapide C’est sur la base de ces observations communes à plusieurs acteurs financiers que consortium R3 est fondé en 2015 par des grandes banques du monde entier, comme Barclays, Société Générale ou Deutsche Bank. La première version officielle de Corda sort en octobre 2017, avant une version d’entreprise en juillet 2018.\n Corda est présentée comme étant une ‘blockchain’ d’entreprise, mais nous aurons l’occasion de revenir sur la notion de ‘blockchain’ dans ce cas précis. R3 part du constat que les solutions comme Ethereum ou Bitcoin ne répondent pas aux besoins spécifiques d’une entreprise.\n   Deux versions de Corda Corda se décline en deux versions, chacune dédiée à un usage différent.\n Community Edition Version open-source de Corda, accessible à tous et dont le code source est disponible sur GitHub.\n Un très bon moyen de se former aux concepts de base, créer sa première CorDapp (application Corda, par analogie aux ÐApps d’Ethereum — applications décentralisées). Pour cela, la documentation de Corda propose des tutoriels qui permettent assez rapidement de faire fonctionner un réseau Corda, de développer des contrats et faire des transactions.\n  Enterprise Edition Version dédiée aux entreprises, accès aux exécutables sur demande.\n Cette version entend répondre plus spécifiquement aux besoins des entreprises. C’est ainsi qu’elle permet de coller aux exigences de haute performance et haute disponibilité, propose un support de plusieurs bases de données (Oracle, SQL Server, etc.) et une migration d’un système à l’autre.\n Mais surtout, cette version inclut un composant supplémentaire : le firewall. Élément à l’aspect marketing indéniable, surtout en pensant à des contextes très sécurisés comme dans une banque. Composé de deux éléments (bridge \u0026amp; float), il entend garantir la qualité des données en entrée et sortie du SI.\n    Quelques concepts de base Un réseau sous contrôle Un réseau Corda est semi-privé, dans la mesure où chaque nœud doit fournir des éléments permettant de justifier de son identité à un doorman pour rejoindre le réseau. Ce service permet de s’assurer que chaque entité participant au réseau est connue de manière non ambigüe. On parle alors d’un processus de KYC (Know Your Customer), largement répandu dans de nombreuses activités hors blockchain.\n Un réseau Corda est donc composé de nœuds et d’un doorman, mais aussi d’un ou plusieurs service(s) de notaire. Un notary est un nœud particulier, chargé de garantir l’unicité des mises à jour du registre.\n Une fois validée par le doorman, l’identité d’une entité est attestée par un certificat X.509 délivré par celui-ci, et publiée sur l’ensemble du réseau via un service dit de network map.\n  Un registre semi-partagé Contrairement à Ethereum où tous les participants ont accès au registre de toutes les transactions, le registre de Corda n’est pas entièrement partagé. En effet, un nœud n’a accès qu’aux données pertinentes pour son activité. C’est ainsi que sur le schéma ci-dessous, le nœud Alice n’a pas accès aux données partagées entre Bob et Carl alors même qu’il partage certaines informations avec Bob.\n   De plus, Corda va synchroniser en continu les données partagées par deux nœuds. Ainsi, nous avons l’assurance qu’à tout moment Alice et Bob voient la même chose.\n    Des ‘états’ propres aux nœuds Chacun des points de couleurs des schémas ci-dessus représente un fait, un ‘état’. Les states d’un nœud peuvent évoluer dans le temps. Pour réaliser cette évolution, l’état courant est marqué comme ‘historique’ et un nouvel état mis à jour est créé. L’ensemble des états, y compris leurs versions historiques, est stocké dans un vault propre à chaque nœud. Ce coffre-fort ne contient donc que les états relatifs au nœud.\n En rentrant un peu plus dans les détails, ce vault est en réalité une base de données classique (par défaut H2 pour la version Community, avec possibilité de passer sur des systèmes plus complets comme Oracle avec la version Enterprise). Nous sommes donc bien loin d’une blockchain puisque la notion même de ‘bloc’ est totalement absente de Corda. C’est notamment cette architecture radicalement différente qui permet d’assurer de meilleures performances de transaction.\n  Automatisation via des contrats Si vous avez déjà approché Ethereum, vous avez entendu parler des smart contracts. Ces bouts de code qui n’ont d’intelligent que le nom permettent l’automatisation de certaines tâches lors de transactions avec la blockchain.\n Corda n’échappe pas à ce concept, en le nommant plus sobrement ‘contrat’. Les contrats Corda sont rédigés dans des langages exécutables par la JVM (Java Virtual Machine), Java et Kotlin en tête. C’est ainsi qu’un développeur Java ‘classique’ pourra s’approprier un contrat Corda assez facilement, sans apprendre un nouveau langage, uniquement en exploitant l’API fournie par Corda.\n L’exécution d’un contrat doit être ‘déterministe’, dans la mesure où l’acceptation d’une transaction par le contrat dépend uniquement du contenu de celle-ci.\n  Des transactions sous conditions Nous venons tout juste d’évoquer le terme de ‘transaction’, il est temps de le définir. Une transaction est avant tout un souhait de mettre à jour le registre. Ce souhait ne peut ensuite être entériné dans le registre qu’à trois conditions :\n   Elle ne contient pas de ‘double dépense’ (autrement dit elle n’essaye pas de consommer un état déjà consommé)\n  Elle respecte les termes du contrat Corda (nature des paramètres d’entrée, etc.)\n  Elle a été signée par les parties requises\n    Des flows pour atteindre le consensus Entre l’émission du souhait de mettre à jour le registre et sa prise en compte par les nœuds impliqués, il y a donc plusieurs étapes, qui peuvent paraître lourdes et redondantes, là où une blockchain classique se charge entièrement de traiter la transaction émise.\n C’est là que la notion de flow entre en jeu. Les flows Corda visent à automatiser le processus de consensus. Fort heureusement, il n’y a pas besoin de développer les mêmes flows de manière répétitive, car Corda fournit par défaut des flows standardisés pour les actions courantes (signature de transaction, vérification d’un ensemble de transactions, etc.).\n  Validité des transactions en deux volets Comme nous l’avons vu, pour être intégrée au registre, une transaction doit répondre à des règles précises et inaltérables. Ces règles ont deux pendants :\n Validité - Les entités devant signer une transaction vérifient sa validité avant d’apposer leur signature. Pour cela, les inputs et outputs de la transaction doivent répondre aux règles pré-établies, ainsi que toutes les transactions antérieures ayant amené au state que nous essayons de modifier. On peut donc parler de ‘chaînage’ des transactions même si on est loin du fonctionnement d’une blockchain.\n Unicité - Une transaction ne doit pas consommer un état qui a déjà été consommé. Par exemple, une entité ne doit pas pouvoir dépenser de l’argent qu’elle a déjà donné à une autre entité. Donc, les inputs de la transaction ne doivent pas avoir été historisés auparavant.\n  Le cœur de Corda : le nœud Un nœud Corda est une instance de JVM possédant une identité unique au sein d’un réseau. Il possède deux interfaces avec l’extérieur :\n   Une couche réseau pour interagir avec les autres nœuds\n  Une interface RPC pour interagir avec son propriétaire\n   Comme nous l’avons vu, un nœud contient aussi son vault qui contient les states et un service de stockage pour héberger les transactions et leurs éventuelles pièces jointes.\n Tel quel, le nœud permet donc des opérations basiques, mais ses fonctionnalités sont évidemment restreintes et doivent être étendues par l’ajout de CorDapps.\n  L’application distribuée : la CorDapp Une CorDapp est une application distribuée sur un réseau Corda, à l’image des ÐApps sur un réseau Ethereum. Elles sont constituées de :\n   states qui définissent les ‘faits’ manipulés par la CorDapp\n  contracts qui définissent les règles de validation de transaction à appliquer\n  services qui fournissent des méthodes utilitaires pour interagir avec les composants du nœud\n  whitelists qui imposent les données que le nœud peut recevoir.\n   Contrairement à un réseau Ethereum, la CorDapp ne sera pas propagée dans tous les nœuds du réseau, mais installée de manière optionnelle sur les nœuds volontaires. Tant qu’un nœud n’a pas la CorDapp ad hoc pour exécuter un certain flow, il ne pourra pas l’exploiter.\n    Pour aller plus loin…​ Nous avons évoqué de nombreux concepts à la base de Corda et de ses composants, mais il est évident que de nombreux détails pourraient être ajoutés. La documentation officielle de Corda pourra sûrement répondre aux questions plus poussées !\n Dans un prochain article, nous évoquerons les limites de Corda, pour garder un regard critique sur cette solution largement employée dans le milieu bancaire notamment.\n \n   Cet article a initialement été publié sur Medium.\n   ","date":"Mar 11, 2019","href":"https://blog.talanlabs.com/r3-corda-presentation/","kind":"page","labs":null,"tags":["corda","R3","DLT"],"title":"R3 Corda : Presentation"},{"category":null,"content":"La vélocité indique la quantité de fonctionnalités que l’équipe Agile a terminée lors de la dernière itération.\nLors de la planification d’itération, il est assez courant de considérer, en première approximation, que la vélocité est proportionnelle à la présence de l’équipe (ex. pratique du “yesterday weather”).\nIl n’en faut pas plus pour les insatiables des KPI (Key Performance Indicator) pour considérer la vélocité comme l’Indicateur de productivité de l’équipe.\nProductivité = vélocité (?)\nPassons sur ce raccourci et considérons la productivité au sens large. D\u0026rsquo;après l'INSEE, \u0026ldquo;la productivité est définie comme le rapport, en volume, entre une production et les ressources mises en œuvre pour l\u0026rsquo;obtenir\u0026rdquo;.\nLa métrique que surveillent nos chers gestionnaires est un peu plus simple : les ressources se limitent aux personnes. Il s’agit de la productivité apparente du travail qui est un rapport entre la valeur ajoutée et le volume de travail mis en œuvre (cf. INSEE).\nLa formule pour la productivité de nos équipes est donc :\nSoit \u0026hellip; Mais quand on surveille la productivité, on surveille la santé de notre système de production avec le souhait de l’améliorer.\nKarl Marx a analysé la productivité du travail en long, en large et en travers. Il a identifié les leviers principaux de la productivité (cf. Le Capital Livre III).\nMettons les en regard des principes et pratiques Agiles :\nMoyens de production plus performants\nIntégration et livraison continues.\nOrganisation plus coopérative\n“Les utilisateurs ou leurs représentants et les développeurs doivent travailler ensemble quotidiennement tout au long du projet.” (cf. Manifeste pour le développement Agile de logiciels)\nMeilleure qualification des travailleurs\n“Une attention continue à l\u0026rsquo;excellence technique et à une bonne conception renforce l’Agilité.” (cf. Manifeste pour le développement Agile de logiciels) Ce principe se traduit souvent par des bonnes pratiques qu’il faut apprendre à maîtriser pour qu’elles soient efficaces (cf. l’excellence technique selon LeSS)\nCadences de travail plus élevées mais qui doivent rester soutenables pour être rentables\n“Les processus Agiles encouragent un rythme de développement soutenable.” (cf. Manifeste pour le développement Agile de logiciels)\nMoyens de production plus performants : intégration et livraison continues\nOn voit que l’Agilité possède les leviers nécessaires pour augmenter la productivité mais, surtout, plus de cent cinquante ans après Karl Marx, nous en sommes au même point avec quelques outils supplémentaires.\nPeu importe les raccourcis que l’on prend (ex. vélocité = productivité) ou ses convictions, tant que la définition de la productivité ne changera pas, ses leviers resteront les mêmes.\nSi vous tenez à améliorer la productivité de vos équipes, Agiles ou non, remettez-vous-en à votre bon sens : suivez les recommandations basées sur la longue expérience de vos pairs et offrez leur les outils et l\u0026rsquo;environnement dont elles ont besoin … Ou innovez en créant votre propre productivité ! Le danger reste toujours de faire les choses à moitié et de rester avec des solutions inefficaces car incomplètes.\n","date":"Mar 5, 2019","href":"https://blog.talanlabs.com/etre-productif-une-question-de-principes/","kind":"page","labs":null,"tags":["bonnes pratiques","management"],"title":"Être productif, une question de principes"},{"category":null,"content":"Une nouvelle fois Talan Labs a accueilli l’Asseth pour son meetup du jeudi 28 février. Au programme : 2 présentations pour une soirée bien remplie !\n CryptoBooks : l’édition transparente Jean-René Krasucki et Thibault Verbiest présentaient leur création : une application permettant d’acheter des livres en Ether, mais aussi en une vingtaine de tokens ERC20. Partant du principe que le milieu de l’édition a tout d’une \u0026#34;boîte noire\u0026#34;, ils ont créé une ÐApp qui rend transparente la répartition du prix d’achat d’un livre entre les différents acteurs (éditeurs, auteur, ayant-droits, illustrateurs, etc.).\n Les premières ventes vont bientôt avoir lieu, rendez-vous sur CryptoBooks.club !\n   DAI : présentation d’un stablecoin Rémi Foult présentait le stablecoin DAI, créé par la plateforme MakerDAO. Si des doutes existent sur l’existence d’un collatéral pour Tether, DAI propose une plus grande transparence et surtout une résilience plus importante quant à sa capacité à ne pas fluctuer et surtout à toujours garantir sa valeur réelle.\n   La vidéo du meetup La vidéo du meetup est disponible sur YouTube :\n      ","date":"Mar 1, 2019","href":"https://blog.talanlabs.com/asseth-meetup-fevrier/","kind":"page","labs":null,"tags":["meetup","asseth"],"title":"Meetup Asseth : CryptoBooks \u0026 DAI"},{"category":null,"content":"Je me suis remis à ElasticSearch après plusieurs années d\u0026rsquo;absence et le choc a été dur : revenir à Oracle après 5 ans, pas de souci, mais dans le cas d\u0026rsquo;Elasticsearch, ça avait tellement évolué que j\u0026rsquo;ai du prendre un peu de temps pour m\u0026rsquo;y remettre.\nQue s\u0026rsquo;est-il donc passé entre un ES 0.90.5 de 2013 et un ES 6.5.4 de 2019 ?\nMes usages 2013 - Cadremploi En 2013, je travaillais chez Cadremploi et nous avions besoin de faire évoluer le moteur de recherche du site qui fonctionnait uniquement avec Lucene. La promesse d\u0026rsquo;Elasticsearch : un cœur Lucene, une recherche simplifiée (la syntaxe Lucene est étrange) disponible par une API REST et une base de données distribuée répliquée automatiquement. Le projet avait été un succès et avait été mis en production par l\u0026rsquo;équipe suivante.\n2018 - GrDF Retour à ElasticSearch chez GrDF qui souhaite accélérer sa recherche dans une de ses bases clientes. Environ 30 millions de clients présents dans la base et des nouveaux besoins de recherche : une recherche libre \u0026ldquo;à la google\u0026rdquo; ainsi qu\u0026rsquo;une recherche par préfixe.\nLes changements Voyons maintenant ces changements qui m\u0026rsquo;ont marqué.\nLa fin des plugins \u0026ldquo;site\u0026rdquo; embarqués En 2013, la meilleure façon de visualiser son cluster ElasticSearch était d\u0026rsquo;utiliser un plugin \u0026ldquo;site\u0026rdquo; comme head ou kopf (notez le lien entre les deux). Une fois installée, ElasticSearch servait directement ces pages web sur son port 9200 (par défaut). Il y en de toute sorte, du monitoring de cluster aux tests des plugins phonétiques. En octobre 2016, la version 5.0.0 d\u0026rsquo;ElasticSearch sonnait la fin des plugins site notamment à cause de problèmes de sécurité (ici et là). Désormais, il faut embarquer un serveur indépendant pour pouvoir distribuer une fonctionnalité annexe à ElasticSearch. C\u0026rsquo;est peut-être un peu contraignant, mais cela garantit une vraie indépendance des produits, indispensable quand on passe en production.\nA noter que Kibana fournit un monitoring basique dans sa version gratuite.\nPlugin head\nLogstash, principal point d\u0026rsquo;entrée Une façon d\u0026rsquo;alimenter ElasticSearch était d\u0026rsquo;utiliser une river, c\u0026rsquo;est à dire un batch intégré à ElasticSearch capable de parcourir plus ou moins fréquemment une source pour en extraire les données. Il y en avait plusieurs, en particulier pour extraire des données d\u0026rsquo;une base de données (à partir d\u0026rsquo;une requête), de CouchDB, RabbitMQ, Wikipedia ou encore Twitter. Le problème venait de la consommation des ressources nécessaires (mémoire, sockets…) au bon fonctionnement du batch au détriment du fonctionnement d\u0026rsquo;ElasticSearch. En mars 2015, la version 2.0.0 les fait disparaitre et ElasticSearch propose soit d\u0026rsquo;extraire le code des rivers pour externaliser le fonctionnement ou d\u0026rsquo;utiliser LogStash comme solution d\u0026rsquo;approvisionnement.\nMême si les rivers wikipedia ou twitter revêtaient un caractère pratique pour remplir sa base, dans un contexte de production, il y avait une vraie appréhension à en utiliser une, la question de la reprise sur erreur en cas de crash du nœud hébergeant la river étant difficilement solvable.\nLe TTL est mort ! Le TTL (time to live) est une fonctionnalité que l\u0026rsquo;on retrouve dans de nombreuses SGBD : lorsque vous ajoutez une donnée, vous pouvez préciser sa durée de vie. Une fois le temps écoulé, le système supprime automatiquement cette donnée.\nCe mécanisme peut sembler séduisant mais il présente de nombreux inconvénients : il faut fréquemment (60s) parcourir l\u0026rsquo;ensemble des données pour détecter celles à supprimer mais également \u0026ldquo;recompacter\u0026rdquo; les fichiers dans lesquels on supprime ces données. Le champ _timestamp (date d\u0026rsquo;insertion de la donnée) est utilisé pour calculer le ttl. La version 5.0.0 d\u0026rsquo;ElasticSearch supprime cette fonctionnalité précédemment dépréciée. Il est conseillé d\u0026rsquo;utiliser sa propose date au lieu du champ timestamp (si nécessaire) et d\u0026rsquo;utiliser des index temporels pour \u0026ldquo;partitionner\u0026rdquo; les données. De cette manière, il est facile de supprimer les données ajoutées au même moment en supprimant directement l\u0026rsquo;index.\nSi vous avez cependant besoin de définir des durées de vie différentes en fonction de vos données, il faudra gérer le mécanisme à la main : créer un champ spécial et appeler manuellement une requête \u0026ldquo;delete_by_query\u0026rdquo;. Ce sera toujours plus efficace qu\u0026rsquo;avec un ttl car le parcours et la suppression des données ne se fera qu\u0026rsquo;une seule fois (et en utilisant les index).\nDe manière générale, je pense que les TTL sont une fausse bonne idée : ils dégradent les performances et une meilleure conception de sa structure de données permet d\u0026rsquo;anticiper la question du nettoyage des données.\nLa gestion avancée du type de données string De nombreux types de données sont disponibles dans ES (dates, numériques, les tableaux…).\nLe type string, qui représente une chaîne de caractères, a vu son fonctionnement évoluer avec la version 5.0.0 (toujours la même). Celle-ci introduit la séparation du type string en text et keyword. La nuance est importante car elle est influence directement la manière dont on recherche les données :\n Le type keyword représente un champ qui ne pourra être recherché que par sa valeur exacte et pourra être utilisé dans le filtrage, les agrégations et les tris. On privilégie ce champ pour des données de référentiels (code postaux, mails, statuts…). * Le type text représente un champ dans lequel on pourra rechercher une partie de texte.\nIl est toutefois possible d\u0026rsquo;agréger / trier pour un type text en utilisant fielddata lors du mapping.  Ces nouvelles notions sont intéressantes car elles nous encouragent à mieux penser notre schéma de données en gardant bien à l\u0026rsquo;esprit que la question la plus importante est \u0026ldquo;comment je veux chercher mes données\u0026rdquo;.\nLa fin des mapping types En étant légèrement grossier, on pourrait dire que les mapping types sont à un index Elasticsearch ce que les tables sont à un schéma sur une base de données. On peut créer plusieurs mapping types sur un index, ce qui n\u0026rsquo;implique pas pour autant de jointure. La version 7 d\u0026rsquo;ElasticSearch va déprécier cet usage qui sera supprimé dans la version 8.\nPourquoi ? Parce que lorsque sur un même index, on a deux mapping types différents mais que des champs ont le même nom (par exemple une date), ces champs doivent être de même type (string, int\u0026hellip;). On peut supposer qu\u0026rsquo;une date sera toujours du type date, mais parfois, on va avoir un long, une string…\nLa solution de contournement ? Un index pour chacun des mapping types (qui s\u0026rsquo;appelle tous _doc). Elasticsearch permet de requêter dans plusieurs index en même temps (et même sur plusieurs clusters).\nMulticast vs Unicast Je me rappelle encore de ma première installation d\u0026rsquo;ElasticSearch : lors du premier démarrage, je regarde les logs et je vois le serveur commencer à discuter avec pleins de machines\u0026hellip;mais que se passait-il ?\nLa raison était très simple : par défaut ElasticSearch était configuré en mode multicast, c\u0026rsquo;est à dire qu\u0026rsquo;il demandait sur le réseau si des copains voulaient bien jouer avec lui\u0026hellip;autant dire que la première étape était de couper son serveur et d\u0026rsquo;aller voir la doc (ce que j\u0026rsquo;aurais surement dû faire depuis le début\u0026hellip;)\nLa version 5.0.0 supprime le plugin multicast, il faut désormais configurer le champ discovery.zen.ping.unicast.hosts afin de définir les nœuds à contacter (par défaut à 127.0.0.1).\nL\u0026rsquo;écosystème étendu En 2013, ElasticSearch était un produit isolé autour duquel gravitaient des outils de la communauté : des plugins, des outils. Kibana en est un parfait exemple (outil de dataviz).\nExemple de dashboard avec Kibana\nLes choses ont beaucoup changé et désormais, la suite Elastic s\u0026rsquo;est agrandie :\n Kibana est pleinement intégré à Elastic et fournit des fonctionnalités de Dataviz, monitoring\u0026hellip; * Logstash constitue un outil d\u0026rsquo;approvisionnement d\u0026rsquo;ElasticSearch (à la manière d\u0026rsquo;un ETL) * Beats et ses déclinaisons (FileBeat, MetricBeat…) sont des agents permettant de collecter des métriques sur des machines et de les transférer, entre autre, vers ElasticSearch. * Suite Elastic (anciennement X-pack) pour le monitoring et la gestion de la sécurité\u0026hellip; * Bonus : avec la version 6.5 vient une fonctionnalité utile et inattendue, la suppression de la licence gratuite d\u0026rsquo;un an pour l\u0026rsquo;usage basique d\u0026rsquo;ElasticSearch. C\u0026rsquo;est un réel avantage car sur un petit cluster, devoir chaque année régénérer une licence pour la mettre à jour…c\u0026rsquo;était fatigant.  Outils de la Stack Elastic\nConclusion ElasticSearch a beaucoup évolué ces dernières années. Cela tient en partie aux nouveaux marchés qu\u0026rsquo;il vise : à l\u0026rsquo;origine, on utilisait uniquement ES pour faire de la recherche full-texte, le cas d\u0026rsquo;usage était très précis et ça fonctionnait. Cela fait plusieurs années que le terme \u0026ldquo;ELK\u0026rdquo; est bien associé au stockage et à la recherche de log.\nDésormais, la réorientation time-series du moteur ainsi que les nombreux outils annexes ouvrent la voie à de nouveaux usages comme le monitoring. Attention toutefois à ne pas toujours utiliser ElasticSearch en pensant qu\u0026rsquo;il est le plus adapté car il \u0026ldquo;sait\u0026rdquo; le faire : InfluxDB est une solution particulièrement performante en terme de time-series, mais plus onéreuse.\nLes cas que j\u0026rsquo;ai traités dans cet article sont uniquement ceux auxquels j\u0026rsquo;ai été confronté. N\u0026rsquo;hésitez pas à commenter pour me faire part des évolutions qui vous ont particulièrement marqué.\n","date":"Feb 19, 2019","href":"https://blog.talanlabs.com/elasticsearch-back-to-the-future/","kind":"page","labs":null,"tags":["elasticsearch","retour d'experience"],"title":"ElasticSearch, back to the future"},{"category":null,"content":"Le contexte Une des grandes problématiques lorsque l’on constitue une équipe autour des technologies blockchain et DLT est la pénurie de talents. Si ce constat n’est évidemment pas nouveau dans l’IT, il est clairement renforcé lorsque l’on parle d’un secteur encore émergent. A travers nos différentes interventions auprès d’étudiants dans des écoles d’ingénieurs notamment, nous avons aussi saisi une tendance : la blockchain impressionne et peut parfois rebuter par son apparente complexité. C’est ainsi que nous avons choisi une autre approche en 2018/2019 : accompagner des étudiants dans un projet sur le long terme, afin de démystifier le sujet.\n Talan Labs a choisi de nouer un partenariat avec l’EPF, une école formant des ingénieur-e-s généralistes, pour un projet de 5 mois avec 5 étudiants de dernière année de la majeure Ingénierie du Numérique. De septembre 2018 à janvier 2019, nous avons donc accueilli Samuel, Romain, Robin, Théo et Florian tous les mercredis au siège de Talan.\n   Le sujet L’organisation d’événements ouverts au public auxquels les participants s’inscrivent et assistent à des conférences, des présentations de projets ou autre sur tous les thèmes. Des groupes se créent autour de thématiques et forment des communautés de passionnés qui organisent ces événements.\n Le projet Share2gether vise a créer une plateforme décentralisée d’organisation d’événements, basée sur Ethereum et accédée via une interface web.\n   La problématique Souvent le nombre de personnes inscrites est largement supérieur au nombre de personnes qui participent vraiment à l’événement : c’est la problématique du no-show. De ce fait, il est difficile de prévoir l’organisation nécessaire pour son événement, il y a beaucoup de gâchis au niveau du buffet, les speakers s’attendent à une certaine audience qui n’est pas là etc.\n De plus, pour des événements dont le nombre de place est limité, des personnes s’inscrivent qui voudraient vraiment venir, et sont placées en liste d’attente. Ces personnes ne pourront finalement pas participer à l’événement alors qu’il y aurait eu de la place pour les accueillir (puisque beaucoup de personnes s’inscrivent mais ne viennent pas).\n   Le produit Une application décentralisée qui permet de créer des communautés et d’organiser des événements tout en responsabilisant les utilisateurs, pour contrer la problématique du no-show. Plus d’info seront dévoilées dans un prochain article !\n   L’équipe Pour réaliser notre produit, Talan Labs a formé une équipe entière pour travailler en mode agile. Avec une dev team composé des cinq étudiants, de l’équipe Blockchain de Talan Labs et deux designers, un Scrum Master, et un Product Owner (PO).\n  La dev team presque au complet    Le rythme de travail En venant une journée complète par semaine, les étudiants bénéficiaient des conseils et de l’aide des Talan. Mais ce n’était pas leurs seuls moments de travail sur le projet. Avec un investissement conséquent toute la semaine, à distance. Ce rythme particulier n’a pas été sans poser des questions d’organisation.\n Nous avons donc adapté nos méthodes de travail habituelles, en tenant compte de contraintes opérationnelles inhérentes à nos journées souvent chargées par ailleurs. C’est ainsi que nous commencions nos mercredis par une démo et un sprint planning, ne faisant la rétrospective du sprint passé que dans l’après-midi. Le DSM clôturait la journée pour que chacun bénéficie du même niveau d’information.\n Très vite, il a fallu renforcer le rythme de ces cérémonies, avec un DSM les vendredis et lundis après les cours. Et puisque la démonstration des fonctionnalités réalisées est fondamentale pour rassurer le product owner, les mardis soirs permettaient aux étudiants de répéter la démo.\n C’est ainsi que petit à petit l’équipe a pris son rythme et que les réflexes se sont installés.\n   L’apprentissage Tout au long de ces cinq mois, les étudiants se sont formés sur plusieurs technos et concepts. Tout d’abord, il a été nécessaire d’apprendre les concepts de la blockchain, puis les bases du développement en solidity, le langage de développement des smarts-contracts sur Ethereum. Florian nous a d’ailleurs confié\n \u0026#34;Nous avons rapidement découvert comment mettre en place notre environnement de travail, avec des outils comme Ganache et Truffle qui nous ont permis de commencer à développer très rapidement.\u0026#34; — Florian\n Mais ce n’est pas tout, nous avons décidé d’utiliser un framework JS : Vue, qu’il a fallu que les étudiants appréhendent aussi. Et puis, nous avons aussi respecté au mieux les bonnes pratiques et notamment l’automatisation de tests.\n Enfin il y a bien sûr eu le côté plus humain :\n \u0026#34;J’ai beaucoup aimé le côté humain de ce projet, les membres de l’équipe de projet étaient toujours soudés entre eux et il y avait toujours beaucoup de respect entre chaque membre. Cet aspect nous a énormément aidé lors de phases plus difficiles en nous apportant de la motivation pour surmonter les problèmes qu’on a pu avoir.\u0026#34; — Robin\n   Conclusion L’équipe de Talan Labs a assisté à la soutenance finale du projet sur le site de l’EPF le 25 janvier, une équipe solidaire jusqu’au bout, qui a réussi une belle collaboration sur le thème de la blockchain.\n Vous pouvez retrouver la vidéo de leur soutenance ci-dessous.\n      ","date":"Feb 15, 2019","href":"https://blog.talanlabs.com/experience-share2gether/","kind":"page","labs":null,"tags":["ethereum","EPF","share2gether"],"title":"Share2gether : collaboration avec l'EPF"},{"category":null,"content":"Le petit dernier Il y a un an, Jeff Sutherland, co-fondateur de Scrum, a officialisé la naissance de son dernier né : Scrum@Scale®️. Malgré tout sa gestation a été longue (il se base sur des pratiques mises en œuvre dès 2001) et on peut déjà compter sur plusieurs retours d’expérience.\nCarte d’identité _Nom _: Scrum@Scale®️ Catégorie : mise à l’échelle de Scrum _Créateur _: Jeff Sutherland _Documentation de référence _: https://www.scrumatscale.com _Cible _: toute ou partie d’une organisation\nUne nouvelle organisation Balance des rôles Scrum d\u0026rsquo;après Hendrik Kniberg (Agile Product Ownership in a Nutshell)\nL’efficacité d’une équipe Scrum est le résultat de la synergie de trois rôles : le Product Owner qui définit ce qui doit être fait, les développeurs qui définissent comment cela doit être fait et le Scrum Master qui aide à améliorer la productivité en supprimant les obstacles, entre autres.\nL’ambition de Scrum@Scale®️ est de faire la même chose à l’échelle de l’organisation.\nScrum à tous les étages L’organigramme d’une organisation Scrum@Scale®️ est un réseau d’équipes Scrum.\nChaque groupe d’équipes ayant besoin de se coordonner possède deux équipes virtuelles : le Scrum de Scrum (SoS) et l’équipe des Product Owners (elle n’a pas de nom spécifique ; MS ou CPO sur les schémas)\nSelon les besoins ces équipes virtuelles peuvent elles-mêmes se regrouper autour de deux autres équipes virtuelles : le Scrum de Scrum de Scrum (SoSoS) et l’équipe d’équipes des Product Owners (CCPO MS sur les schémas)\nCes équipes virtuelles représentent la mise à l’échelle des rôles du Scrum Master et du Product Owner.\nMise à l’échelle du Scrum Master Le Scrum Master à l’échelle s’appelle le cycle du Scrum Master.\nSes principaux objectifs sont de :\n partager les bonnes pratiques * exposer et supprimer les obstacles * assurer la coordination entre les équipes * assurer l’intégration des incréments produits par chaque équipe  Il travaille autour d’un backlog des obstacles.\nMise à l’échelle du Product Owner Le Product Owner à l’échelle s’appelle le cycle du Product Owner.\nSes principaux objectifs sont de :\n maintenir la vision produit * optimiser la valeur métier * répondre aux changements du marché  Il travaille autour d’un backlog produit partagé qui alimente les différentes équipes de réalisation.\nAvec l’engagement de la direction La direction de l’organisation fait partie du système sous la forme de l’Executive Action Team (EAT) et l’Executive Meta Scrum (EMS).\nMise à l\u0026rsquo;échelle du Scrum Master\nL’Executive Action Team est à la tête du cycle du Scrum Master.\nSa mission est de :\n gérer le backlog de transformation de l’organisation * supprimer les obstacles n’ayant pas pu être gérés au niveau des équipes * mesurer et améliorer la qualité de Scrum  En tant que dernier échelon dans la suppression des obstacles, l’EAT est composé de personnes ayant le pouvoir politique et financier de les supprimer.\nMise à l\u0026rsquo;échelle du Product Owner\nL’Executive Meta Scrum est à la tête du cycle du Product Owner.\nSa mission est de :\n aligner et définir les priorités stratégiques de l’organisation * définir la vision de l’organisation * donner les priorités de l’organisation  Mise en œuvre Une mise à l’échelle progressive Le guide de Scrum@Scale® propose un déploiement progressif en partant (d’un modèle de référence constitué) de quelques équipes soutenues par l’EAT. L’objectif de cette approche est d’adresser les problèmes au plus tôt, avant qu’ils ne soient décuplés par la mise à l’échelle.\nLa stratégie est : commencer petit, optimiser en réglant les obstacles, faire grossir en fonction des besoins et optimiser de nouveau.\nAdaptation des processus Cycles du Scrum Master et du Product Owner - les modules\nScrum@Scale® est un framework modulaire. Chaque module est défini par un triplet objectifs/entrées/sorties. Toute pratique existante qui correspond au triplet d’un module Scrum@Scale®️ fonctionnera avec les autres modules du framework.\nCette approche permet une mise en œuvre par adaptation des pratiques existantes plutôt que de partir de zéro.\nLa stratégie devient : garder ce que l’on fait bien, ajouter ce que nous ne faisons pas, jeter le superflu et améliorer.\nConclusion Le dernier né des frameworks de mise à l’échelle de Scrum se pose comme l’évolution ultime de Scrum avec son ambition déclarée de “saturer l’organisation avec Scrum”.\nIl soutient l’idée que la mise en œuvre de Scrum, et par conséquent de Scrum@Scale®️, ne peut être efficace qu’avec l’engagement de la direction.\nAvec sa capacité de s’appliquer à toute une organisation, Scrum@Scale®️ s’affiche comme un concurrent de SAFe®️. Son signe distinctif : être simple à comprendre et léger en offrant le minimum de bureaucratie viable.\nComme Scrum et tous les autres frameworks de mise à l’échelle, Scrum@Scale®️ reste toutefois difficile à maîtriser car il demande de redéfinir le modèle opérationnel de l’organisation.\n","date":"Feb 5, 2019","href":"https://blog.talanlabs.com/scrum-scale-cest-quoi/","kind":"page","labs":null,"tags":["Agilité à l'échelle","Scrum","Scrum@Scale"],"title":"Scrum@Scale®️, c'est quoi ?"},{"category":null,"content":"Le mercredi 30 janvier 2019, Talan Labs a accueilli l’Asseth, l’association française dédiée à l’éducation et la promotion de la blockchain Ethereum et son écosystème. Ce premier meetup de l’année était tourné sous la forme d’une table ronde afin de donner la parole aux membres sur l’avenir de l’association.\n Retour sur 2018 En 2018, l’Asseth a continué de promouvoir, développer, expérimenter et mettre en pratique la technologie Ethereum. Mais c’est aussi une association ouverte d’esprit qui accueille volontiers lors de ses meetups des présentations sur des technologies autre qu’Ethereum.\n L’Asseth porte dans ses valeurs le partage de la connaissance, l’association a organisé 24 meetups au cours de l’année 2018 (et pas seulement à Paris !) et a organisé EthCC (Ethereum Community Conference) qui propose de nombreuses conférences et workshops qui regroupent la communauté Ethereum. L’Asseth a aussi sponsorisé des événements comme le Swarm Summit ou encore Blockchain for social impact hackathon.\n Enfin l’Asseth garde sa volonté de rester indépendante et désintéressée financièrement. L’assocation refuse de promouvoir des ICO, n’obtient pas de rémunération pour faire des meetups et aucune entreprise n’est autorisée à devenir membre.\n   Vers 2019 L’Asseth lance la Community Blockchain week, une initiative qui permet de proposer des événements (workshops, meetups, hackathons …​) sur des thématiques techniques liées à n’importe quelle blockchain.\n En 2019, l’Asseth organise la deuxième édition d’EthCC, va soutenir EthParis (un hackathon Ethereum) et continuer de soutenir la \u0026#39;Fellowship of Ethereum Magicians\u0026#39;. L’association va aussi fusionner avec Ethereum-France.com.\n Et enfin, l’Asseth va organiser au moins un meetup par mois dans les locaux de Talan à Paris, sur des sujets variés. Nous pouvons d’ores et déjà vous annoncer que le prochain meetup Asseth hébergé chez Talan aura lieu le jeudi 28 février 2019.\n   Table ronde Les membres ont eu leur mot à dire lors de ce meetup Table ronde, et beaucoup d’idées sont apparues pour améliorer l’Asseth, par exemple, organiser des meetups \u0026#34;débutant\u0026#34; pour attirer des profils encore novices sur la technologie, faire plus de meetups en régions, ou encore se rapprocher des écoles et universités.\n D’autres idées ont encore été évoquées comme participer à des événements plus généralistes ou s’intégrer à des événements orientés sur d’autres technos, par exemple participer à un événement Java en amenant le côté blockchain.\n Des idées sur l’organisation de l’association ont aussi émergées, faut-il confier des rôles aux membres ? Ouvrir les calls bi-mensuel aux membres ?\n   Conclusion L’Asseth organise des meetups récurrents chez Talan, n’oubliez pas de venir !\n   ","date":"Feb 4, 2019","href":"https://blog.talanlabs.com/asseth-meetup-janvier/","kind":"page","labs":["Lab K"],"tags":["meetup","asseth"],"title":"Meetup Asseth : table ronde"},{"category":null,"content":"I’ve recently started working on a new project with a new team. As we were just discovering each other’s, I was looking for a funny game with as objectives: helping on the team building, teaching on the importance of the communication and showing that we become better by getting used to work together (hehe easy 😉). Here is what I’ve prepared: the “threads race”.\nPreparation You need a bit of preparation for this game and some materials:\n A cardboard 20cm*20cm * One felt pen like a Sharpie * Threads between 60 and 80 cm, one by player * A wide sheet of paper (I’ve used 4 A3 sheets) * Something sharp to do little holes in the cardboard  The cardboard will be the support for the pen, drill a whole in the middle of the card and drill as much little wholes for the threads all around the cardboard. Attach the threads to the cardboard and put the pen in the middle.\nOn the Paper draw a race circuit, the difficulty of the game will be in the width of the road and in the number of bends. My advice here is to make it hard 😉.\nHow to play? Each of the participant will take a thread and the team will have to move the pen all along the circuit without interruption in the line and without living the circuit. The card board is hiding a part of the road which is complicating the task. This activity will ask to the team a bit of collective coordination which is great, this is my target.\nHow to organize the workshop?\n1rst iteration I started by asking the team to put the pen on the starting line and to do as much tries as they want until they succeed. Conditions for the success: No blanks in the draw line and no draw outside of the road, if this happens, they must go back to the starting line.\nAfter few tries the team has succeed even if it was not easy task.\nFacilitator advise here: Limit this phase to 8 minutes including 3 minutes of retrospective.\n2nd iteration Ask everybody to close their eyes and propose to the team to choose one of them to guide the crew through the bends to victory.\nhttps://youtu.be/Ax3Kgc9Vit8\nAfter two very slow tries (\u0026gt;5 minutes) the pen has crossed the finishing line, we’ve took few minutes to debrief on what was working well or not and I’ve proposed a last challenge.\n3rd iteration Ask the team to close their eyes, except two of them, and to cross the finishing line in less than 2min. This time I gave them two minutes to organize themselves as they want and to tell me when they were ready. 2 minutes later the team were succeeding the challenge !\nWhat happened? During the first iteration, the team were not communicating, so they did succeed, but with a lot of difficulties and with many tries.\nDuring the Second iteration, the team had to communicate to the only person who was not blind, it was not the most efficient way, they where slow, but only two tries were necessary to cross the line.\nDuring the last iteration, they were auto-organized quite efficiently. They didn’t thought that they would succeed but they did it, because in few iterations they started to know each other and communicate intensively. That was the key of the success.\nConclusion We did have a lot of fun playing this game and it was a great channel to aboard different important aspects of the collective skills we need to develop as a Scrum Team. I think I will maybe use it in the future as ice breaker for some training and I hope it will be useful for you as well.\n   ","date":"Jan 31, 2019","href":"https://blog.talanlabs.com/threads-race-a-funny-team-building-game-30min/","kind":"page","labs":null,"tags":["communication","continuous improvement","gamification","jeux","ludification","serious game","team building"],"title":"« Threads race » a funny team building game (30min)"},{"category":null,"content":"Les quatre questions de Norman Keth\nUne rétrospective de projet J’ai eu à organiser et animer une rétrospective de projet avec un public particulier : les parties prenantes. Celles-ci venaient de tous les horizons (responsable de département, chargé de clientèle, juriste, etc) et tous les âges étaient représentés. Ce sont des personnes qui n’ont pas forcément l’habitude de la rétrospective et dont le temps est précieux.\nPour prendre en compte toutes ces contraintes, j’ai choisi une approche simple, directe et surtout accessible : les quatre questions de Norman Keth.\n What did we do well, that if we didn’t discuss we might forget? * What did we learn? * What should we do differently next time? * What still puzzles us?  Ou en français :\n Qu’avons nous bien fait, que nous pourrions oublier si nous n’en discutons pas ? * Qu’avons nous appris ? * Que devrions nous faire différemment la prochaine fois ? * Qu’est-ce qui nous étonne encore ?  Norman Keth est l’auteur du livre “Project retrospectives : a handbook for team reviews”. Il est aujourd’hui considéré comme le père de la rétrospective. On lui doit en particulier cette déclaration qui illustre l’état d’esprit dans lequel nous devons être quand nous faisons une rétrospective :\n “Regardless of what we discover, we understand and truly believe that everyone did the best job they could, given what they knew at the time, their skills and abilities, the resources available, and the situation at hand”\nNorman Kerth, Project Retrospectives: A Handbook for Team Review\n Ou en français :\n “Indépendamment de ce que nous découvrons, nous devons comprendre et croire sincèrement que chacun a fait le meilleur travail qu’il [NdA : ou elle] pouvait, au vu de ce qu’il [elle] savait alors, de ses compétences, des ressources disponibles et du contexte.”\nNorman Kerth, Project Retrospectives: A Handbook for Team Review\n Une phrase plutôt qu’un dessin Learning Matrix\nIl existe plusieurs exercices de rétrospective basés sur le même principe que les quatre questions. Par exemple “Mad Sad Glad” ou “Learning Matrix” qui permettent de répartir les idées selon trois ou quatre catégories.\nCes exercices sont néanmoins très différents dans leur présentation. En effet, chaque catégorie qu\u0026rsquo;ils définissent sont représentées par un mot clef ou une illustration. Cette forme concise force les participants à interpréter le sens de la catégorie avant de pouvoir partager l\u0026rsquo;expérience passée de leur projet.\nPar exemple, si on considère la question “What did we do well, that if we didn’t discuss we might forget?”, elle incite explicitement à regarder tous les petits pas qui nous ont permis d’arriver là où nous sommes. A contrario, dans un exercice comme Learning Matrix, ce qui ressort en général avec un “:)” ou un “Appreciation” est équivalent à une tape dans le dos… sauf si nous avons l’expérience de la rétrospective ou que l’animateur nous guide.\nAvec les quatre questions, les participants rentrent plus vite dans l’exercice et leur réflexion est mieux guidée, on gagne du temps surtout avec une audience néophyte.\nDes faits plutôt que des émotions Beaucoup d’exercices de rétrospective font appel aux émotions des participants et, soyez étonnés ou non, cela peut braquer certaines personnes : “on a un travail à faire, on le fait ; nos émotions n’ont rien à faire là-dedans”.\nLes questions de Norman Keth ont l’avantage de faire ressortir les faits plutôt que les ressentis.\nComme dans la communication non violente, en partant de faits, nous construisons une base saine pour la discussion. L’exercice a plus de chances d’être efficace.\nFaire l’analyse des succès La question “What still puzzles us?” incite à regarder ce que nous n’avons pas maîtrisé. Les facteurs extérieurs sont rarement considérés dans les ateliers de rétrospective \u0026ndash; je ne vois que le “Speedboat”. C’est un des aspects des quatre questions que j’apprécie car il nous permet de mettre en balance nos succès (ou échecs) et nos réalisations.\nDans sa présentation “Keep your crises small”, Ed Catmull, co-fondateur du studio d\u0026rsquo;animation Pixar, met l’accent sur le fait que même un succès peut cacher un problème : il faut prendre en compte la chance car on ne peut pas toujours compter dessus.\nAlors comment ça s’est passé ? Dans la pratique, quelques interviews préalables des parties prenantes ont permis de centrer le débat sur quatre problèmes. Pour chaque problème, les participants ont répondu aux quatre questions, suivi par un vote et par une discussion pour la définition des actions.\nC’est avec un certain plaisir que j’ai pu voir tous les participants contribuer. Les débats ont été bienveillants et constructifs. Le ROTI (Return Of Time Invested) a été de 4/5.\nÀ refaire Ce format de rétrospective est très utile quand on a une audience variée et dont on ne connaît pas tous les membres. Je le recommande aussi pour sa partie “analyse du succès” pour rappeler à votre équipe de ne pas s’endormir sur ses lauriers.\n","date":"Jan 21, 2019","href":"https://blog.talanlabs.com/retrospective-les-quatre-questions-de-norman-keth/","kind":"page","labs":null,"tags":["atelier","rétrospective"],"title":"Rétrospective : les quatre questions de Norman Keth"},{"category":null,"content":"Carte d’identité Nom : Large-Scale Scrum aka LeSS\nCatégorie : mise à l’échelle de Scrum\nCréateurs : Craig Larman \u0026amp; Bas Vodde\nDocumentation de référence : https://less.work\nCible : plusieurs équipes travaillant ensemble sur le même produit ; produit au sens solution adressant les problèmes clients/utilisateurs et utilisée par de vrais clients/utilisateurs.\nPetit rappel sur Scrum Scrum est un cadre de développement dans lequel une équipe développe un produit de manière incrémentale et itérative. À chaque Sprint, un incrément de produit est livré. Un Product Owner (PO) est responsable de la maximisation de la valeur du produit, de la hiérarchisation des éléments dans le backlog de produit et de la détermination du but de chaque sprint tout cela en se basant sur des retours d\u0026rsquo;expérience et un apprentissage constants. Une équipe de développement est responsable de la réalisation de l\u0026rsquo;objectif de Sprint. Un Scrum Master aide le Product Owner, l’équipe de développement et l’organisation à appliquer Scrum et à en tirer profit.\nVous trouverez ici une présentation succincte illustrée et là la documentation officielle.\nLeSS c’est Scrum LeSS se veut une extension de Scrum pour plusieurs équipes travaillant sur le même produit. Nous retrouvons donc dans LeSS tous les rôles, événements et artéfacts définis par Scrum.\nLeSS se veut léger, simple et peu contraignant. Pour cela, il n’introduit ni nouveaux rôles ni nouveaux artéfacts.\nÀ quoi ça ressemble ? À l’échelle du produit, LeSS prend la forme du Scrum classique : un PO unique qui donne la vision, un Backlog produit, un Sprint, un planning de Sprint, une revue de Sprint, une rétrospective, un incrément et une définition de “Fini”.\nCe qui diffère de Scrum Là où, dans Scrum, on a une équipe de développement accompagnée d’un Scrum master, on trouve dans LeSS plusieurs équipes de développement chacune aidée d’un Scrum master.\nÀ cause du nombre de personnes impliquées, les événements Scrum ont été adaptés.\nLe planning de Sprint a pour objectifs de déterminer ce que nous allons faire et comment nous allons le faire. LeSS le divise donc en deux phases :\n le PO et les représentants (ie. personnes pertinentes) de chaque équipe déterminent les fonctionnalités à réaliser pendant le Sprint et se les répartissent ; 2. chaque équipe décide de son côté comment elle effectuera le travail nécessaire pour achever l’incrément.  Lors de la rétrospective, l’équipe cherche à améliorer sa manière de travailler. LeSS la divise en deux phases pour adresser deux périmètres :\n chaque équipe de développement avec un focus sur son fonctionnement interne ; 2. le produit pour adresser les problèmes liés à l’organisation ; y participent le PO, les Scrum masters et les représentants (ie. personnes pertinentes) de chaque équipe de développement  Communication/Synchronisation Les besoins de synchronisation entre les équipes sont identifiés lors de la première phase du planning de Sprint.\nLa synchronisation entre les équipes est de la responsabilité des équipes. Il n’y a pas de rôle ou d’évènement spécifique pour la coordination des équipes.\nUn dialogue direct entre les clients/utilisateurs et les équipes permet à la fois d’alléger la charge du PO et de rapprocher les développeurs du terrain. Le PO se positionne alors en tant que facilitateur plutôt qu’intermédiaire pour les tâches de clarification des éléments du Backlog.\nEt LeSS ne spécifie rien de plus sur la forme.\nLa partie immergée de l’iceberg Les règles qui décrivent le cadre ne représentent qu’une petite partie de LeSS.\nLe cœur de LeSS est constitué de dix principes qui guident la mise en œuvre :\n Large Scale Scrum est Scrum : facile à comprendre, difficile à maîtriser et dont la mise en œuvre risque de révéler les problèmes organisationnels ; 2. transparence : principe édicté par Scrum comme la base de l’amélioration continue ; 3. “more with less” : être plus efficace avec moins de complexité (procédures, rôles, bureaucratie, etc) ; 4. le produit est un tout (whole product focus) : tant que ce n’est pas intégré au produit ça n’a aucune valeur pour le client/utilisateur ; 5. centré sur le client/utilisateur final : les processus et les personnes doivent apporter de la valeur au client de la manière la plus directe possible ; 6. amélioration continue jusqu’à la perfection : continuer de s’améliorer même si le système en place est satisfaisant ; 7. Lean : embrasser les principes du Lean ; 8. pensée systémique (Systems thinking) : travailler sur le système dans sa globalité et éviter les optimisations locales ; 9. processus de contrôle empirique : c’est l’essence de Scrum, tester, analyser et adapter de manière itérative ; 10. la théorie des files : supprimer les files dans l’organisation et les processus séquentiels pour éviter les goulots d’étranglement et les ralentissements.  Vous en aurez reconnus la plupart : ils sont essentiellement issus ou inspirés du manifeste Agile, de Scrum et du Lean.\nLa documentation fournit aussi des conseils et recommandations pour la mise en œuvre de LeSS et en faciliter l’adoption. Ils concernent :\n l’adoption pour que les équipes s’approprient la nouvelle organisation ;  l\u0026rsquo;excellence technique pour que les équipes puissent livrer fréquemment un logiciel de qualité ; la structure des équipes pour qu’elles partagent au mieux les informations utiles sans être ralenties par les interactions superflues ; le management pour que celui-ci passe de la direction à l’accompagnement des équipes.    On y trouve aussi de nombreux retours d’expérimentations dans diverses entreprises qui constitueront une base d’inspiration pour construire votre propre implémentation.\nJusqu’où le modèle peut-il tenir ? Jusqu’à ce que le PO fasse un burnout !\nLe PO est en effet unique quelque soit le nombre d’équipes \u0026hellip; Équipes qu’il faut alimenter en fonctionnalités \u0026hellip; Fonctionnalités qu’il faut recueillir et affiner \u0026hellip;\nEmpiriquement, la limite a été mise à huit équipes. Au delà de huit équipes, LeSS propose une version Huge du framework qui est une mise à l’échelle de la version standard.\nLa version Huge propose simplement de séparer les éléments du Backlog par “domaines de prérequis” et d’en déléguer la gestion à des PO de domaine (ou APO). Pour le reste tout fonctionne comme dans la version pour moins de huit équipes.\nPoints d’attention La charge du PO Le point faible de LeSS est la charge que l’on met sur le PO. Même si LeSS a quelques conseils pour la limiter, elle est importante à prendre en compte, et ce, dès le début de la mise en place de LeSS. On peut pour cela porter une attention particulière à la définition du produit. La définition du produit permet d’identifier naturellement le nombre de personnes impliquées. Le périmètre sur lequel vous souhaitez mettre en place LeSS ne sera peut-être plus celui auquel vous pensiez au départ …\nMore with LeSS S’il ne fallait retenir qu’un principe ce serait celui-là : « More with LeSS ». Il incarne la philosophie de la démarche des créateurs de LeSS : réduire la complexité (procédures, rôles, bureaucratie, organisations imposées, etc.) pour augmenter la valeur apportée aux clients/utilisateurs.\nEn suivant ce principe et le constat par lequel il est plus facile d’ajouter que de retirer des règles, LeSS donne une structure minimale juste suffisante dont les équipes ont besoin pour démarrer. L’adaptation nécessaire qui suivra permettra aux équipes de s’approprier leurs process.\nCe principe encourage aussi les équipes à résister à la tendance naturelle de rajout de complexité pour résoudre les problèmes. Un des moyens d’y parvenir est d’adresser la source du problème plutôt que de s’attaquer aux symptômes.\nPasser à l’échelle c’est transformer LeSS est un framework qui séduit de par sa simplicité surtout quand on est déjà familier avec Scrum. Mais, comme Scrum, sa simplicité cache la difficulté à le maîtriser. Cette difficulté n’est pas propre à LeSS, elle est au cœur de toutes les transformation organisationnelles.\nLe succès d’une transformation repose sur quelques principes clefs : ne pas changer pour changer, accepter de sortir les squelettes du placard, engager les individus, faire en sorte que les acteurs s’approprient le système, etc. Les auteurs de LeSS adressent cette difficulté sous forme de conseils et d’exemples concrets issus d’expérimentations de mise à l’échelle de Scrum. Leur présence dans le framework même rassure quant à la maturité de LeSS : les auteurs vous annoncent clairement la couleur, la mise en œuvre demandera du travail.\nCommencer petit Tous les auteurs de framework de mise à l’échelle de Scrum sont d’accord sur ce point : il faut commencer par faire fonctionner Scrum correctement dans une équipe avant de passer à l’échelle.\nUne des raisons pour laquelle la mise en place Scrum au sein d’une équipe échoue est souvent que les principes de Scrum ne sont pas respectés :\n l’équipe de développement est-elle réellement en capacité de réaliser l’incrément ? Sans dépendance extérieure qui la ralentisse ?  l’équipe de développement est-elle réellement habilitée à s’auto-organiser ? le PO a-t’il le dernier mot quant à la priorisation du backlog produit ? est-ce que la mission principale du PO est bien d’être PO ? L’organisation lui accorde-t’elle bien le temps et l’aide nécessaire pour accomplir sa mission ?    Pour LeSS c’est pareil … et c’est d’autant plus important que la mise à l’échelle accentue les problèmes. Alors autant les résoudre avant de passer à l’échelle ;)\nConclusion Comme Scrum, LeSS propose une organisation simple qui laisse de la marge pour s’adapter à votre situation. Comme avec Scrum, pour en tirer le maximum de bénéfices de LeSS, il faut respecter les principes et les valeurs qui soutiennent le framework. Cela représente un challenge qui, je pense, mérite d’être relevé. Réussir à mettre en place LeSS permettra aux équipes de se concentrer sur livrer de la valeur aux clients/utilisateurs plutôt que suivre des processus dont elles ont oublié la raison d’être. On pourra appeler ça être agile si ça fait plaisir, mais est-ce le plus important ?\n","date":"Jan 14, 2019","href":"https://blog.talanlabs.com/less-cest-quoi/","kind":"page","labs":null,"tags":null,"title":"LeSS, c’est quoi ?"},{"category":null,"content":"Carte d’identité Nom : Nexus Créateur : Ken Schwaber Documentation de référence : https://www.scrum.org/resources/nexus-guide Cible : un produit de développement logiciel avec trois à neufs équipes Scrum impliquées\nAvant d’aller plus loin Il est fortement conseillé de connaître les bases Scrum avant de lire cet article. Par ici pour la documentation officielle et là pour une présentation succincte.\nTout pour l’intégration Nexus se veut complémentaire à Scrum en ajoutant juste ce qu’il faut pour adresser les principaux problèmes qui apparaissent quand plusieurs équipes doivent travailler ensemble : la communication, les dépendances et la capacité à livrer un incrément intégré.\nÀ quoi ça ressemble ? À l’échelle du produit, c’est le Scrum classique : un PO unique qui donne la vision, un Backlog produit, un Sprint, un planning de Sprint, une revue de Sprint, une rétrospective, un incrément et une définition de “Fini”.\n“L’équipe de développement” est constituée de trois à neuf équipes de développement chacune accompagnée d’un Scrum master.\nLa différence la plus visible avec Scrum est l’ajout d’une équipe d’intégration. Sont but est de coordonner, coacher et superviser l’application de Nexus. En ce sens elle est à Nexus ce que le Scrum master est à Scrum.\nScrum à l’échelle du Nexus Nexus redéfinit les cérémonies et artéfacts de Scrum pour que, en plus de leurs objectifs définis dans le guide Scrum, ils gèrent les dépendances et assurent la livraison d’un incrément intégré et “Fini”.\nLe **Backlog de Sprint **Nexus, en tant que Backlog de Sprint partagé par toutes les équipes sert à matérialiser les dépendances entre tous les items du Backlog produit traités lors du Sprint.\nLe raffinement de Backlog Nexus sert à identifier les dépendances.\nLa planification de Sprint Nexus se divise en deux phases :\n comme la planification Scrum, le PO discute avec toute l’équipe de réalisation (ie. tout le Nexus) ou ses représentants appropriés l’objectif du Sprint et la liste des items à embarquer ; 2. après la répartition des items entre les différentes équipes, chaque équipe finit sa planification dans son coin.  Le **Daily **Nexus se divise en deux phases :\n entre les représentants appropriés des équipes pour inspecter l’état de l’incrément intégré et les dépendances ; 2. chaque équipe prend en compte les informations issues de la première phase dans son propre daily.  La revue de Sprint Nexus est la revue unique du produit. Elle implique donc tout le Nexus et remplace les revues individuelles.\nLa rétrospective de Sprint Nexus se divise en trois phases :\n entre les représentants appropriés des équipes pour se focaliser sur les problèmes qui ont impactés plusieurs équipes ; 2. chaque équipe tient sa propre rétrospective telle que décrite dans le guide Scrum ; 3. à nouveau entre les représentants appropriés des équipes pour s’accorder sur le suivi des actions.  La botte secrète : l’équipe d’intégration L’équipe d’intégration est une équipe dont l’objectif est d’assurer que les équipes de développement produisent un incrément intégré. Et pour cela, tous les moyens sont bons :\n mise en avant des dépendances ;  accompagner les équipes du Nexus pour la mise en œuvre des pratiques et des outils nécessaires pour détecter les dépendances ; mise en avant des problèmes inter-équipes ; résolution des contraintes techniques et non techniques inter-équipes ; accompagner les équipes du Nexus pour la mise en œuvre des pratiques et des outils nécessaires pour intégrer fréquemment leur travail dans l’incrément ; accompagner les équipes du Nexus sur l’application des standards de développement, d’infrastructure ou d\u0026rsquo;architecture requis par l’organisation.    Pas d’intégration ? Eh non, l’équipe ne réalise pas elle même l’intégration, mais elle travaille pour coordonner, encadrer, identifier les dépendances entre équipes et s\u0026rsquo;assurer que les meilleures pratiques sont suivies.\nL’équipe d’intégration est composée du PO du Nexus, d’un Scrum master et des personnes adéquates pour réaliser sa mission. Ses membres sont souvent aussi membre d’un des équipes Scrum du Nexus mais leur travail au sein de l’équipe d’intégration est prioritaire sur tout autre engagement.\nConclusion S’il faut résumer Nexus, ce serait Scrum avec plusieurs équipes de développement et une équipe d’intégration pour traiter les problèmes de mise à l’échelle.\nCela fait de Nexus un framework simple qui, en proposant une solution concrète pour la mise à l’échelle, peut rassurer ceux qui ont besoin d’un cadre un minimum directif.\nAu cœur du dispositif, l’équipe d’intégration peut, de prime abord, être perçue comme un goulot d’étranglement ou un processus de plus mais en réalité, de par son rôle de servant leader, c’est un outil d’amélioration et d’adaptation pour le Nexus dans son ensemble.\nS’il faut trouver à redire, on peut s’attaquer au fait que Nexus soit conçu pour le développement logiciel. Mais en restreignant son champ d’application, son auteur n’apporte-t’il pas une solution plus pertinente ?\nOn peut aussi regretter que les nombreux retours d’expérience et guides dont se targuent les promoteurs de Nexus ne soient pas disponibles (je n’en ai trouvé moins de 10).\nEnfin, Nexus part du principe que les équipes Scrum font déjà du Scrum “professionnel”, c’est à dire qu’elles comprennent et travaillent déjà en accord avec les valeurs et principes de Scrum. Cela peut paraître contraignant mais, en définitive, ce prérequis est commun à tous les frameworks de mise à l’échelle de Scrum.\n","date":"Jan 9, 2019","href":"https://blog.talanlabs.com/nexus-cest-quoi/","kind":"page","labs":null,"tags":["nexus"],"title":"Nexus, c'est quoi ?"},{"category":null,"content":"I’ve given (with a great pleasure) this Monday and Tuesday my 2 days Scrum Master training, for the first time facing an attendance of external people of Talan Labs. It has been a great success (the audience has without any exception evaluate the training “beyond their expectations”), and a part of this success is the gamification of the ways to teach good practices and print a different mindset.\nI love to prepare my workshops and training based on gamification. I will not try here to explain how and why this is the best way to share ideas, but if you want, we can discuss about it.\nSo my 2 days training is composed of many games and one of them is the “penny game”. I really love it to make understand the scrum approach, the differences between a sequential and an parallelized approach. I use it as well to compare the Fordism to Toyotism, production organized around push or pull philosophy.\nWhat do I need ? I like to use some poker coins but you can use some real coins if you have enough. So first you need 45 coins with different values by group:\n 10 coins with a value of 5  15 coins with a value of 2 20 coins with a value of 1    I advise that the group is not exciding 6 and is constitute at least of 4 persons.\nEach group has his own table and the table is free of other furniture.\nIf you count the global value of all the coins is 100.\nFor each coming exercise do not announce the name of what you are doing, don’t speak yet about phases or Pull or parallelized, I advise to only give instruction on how to flip the coins. At the end of each approach, ask the teams how do they feel with this practice and continue to the following exercise.\nFirst, let’s simulate phases Ask to the first person of the table to flip all the coins one by one and then give the pile (45 coins) to his nearby colleague, which should do the same thing and then give the pile again to the next person etc. etc.\nMeasure the global time to flip all the coins and write it on the board.\nTry a first parallelized approach This time ask the team to flip the coins one by one again but after 15 coins they can pass the pile to the next persons and start to flip another pile of 15 coins. Write the global time to flip all the coins (3 piles of 15 coins passed through everybody in the group), which should be much faster than the first approach.\nReaction of the groups: People observes that its faster than previously.\nTry a second parallelized approach Ask the team to repeat the operation with piles of 5 pieces. They should be lightly faster from the first parallelized approach. Write the time measured on the board.\nReaction of the groups: They observe that it is even faster but their sometime feel a bit more pressure than precedently.\nNow simulate a “Push” production line Ask everybody to flip the coins one by one and after each coin to give it to his next colleague as fast as possible without carrying about the other member of the team. Again, write the global time on the board and the team should have won some more seconds.\nReaction of the groups: We are going even faster but it is like working in a factory. It’s not pleasant, they feel a lot of pressure, some coins are flying out of the tables. You can underline that you loose quality.\nThe “Pull” approach Repeat the same operation but add an instruction: Only pass the coins to your nearby colleague if the space in front of him is free (no coins). Measure the time again and you should have lost few seconds.\nReaction of the groups: the pressure has decreased; a kind of synchronization has appeared and the coins are not flying anymore.\nA kind of … Scrum? Ask the team to deliver 80 points with the minimum effort and in the shortest time using the second parallelized approach (piles of 5 coins).\nAnd Bim! You got it! The first person will prioritize the coins that have a higher value (here only the coins having a value of 5 and 2), and the time measured is generally (not in all cases) the shortest; now you can debrief.\nDebrief Now you can put words on what has been done.\nThe first exercise is comparable to a standard phase approach like in a waterfall project, one person his doing his task for the whole scope and then give the relay to his colleague.\nThe second and the third exercises are parallelized approaches and show how we can save some time here. About the pressure feeling that can appear with the 5 coins pile I Explain that it is similar to short sprint in scrum which is requiring rigor.\nThe 4th exercise is generally the fastest I compare it to the Fordism, this is a Push approach, and I underline the pressure feeling, the creation of bottlenecks and the lost of quality. For those 3 reasons and even if it’s the fastest that’s not what we want to do. That’s a good occasion to remember the Agile manifesto: “Agile processes promote sustainable development…”*.\nIn the 5th exercise people have synchronized their movements (this means that some communication is required, even if here it’s nonverbal). The bottlenecks have been removed and quality is back. This is a pull approach production is based on the demand, approach used in Toyotism.\nAnd finally, the “Scrum”, it’s not a real scrum but it’s giving a good idea of what we want to do, we are prioritizing the coins to deliver the highest value in a shortest time, it’s the perfect moment to introduce the rule of the 80/20**. You can underline again that we are not choosing the fastest approach but the more sustainable one providing the highest value.\n*Agile Manifesto : https://agilemanifesto.org/principles.html\n**Pareto principle: https://en.wikipedia.org/wiki/Pareto_principle\n","date":"Dec 14, 2018","href":"https://blog.talanlabs.com/the-penny-game/","kind":"page","labs":null,"tags":["Agile","gamification","Scrum","workshop"],"title":"The penny game (30min workshop)"},{"category":null,"content":"CONTEXTE Scrum Master depuis maintenant 5 ans, la gamification est au centre des mes réflexions et aujourd’hui je vais vous présenter une première version d’une gamification pour vos équipes.\nFan de jeu vidéo/société et grâce à une idée suggérée de Constantin Guay, j’ai réfléchi à une sorte d’arbre de compétence façon jeu de rôles mais pour une équipe Scrum.\nPOURQUOI ? Il a fallu se poser la question du but premier d’un tel dispositif.\nLa réponse est venue d’elle-même, et je voulais que les équipes continuent de progresser dans leur compréhension de l’agile et de son application au quotidien.\nCONSTRUCTION J’ai donc construit mon arbre de compétence avec pour finalité le delivery du sprint au complet.\nCela passe évidemment par plusieurs étapes et il a fallu commencer par les fondamentaux.\nAvec mon équipe nous nous sommes fixé des règles de vie et elles sont affichées sur le board (Management Visuel 😉), il était naturel qu’une des premières choses à respecter était ces règles de vie d’un côté et de l’autre la présence à l’heure au daily pour bien commencer la journée.\nRien ne vous empêche de votre côté de commencer par des choses déjà respectées et acquises par votre équipe pour faire avancer l’arbre de compétence le plus vite possible.\nAMELIORATION Progressivement j’ai ajouté des étapes propres à mon équipe mais aussi des choses nécessaires à la bonne marche d’une équipe, comme le découpage en tâches en plus fin possible, la collaboration avec d’autres équipes, etc.\nSoyez libre de mettre autant d’étape que nécessaire, qu’elle soit réalisable rapidement.\nJ’avais prévu initialement que la complétion de ces tâches soit faite sur une semaine, mais je me rends compte que cela est trop long pour faire avancer les choses et il est préférable d’avoir des objectifs qui peuvent être rapidement Done comme dans un sprint !!!!\nMOTIVATION Comme pour un sprint, c’est plus motivant quand on avance rapidement, cela donne envie d’aller voir la suite.\nPour encore plus de motivation, j’ai noté des récompenses à gagner. Cela va d’un café, jusqu’au restaurant pour l’équipe. Les récompenses sont toujours de l’équipe, vers l’équipe. On ne cherche pas l’approbation d’un manager ou autres, mais le gain pour l’équipe.\nIl est impératif que l’ensemble de l’équipe réussisse le défi ensemble et non pas juste quelques personnes. Tout le monde est impliqué.\nEvidemment je ne leur dis pas ce qu’ils vont gagner à quel moment mais toutes les récompenses sont affichées afin de leur donner un peu de visibilité.\nSOYEZ FUN ! Une partie plus fun a été ajouté un peu partout dans l’arbre afin de sortir du cadre travail (ex : toute l’équipe doit porter un haut de la même couleur). C’est une manière de souder encore plus l’équipe.\nÀ chaque réalisation notre arbre se colorise et de nouveaux objectifs sont débloqués.\nOn a donc aussi plusieurs choix vers lequel on souhaite s’améliorer et c’est l’équipe entière qui choisit par où elle part.\nMerci à tous pour vos retours, n’hésitez pas à partager et à me contacter pour plus de précisions et discussion ^^.\nMerci tout particulièrement à Benjamin Noël qui sans lui le projet visuel n’aurait pas vu le jour.\n","date":"Dec 14, 2018","href":"https://blog.talanlabs.com/gamifier-votre-quotidien-quand-travailler-peut-devenir-un-jeu/","kind":"page","labs":null,"tags":["Agile","gamification","Scrum","Scrum Master"],"title":"Gamifier votre quotidien - Quand travailler peut devenir un jeu"},{"category":null,"content":"I recently played game named “Lost in the desert” to train on the backlog prioritization. I really like it to bring some tools to the audience, but this game can be so much more powerful when it’s used as well to share the importance of the product vision for a product Owner.\nWhat is Lost in the desert ? For those who don’t know this game I advise you to have a look on google about it, there is a lot of reference and even the US Navy have proposed a solution. Here is a quick summarize : You are playing the role of a plane crash survivor, all the crew is dead and all the passengers are perfectly safe. You are crashed in the middle of Arizona desert, 80km away from the standard flights corridors and 110 km far from the closest town.\nI know… There are some better days\u0026hellip; but don’t worry you have saved 15 items from the burned plane and you need now to prioritize them from the most important to the less important to survive.\nMost of the time, the game is organised in two session, the first one 10 min of individual prioritization, (it’s important to ask everybody to not communicate). And then the second one, 15 minutes of group prioritization, the team should find a consensus and propose one unique list.\nWhat is happening ? First phase All the elements might be really important for you depends of the strategy you choose to survive, so during the first phase each of the participant is defining is own approach and is trying to prioritize the items.\nYou can let the audience do the prioritization without assistance but it’s from my perspective the perfect moment to pause the timer after 5 min and explain what is the MoSCoW method.\nThe players will then use it according to the strategy they have chosen. For those who don’t know what is MoSCoW don’t worry, nothing \u0026ldquo;rocket science\u0026rdquo; here, you can refer to the holy wikipedia page and contact me if you would like some best practice around the tool : https://en.wikipedia.org/wiki/MoSCoW_method.\nSecond phase In this phase we ask the whole team to propose a common prioritisation. Generally by groups not exceeding 5 persons. I prefer creating a challenging environment for the team and create larger groups.\nMy objective here is clearly to underline the difficulties the team will have to get aligned as there will be more different interactions. So I like to let them struggle ; as there are not align on the strategy, there are discussing for each item if it’s really the most important and they are losing a lot of time. If needed, I disturb them from the possibility to agree on a common strategy and share a vision.\nAfter 7 minutes generally the prioritization is not advanced sufficiently to complete it by the end of the allowed time. So I stop the stopwatch to do a 5 minutes debrief (a retro :)).\nBy simply asking to 2 or 3 person which strategy to survive they prefer, everybody realize quickly that they need to be align on the vision.\nThis being done I let the team complete the prioritisation which goes generally super quickly having a shared vision and using MoSCoW.\nDebrief After that comes the debrief and I can deeply insist on the importance for a product owner to have a clear product vision and to share it to the rest of the developpement team. This should be done from the early beginning in the backlog grooming and the sprint planning for exemple, and all along the developpement.\nI would like to remind here the scrum guide on the definition of the increment :”The increment is a step toward a vision or goal”*. That’s why the product vision should not only be shared through the items of product backlog, because without a clear shared vision we cannot collectively reach the following step.\nDifferent ways to drive the game : In English : http://bitsofknowledge.waterloohills.com/communication/agile-games-night-lost-in-the-desert/\nIn French : http://coach-agile.com/serious-game/perdus-dans-le-desert/\nReference : *“The increment is a step toward a vision or goal” : Scrum guide, section Sprint backlog / Increment. https://www.scrumguides.org/scrum-guide.html#artifacts-sprintbacklog\n","date":"Nov 20, 2018","href":"https://blog.talanlabs.com/get-lost-in-the-desert-and-work-on-product-vision/","kind":"page","labs":null,"tags":["Agile","gamification","prioritization","Product Owner","product vision","Scrum","serious game"],"title":"Get lost in the desert and work on product vision"},{"category":null,"content":"Lourde charge est la mission du Product Owner dans Scrum. Elle est compliquée en termes de responsabilités sur le produit à développer et peut entraîner la centralisation d’un certain nombre de décisions.\nSi l’on se réfère au modèles organisationnels classique de type Waterfall ou cycle V, un tel poste n’existe pas. Cependant je vais le comparer ici au chef de projet qui représente une autre forme de centralisation.\nRappel des rôles Dans Scrum, le Product Owner est une personne unique, en charge de manager efficacement le Product Backlog afin d’y traduire sa vision du produit, vision du produit qu’il se doit de construire dans une compréhension fine des besoins métier des clients qu’il représente.\nC’est sur cette base que le PO insuffle sa vision auprès de l’équipe au quotidien. Il se doit pour cela d’avoir des échanges régulier avec ces derniers et être autonome dans la prise de décision sur le produit. Pour avoir ce pouvoir de décision le Product Owner doit être dans une relation de confiance avec le client.\nDans nos approches non agiles, le chef de projet se doit aussi de comprendre les enjeux et besoins métier. Il travaillera sur la base des spécifications fournies (Cahier des charges / URS) par la MOA (maîtrise d’ouvrage) et des échanges qu’il a avec cette même MOA, à la conceptions des spécifications fonctionnelles et techniques (FDS et TDS).\nIls seront par la suite utilisés comme base de référence pour l’engagement contractuel ainsi que la conception du projet sous la responsabilité du chef de projet. Ce contrat et ces documentations servent de garde-fou, c’est le bouclier de notre super héros chef de projet. On a ici une relation purement contractuelle.\nDérives des relations contractuelles Il faut s’avouer que trop souvent les zones d’ombre de ces documentations, qui se veulent exhaustives mais ne le sont jamais totalement, sont exploitées par les chefs de projet en manque de temps ou dans l’incapacité de livrer des solutions qualitatives, pour se protéger dans leurs manquements.\n«\n\u0026lt;En aparté\u0026gt;\n Dit moi David, ce qu’elle veut la cliente là, c’était dans le scope ?  -Non c’est marqué nulle part…\n\u0026lt;/En aparté\u0026gt;\n\u0026lt;Au téléphone\u0026gt;\n Bien sûr madame la cliente, c’est plus pratique une table avec les pieds en dessous oui… Mais ce n’était pas précisé dans le cahier des charges\u0026hellip; Par contre notre commercial peut vous faire une proposition pour changer ça….  \u0026lt;/Au téléphone\u0026gt;\n»Je caricature ici, mais l’on a tous vécu une situation semblable lors de projet waterfall. Cette situation est bien évidemment évitable si le chef de projet entretient une relation basé sur le retour utilisateur avec ses client. Cela dépendra de la façon de travailler de chaque chef de projet ou de son temps disponible à consacrer à ses utilisateurs et inversement. On constate que c’est rarement le cas et l’on fait souvent face à un certain “effet tunnel”.\nLe risque d’un Product Owner distant des utilisateurs Dans certaines applications de Scrum, le Product Owner se retrouvera parfois loin des utilisateurs ou parties prenantes gravitant autour du projet. On voit d’ailleurs régulièrement dans les projets la présence de proxy PO ou assistant PO accentuant cette distance.\nDans ses cas là, où les utilisateurs sont trop distants du projet et non présents aux démos lors des Sprint Review, on crée selon moi des risques sur la conception du produit. On attend du Product Owner d’être un super héro connaissant tout sur le projet et prenant des décisions de façon unilatéral. Ce qui peut être aux utilisateurs naturel et donc non exprimé (ou tout simplement parfois oublié), ne l’est pas universellement et l’on met ici le Product Owner dans la même situation que nos chefs de projets.\nSi l’on revient à l’exemple de notre table, certains pourront potentiellement reprocher au Product Owner de n’avoir pas pensé à préciser que les pieds de la table doivent être en dessous et ce, même si les utilisateurs ou les parties clientes n’ont jamais mentionné ce détail, « parce qu’il en va de soi » et parce qu’elle n’étaient pas présentes lors du développement du produit pour faire les retours appropriés.\nScrum permet de décentraliser l’intelligence de création d’un produit du chef de projet vers l’intelligence collective de l’équipe de développement. Scrum n’apporte cependant pas de solution structurelle à l’importance d’une collaboration étroite entre le Product Owner et les utilisateurs ou le client (tout comme se le doit un chef de projet). C’est donc un point sur lequel il faut être vigilant en tant que Scrum Master.\nA la différence des approches contractuelles où le chef de projet est protégé par le contrat, le PO ne l’est pas, ce qui rend ici la tâche de ce dernier d’autant plus périlleuse de mon point de vue.\nDonner plus de transversalité à la Development Team et la rapprocher des utilisateurs Il est du rôle de l’équipe de développement de supporter et questionner le Product Owner sur ses décisions et elle est la plus à même de le supporter quand aux questions de conceptions. Elle peut être d’autant plus efficace dans son support au Product Owner si elle est en contact avec les utilisateurs, si elle possède une compréhension du contexte business, des notions transverses de marketing produit, d’étude de marché etc… Et donc si elle apporte de l’intelligence collective sur la vision du produit.\nScrum définit la Development Team comme “cross-functional”, et formalise qu’il fait aussi partie de ses tâches d’assister le PO pour le Raffinement du Product Backlog. « Refinement usually consumes no more than 10% of the capacity of the Development Team.» Ne sous estimons nous pas le périmètre de cette interdisciplinarité au seules compétences nécessaire à la production ainsi que le support que doit apporter la dev team au PO ?\nIl me semble primordial que la Scrum Team échangent aussi avec les utilisateurs finaux, il faut travailler à l’élargissement de ses compétences afin d’être le soutien qu\u0026rsquo;elle peut être. Je tiens cependant à préciser qu’il faut être vigilant aux demandes cachées que les utilisateurs peuvent faire auprès de l’équipe de développements et ne pas contourner notre PO.\nIl est central que les utilisateurs et le client soit activement impliqués tout au long de la conception du produit. Dans le cas contraire on risque de reproduire les mêmes problématiques que celles liées aux approches de projet classique et de rendre la mission du Product Owner intenable.\n","date":"Nov 13, 2018","href":"https://blog.talanlabs.com/ne-laissez-pas-votre-product-owner-seul/","kind":"page","labs":null,"tags":["Product Owner","Scrum"],"title":"Ne laissez pas votre Product Owner seul !"},{"category":null,"content":"Le mercredi 17 octobre 2018, Talan Labs accueillait un meetup de l’Asseth dans les locaux de Talan. L’Asseth est une association organisant notamment des meetups, connus comme étant l\u0026#39;un des rassemblements les plus importants autour de la blockchain Ethereum en France.\n Les speakers 50 blockchainers ont fait le déplacement pour profiter de la présence de Philippe Honigman et Nicolas Danjean, fondateurs de la startup Tribute. Tribute a pour but d’aider les organisations au sens large (associations, entreprises, etc.) à développer leur réseau de contributeurs, notamment via des systèmes de récompenses et d’incentives.\n  Tribute    Les sujets Deux speakers pour deux talks. Tout d’abord, une présentation de la notion de token contributif, puis une plongée plus en détails dans ILOT, l’outil d’event sourcing à la base de la plateforme développée par Tribute.\n   Tokens contributifs Les tokens contributifs présentés par Tribute permettent de récompenser les contributeurs d’un projet, mais aussi et surtout ils sont personnalisables en fonction des projets et des organisations qui les utilisent. Ces tokens peuvent représenter des paiements futurs, ou un poids dans les décisions à venir, une part des profits à venir…​\n  Philippe Honigman    ILOT, event sourcing et action semantics ILOT, \u0026#34;I Love Organizing Things\u0026#34;, est un outil de développement qui permet de définir la sémantique des inputs et la nature des outputs. Cette logique a été portée sur Solidity, y compris le builder même de cette logique.\n  Nicolas Danjean    La vidéo des présentations Retrouvez les 2 présentations de la soirée :\n   Tokens contributifs, par Philippe Honigman\n  ILOT, outil de développement basé sur l’event sourcing et l’action semantics, par Nicolas Danjean\n        ","date":"Nov 9, 2018","href":"https://blog.talanlabs.com/meetup-lasseth-recoit-tribute/","kind":"page","labs":["Lab K"],"tags":["tribute","meetup","asseth"],"title":"Meetup - L'Asseth reçoit Tribute"},{"category":null,"content":"Le mardi 9 octobre 2018, l’équipe Pegasys de ConsenSys tenait un meetup sur Ethereum 2.0. Talan Labs était présent et vous fait un retour sur les annonces de la soirée.\n Pourquoi passer à Ethereum 2.0 ? Pour comprendre pourquoi Ethereum 2.0 est nécessaire, il faut se pencher sur les défauts de l’actuel Mainnet et notamment sur sa courbe d’utilisation des blocs.\n  Utilisation des blocs  Depuis 2018, les blocs sont souvent remplis à plus de 85% de leur capacité totale. Les transactions attendant d’être intégrées à un block s’accumulent, le temps d’attente avant validation s’allonge et finalement Ethereum Mainnet arrive à ses limites.\n D’un point de vue plus technique, le problème est lié au fait qu’Ethereum est ‘single threaded’, c’est-à-dire que tous les noeuds participant au réseau doivent traiter toutes les transactions, et que l’état de la Blockchain doit être stocké dans tous les noeuds.\n L’autre problème majeur d’Ethereum, et de nombreuses autres Blockchains, est la production de tonnes de CO2 due au protocole de minage dit ‘Proof of Work’. Les mineurs doivent résoudre un challenge cryptographique difficile demandant une puissance de calcul conséquente, et donc une consommation d’électricité élevée.\n   Ethereum 2.0 qu’est-ce que c’est ? Ethereum 2.0 c’est deux composants principaux ajoutés à Ethereum.\n CASPER FFG CASPER FFG est un protocole de minage de type ‘Proof of Stake’. Les mineurs deviennent des validateurs, et au lieu d’acheter de la puissance de calcul, achètent des Ethers qu’ils bloquent dans un contrat. Les validateurs sont ensuite sélectionnés aléatoirement et amenés à choisir quel bloc ajouter à la Blockchain Ethereum. Si leurs votes sont corrects, ils sont récompensés, sinon ils sont punis et perdent une partie de leurs Ethers bloqués.\n Le challenge qu’implique ce protocol de minage repose dans le choix du prochain validateur, qui doit être aléatoire pour éviter les risques de corruption, si le prochain validateur est connu à l’avance par exemple.\n Par comparaison, dans un protocole ‘Proof of Work’, les mineurs calculent un hash à l’aide d’un nombre qu’ils choisissent au hasard, et répètent cette opération jusqu’à l’obtention d’un hash vérifiant certaines conditions. L’ajout de ce nombre aléatoire assure l’imprévisibilité du prochain mineur.\n En revanche, avec un protocole comme CASPER FFG, le hasard dans la sélection du prochain validateur doit être généré, ce qui est une tâche compliquée.\n  SHARDING Le Sharding consiste à séparer les transactions et l’état de la Blockchain en des milliers de Blockchains indépendantes (des galaxies) qui fonctionnent en parallèle et sont capable de fonctionner ensemble quand nécessaire.\n En pratique, Ethereum 2.0 est composé de 1024 ‘shards’, ce qui permet un débit 1000 fois supérieur à ce qu’Ethereum est capable de produire actuellement. Chaque ‘shard’ possède un nombre garanti de validateurs. Si le nombre de validateurs est insuffisant, la récompense pour choisir les blocs sera divisée pour inciter les validateurs à voter sur plusieurs ‘shards’.\n CASPER FFG et Sharding ont été développés indépendamment mais ont fusionnés en juin 2018 pour former Ethereum 2.0.\n  AUTRES ÉVOLUTIONS Ethereum 2.0 est toujours un projet en phase de R\u0026amp;D et bien que CASPER FFG et Sharding en soient les composantes principales, de nombreux autres changements sont aussi prévus dont eWASM.\n Actuellement, l’EVM (Ethereum Virtual Machine) est l’environnement d’exécution des smart-contracts, lesquels sont écrits en Solidity et compilés en EVM bytecode. L’EVM a été conçue, entre autre, de sorte que le code soit de petite taille, pour que de très nombreux contrats puissent être stockés sur chaque noeud du réseau, et que l’exécution de code soit déterministe, c’est-à-dire que toutes les actions enregistrées dans la Blockchain ne dépendent pas d’éléments extérieurs.\n WASM (Web Assembly) est un standard web pour la construction d’applications de grande performance. eWASM est basé sur WASM, il a été modifié de façon à devenir déterministe, et répondre aux mêmes critères que l’EVM. L’avantage d’eWASM est la possibilité d’écrire des contrats dans des langages plus courants comme le C ou C++, utilisant les performances natives de ces langages.\n Le changement vers eWASM permettra donc d’atteindre de meilleures performances d’exécution, en utilisant des langages de programmation plus courants.\n  ARCHITECTURE L’architecture d’Ethereum 2.0 est divisée en 4 couches.\n  Architecture Ethereum 2.0     Conclusion Ethereum 2.0 sera complètement différent de sa première version. Les évolutions évoquées devront s’échelonner jusqu’à 2021. La Beacon chain sera développée en 2019, suivie des Shard chains en 2020 et enfin de la couche de VM en 2021. Ce projet est réalisé par une communauté diverse, sans autorité décisionnaire, ce qui est à la fois un challenge et une opportunité.\n   ","date":"Oct 26, 2018","href":"https://blog.talanlabs.com/meetup-ethereum-2/","kind":"page","labs":null,"tags":["ethereum","meetup","consensys"],"title":"Meetup ConsenSys : présentation d'Ethereum 2.0"},{"category":null,"content":"Bien que l’ajout de tâches pendant le sprint n’est absolument pas conseillé, dans les faits les équipes y sont régulièrement confrontées. Dans ce cas comment accueillir ces tâches perturbatrices pour le sprint dans les meilleurs conditions ?\nJe connais deux écoles qui se confrontent sur la méthode; garder de la capacité libre dans le sprint, ou remplacer les éléments du sprint par ces nouveaux.\nGarder de la capa pour la prod Garder de la capa pour les incidents de production (ou pour n’importe quel type d’urgence), c’est choisir des éléments de backlog à embarquer dans le sprint tout en réservant une partie “vide” du backlog pour les imprévus.\nDe la place est laissée dans le backlog du sprint\n“On a une vélocité de 20 points par sprint, on prends 15 points d’US et on garde 5 points pour les incidents de prod”.\nEt les incidents de prod arrivants, les 5 points sont effectivement “dépensés” et souvent priorisé au maximum.\nJe suspecte même que certains bugs ou incidents sont estimés… pour rentrer dans cette capacité “tampon”, mais c’est parce que je vois le mal partout sûrement !\nRemplir son sprint Une autre façon de faire est de remplir son backlog de sprint normalement, avec ce que l’équipe pense pouvoir faire.\nQue se passe-t-il alors en cas d’imprévu ou d’incident à réparer d’urgence ?\nLes nouvelles tâches peuvent trouver leur place et leur coût est visible\nC’est effectivement plus compliqué que dans le cas où de la capacité est réservée à l’avance, mais c’est pour la bonne cause.\n L’impact de ces nouvelles tâches est alors visible. Comme la durée du sprint ne peut être étendue, une tractation est nécessaire pour échanger ces nouvelles tâches contre des tâches prévues, pour un total équivalent. On sait alors ce que l’ajout des tâches a coûté en terme de décalage de feature, on peut l’expliquer en revue de sprint. 2. La priorisation des tâches ajoutées peut être discutée. En effet, en voyant que telle feature sera décalée, le PO peut choisir si la nouvelle tâche est vraiment prioritaire ou non. Corriger un bug qui coûte 500 € par jour tant qu’il est là en décalant une feature qui rapporte 1000 € par jour n’a pas de sens, tout comme corriger en urgence un bug touchant 1 utilisateur peut être moins prioritaire que d’ajouter une fonctionnalité améliorant la vie de dizaines de milliers d’autres utilisateurs.  Conclusion Quoi que vous utilisiez ou que vous décidiez d\u0026rsquo;utiliser avec votre équipe, l\u0026rsquo;important est d\u0026rsquo;avoir une bonne visibilité de l\u0026rsquo;impact de ces tâches indispensables mais néanmoins perturbatrices pour le sprint, pour ne pas générer de Shadow-Velocité tout comme je conseillais déjà de visualiser clairement la dette technique.\nEt vous, comment visualisez-vous l’impact des tâches ajoutées au sprint ? Est-ce que les incidents sont ajoutés en priorité haute aveuglément ?\n","date":"Oct 12, 2018","href":"https://blog.talanlabs.com/remplir-son-sprint-ou-garder-de-la-capa-pour-la-prod/","kind":"page","labs":null,"tags":["backlog","Scrum"],"title":"Remplir son sprint ou garder de la capa pour la prod ?"},{"category":null,"content":"Il existe plusieurs façons d’être un bon Scrum Master, et plusieurs façons d’être un \u0026hellip; moins bon Scrum Master. Une de ces moins bonnes postures, je l’ai vue mainte fois, souvent portée par de grands groupes de consultants : c’est la réduction du rôle de Scrum Master à celui de simple gardien du temps, ou chronomètre.\nQu’est-ce que j’entends par Scrum-Master-Chronomètre ? Le Scrum-Master-Chronomètre est la personne qui a le titre de “Scrum Master” (ce qui veut dire qu’il n’y a pas d’autre Scrum Master dans l’équipe et il est même souvent “Certifié Scrum Master©”) mais qui ne s’occupe que de gérer le temps des événements Scrum.\nLe reste du temps, il peut être développeur, Project Manager, PO ou autre, car il a beaucoup de temps du coup.\n⏳ Il m’est même arrivé de voir un Scrum Master ‒pourtant venu d’une des plus grandes sociétés de services (ESN) française‒ interrompre le daily de l’équipe au bout de 15 minutes ! C’est ça, il a empêché l’équipe de continuer à parler.\nEst-ce que c’est vraiment ce que nous attendons d’un Scrum Master ? Il vous tarde sûrement de connaître la réponse à cette importante question, alors lisez la suite \u0026hellip;\nLe Guide Scrum est lu, mais mal compris ? Ce rôle du gardien du temps, le Scrum Master doit bien exister dans le Guide Scrum (2017, la dernière édition à l’heure de cet article), alors cherchons un peu dans la description du rôle du Scrum Master \u0026hellip; C’est parti pour un peu de lecture académique, ça fait du bien de temps en temps, un petit rappel :\n 📖 Le Scrum Master est chargé de promouvoir et supporter Scrum tel que défini dans le Guide Scrum. Les Scrum Masters remplissent leur rôle en aidant tout le monde à comprendre la théorie, les pratiques, les règles et les valeurs de Scrum.\n Le Scrum Master est un leader - serviteur de l\u0026rsquo;équipe Scrum. Le Scrum Master assiste les personnes externes à l\u0026rsquo;équipe Scrum pour identifier quelles sont les interactions bénéfiques avec elle. Le Scrum Master aide tout le monde à adapter leurs interactions avec l’équipe Scrum pour maximiser la valeur créée par cette équipe\nHum \u0026hellip; 🤔 Jusque là, rien de très précis concernant le chronomètre, sauf peut-être : “Les Scrum Masters remplissent leur rôle en aidant tout le monde à comprendre la théorie, les pratiques, les règles et les valeurs de Scrum”\nNous avons là une notion de gardien des règles de Scrum, mais pas spécialement de chronomètre-sur-pattes. Continuons.\n📖 Le Scrum Master au service du Product Owner  Le Scrum Master sert le Product Owner de plusieurs façons, y compris :\n    * S'assurer que les objectifs, le périmètre et le domaine du produit soient compris par tous les membres de l'équipe Scrum de la meilleure façon possible ;  Trouver des techniques pour une gestion efficace du Backlog produit ;\n * Aider l'équipe Scrum à comprendre le besoin de clarté et concision des éléments du Backlog produit ; * Comprendre la planification de produits dans un contexte empirique ; * S'assurer que le Product Owner sait comment organiser le Backlog produit pour maximiser la valeur ; * Comprendre et mettre en œuvre l'agilité ; et, * Faciliter les événements Scrum , en cas de demande ou nécessité.  🤓 Rien en vue concernant le temps.\n📖 Le Scrum Master au service de l’équipe de Développement\n Le Scrum Master sert l’équipe de Développement de plusieurs façons, y compris :\n    * Coacher l'équipe de développement en matière d'auto - organisation et de pluridisciplinarité ; * Aider l'équipe de développement à créer des produits de grande valeur ; * Supprimer les obstacles à la progression de l'équipe de développement ; * Faciliter les événements Scrum, en cas de demande ou nécessité ; et, * Coacher l'équipe de développement dans des environnements organisationnels où Scrum n'est pas encore complètement adopté et compris.  🤓 “Faciliter les évènements” \u0026hellip; On pourrait dire que tenir le chronomètre est une facilitation. Ou pas.\n📖 Le Scrum Master au service de l’Organisation  Ah, l’orga ! Va-t-on enfin y trouver une justification d’être celui-qui-tient-la-montre ?\n Le Scrum Master sert l’organisation de plusieurs façons, y compris :\n   * Accompagner l'organisation dans son adoption de Scrum ; * Planifier les implémentations de Scrum au sein de l'organisation ; * Aider les employés et les parties prenantes à comprendre et adopter Scrum ainsi que le développement empirique de produits ; * Provoquer les changements qui augmentent la productivité de l'équipe Scrum ; et, * Collaborer avec d'autres Scrum Masters pour accroître l'efficacité de l'application de Scrum au sein de l'organisation.  Et \u0026hellip; Non!\nDonc, il semble que le Guide Scrum ne place pas le Scrum Master spécifiquement dans ce rôle de gardien du temps, ou en tout cas, il n’en a pas l’exclusivité.\nCe rôle existe bien La partie “chronomètre” existe bien dans la définition du rôle du Scrum Master. Il aide l’équipe à faire en sorte que les cérémonies ne durent pas trop longtemps et soient le plus efficaces possible :\n✅ Il ne le fait pas à la place de l’équipe\n✅ Il aide l’équipe à le faire\nTrès rapidement, l’équipe doit prendre en charge ce rôle de gardien du temps. Chaque membre peut et devrait endosser ce rôle afin de devenir autonome. Scrum Master compris.\nEn tant que Scrum Master, j’aime rester en retrait assez rapidement dans les évènements, surtout le Daily Scrum, pour observer et rappeler de temps en temps que les discussions peuvent avoir lieu juste après, avec les personnes concernées (en fin d’article vous trouverez un lien vers un article détaillant ma posture au Daily Scrum).\nLe rôle de Scrum-Master-Chronomètre est à partager Bien sûr ! Les activités du Scrum-Master-Chronomètre peuvent et doivent être partagés dans l’équipe. Cela a diverses vertues :\n Le Scrum Master n’est pas vu comme un simple chronomètre  Si le Scrum Master est absent, l’équipe sait faire L’équipe bénéficie de cet apprentissage pour toutes les autres réunions Scrum ou pas Scrum    Est-ce que c’est grave ? Alors est-ce grave si vous êtes un Scrum-Master-Chronomètre ?\nEt bien oui, c’est grave. Dans une équipe Scrum, le Scrum Master à un rôle bien plus important qu’un simple gardien du temps. Le Scrum Master est là pour coacher et soutenir l’équipe.\nLes tâches du Scrum Master ne sont pas simples à définir précisément. On me demande souvent : et donc, tu fais quoi ? Soutenir une équipe, c’est soutenir l’humain et pas deux jours ne se ressemblent, car les missions ne se ressemblent pas, car les humains avec qui nous travaillons sont uniques, ce qui fait d’une équipe un mélange unique au sein d’une organisation tout aussi unique.\nCroire qu’on peut se dire être le Scrum Master d’une équipe (ou de plusieurs !) juste parce qu’on vérifie que les Daily Scrum ne dépassent pas 15 minutes ou qu’on a organisé une rétro, c’est dangereux et ça tue les équipes.\nNe soyez pas ce Scrum Master.\nVoici quelques liens choisis en relation avec ce sujet :\n Ma posture au Daily Scrum, et celle du PO  https://twitter.com/johncutlefish/status/1045823487552573440?s=21 [Vidéo] Faire grandir une application (Rex) Le Scrum Guide, en français    ","date":"Oct 2, 2018","href":"https://blog.talanlabs.com/le-role-convoite-de-scrum-master-chronometre/","kind":"page","labs":null,"tags":null,"title":"Le rôle convoité de Scrum-Master-Chronomètre"},{"category":null,"content":"Salut ! Avant de rentrer dans le vif du sujet, je tiens à préciser que cet article ne va pas traiter le pourquoi faire des interviews utilisateur, mais plutôt sur le coté pratique de l’exercice.\nLe : Comment enregistrer, Transcrire et Utiliser les données récoltées.\nNous nous plaçons dans le contexte d’une interview d’un utilisateur dans le but de récolter des informations/feedbacks/besoins – sur l’utilisation d’un produit/service déjà existant ou à venir– venant d’un utilisateur final de ce dit produit/service.\nCe que je vous propose est ma manière de faire, prenez ce qui vous intéresse et créez votre propre méthode, faite ce qui marche pour vous :)\nCeci étant dit, Go !\nLa Préparation 🗒 Cela peut paraître bateau mais préparez vos interviews !\nJe vous assure qu’une interview mal préparée est une perte de temps pour vous et pour l’interviewé. Cela se traduit par un échange bâclé, des informations qui vous sont inutiles, des questions hors sujets et j’en passe.\nC’est très frustrant de se rendre compte pendant ou après une interview que tout ce qui s’est dit sera inutilisable pour le produit/service que vous designez.\nLa préparation passe par :\n L’organisation de la rencontre – l’heure du rendez-vous, le lieu, l’accès au bâtiment, faut-il une pièce d’identité pour entrer dans le bâtiment ?, etc – ça vous évitera du stress inutile.  La recherche préalable sur la personne que vous rencontrez. Je ne parle pas de stalking hein… Juste savoir son prénom, son métier. Apportez du contexte à votre interview. L’écriture d’un guide d’interview. Cela peut-être le sujet d’un seul article, mais pour résumer : décrivez l’objectif de l’interview, listez les questions démographiques, les questions sur le produit, et votre dernière question pour clore l’échange. Ce guide sera votre meilleur ami pour relancer votre interlocuteur lors d’un coup de mou.    Une fois prêt : Fast-forward ⏩ …\nLe jour J 🎙 Vous êtes avec l’interviewé, vous avez commencé à papoter pour briser la glace et des informations clés commencent à montrer leur bout du nez.\nC’est le moment de lancer votre enregistrement audio. Personnellement, j’utilise simplement mon téléphone avec l’application Dictaphone. Et avec l’arrivée d’iOS 12, l’app évolue, et c’est cool !\n_En passant : N’oubliez pas de demander l’autorisation d’enregistrer _😉\nCertes, les rush ne seront pas dignes d’un enregistrement radio mais, ce n’est pas ce qui nous intéresse.\nCependant pour gagner en qualité vous pouvez également utiliser un micro dédié, mais cela vous demandera un peu plus de préparation et le micro aura moins tendance à se faire oublier. Ce point n’est pas négligeable, votre interlocuteur peut facilement passer en position défensive fasse à vos questions. L’entretien va devenir compliqué à mener\u0026hellip;\nAujourd’hui, les gens sont habitués à voir un téléphone posé sur une table. Lors des premières minutes, l’interviewé lancera des regards interrogatifs vers votre téléphone, mais croyez-moi, votre smartphone deviendra invisible.\nHa et du coup : préparez le coup – ayez de la batterie. Ça serait bête de perdre 1h de rush audio avec des informations capitales 🙃\nN’OUBLIEZ PAS DE FAIRE UNE BACKUP DU RUSH ! Un simple mail à vous-même avec le fichier audio peut vous sauver !\nFast-forward ⏩ …\nLe derush \u0026amp; transcription ⌨ L’interview s’est déroulée sans encombre, vous avez un fichier audio rempli d’infos : excellent ! 👍\nMaintenant, le travail de petite main commence.\nL’outil que j’utilise pour la transcription est oTranscribe.\nIl propose tous les raccourcis clavier dont vous avez besoin lors de l’écoute : Le play/pause, l’avance/retour rapide, la sélection de la vitesse, etc. Ça a accéléré drastiquement mes transcriptions !\nInsérer simplement votre rush audio, et c’est parti :\n1) Première écoute — Vitesse x3 L’écoute en x3 peut être un peu déstabilisante au départ, mais vos oreilles \u0026amp; cerveau vont vite s’habituer. Vous aurez le même niveau d’information attendu qu’une écoute en vitesse réelle, mais en trois fois moins de temps 👌\n Objectifs : Vous remémorez l’interview et son déroulé.  À Extraire : Le plan de l’interview avec les time-codes, les passages qui vous “saute aux oreilles”    2) 2ème Round — Vitesse x2 La deuxième écoute est la plus intéressante. C’est comme quand vous regardez un film pour la deuxième fois. Vous savez ce qu’il va se passer, mais il y a toujours un petit détail, un truc qui vous a échappé.\n Objectifs : Vérifier le plan, détecter les passages que vous avez “raté”.  À Extraire : Des notes avec vos mots sur ce qu’il vient de se dire en faisant pause aux moments clés de l’interview. Comme si vous preniez des notes pendant l’interview, mais avec le temps pour les écrire.    3) L’écoute détaillée — Vitesse x0,2 Çaaaaa vaaaa êtreeee asseeeeeeeeeez leeeeeeeeent. Mais très utile ! N’écoutez pas toute l’interview en x0,2 hein. Allez directement aux times code que vous avez identifié aux deux premières écoutes.\n Objectif : Avoir les informations clé avec les mots de l’utilisateur.  À Extraire : Les citations exactes de l’interviewé. Le verbatim.    Une fois votre transcription terminée nous pouvons passer au tri et à l’utilisation des données !\nPoint d’attention : La transcription brute n’est pas un livrable en tant que tel. Elle peut contenir des informations qui, mal utilisées, peuvent mettre en porte à faux l’interviewé. C’est génial de recueillir des “punchlines”, mais gardez secret le nom de l’auteur.\nLe tri des données 📚 Je trie systématiquement mes retranscriptions dans cinq familles :\nLes Points d’appréciations Tous les points positifs de l’actuel et du futur produit/service que vous allez designer.\nLes “C’est bien que/quand …”.\nParce que oui, il n’y a pas que du mauvais, même sur un produit/service avec zero design. 😊\nLes points à améliorer \u0026amp; gênants Les : ”C’est embêtant …”, “Ce n’est pas pratique …”\nEn gros tous les points de frictions de l’utilisateur : les fameux Pain Points.\nLes Attentes \u0026amp; Recommandations Les : “Ça serait vraiment bien d’avoir … , pour faire … ”\nNotez-les, mais surtout étudiez (a posteriori) le “pourquoi” qui est caché derrière.\nPar exemple : Si un utilisateur vous remonte le besoin d’avoir un bouton d’export de données, au lieu d’ajouter ce bouton, demandez lui pourquoi et réfléchissez aux motivations derrière son souhait.\nLes Faits Marquants Notez les petites histoires ou anecdotes de votre interlocuteur. Elles sont très révélatrices du terrain et de la vie de tous les jours des utilisateurs.\nCes infos sont facilement utilisables pour des empathies map ou encore vos personae.\nEt pour finir, j’extrais :\nLes Citations Marquantes Toutes les citations verbatim (punchlines 👊) qui vous ont fait tiquer lors de l’interview.\nPour m’aider à retranscrire toutes les infos, j’utilise Milanote. Je vous laisse y jeter un coup d’oeil, ça vaut le détour ! 🤩\nL’utilisation des données 🔍 Déjà, toutes les informations que vous avez tirées de votre interview peuvent constituer un livrable, et ça, c’est cool.\nEnsuite, je vous propose deux manières de traduire les data :\nHMW Les “How Might We” sont des questions, qui permettent de cadrer un problème tout en ayant une tournure de phrase qui permet de se concentrer sur la création de solution adaptée au problème. Plus d’info : https://designsprintkit.withgoogle.com/methods/understand/hmw-directions/\nÀ partir des quatre familles, des faits marquants et des citations, j’écris un maximum d’HMW qui me permettent de bien faire ressortir le besoin utilisateur et les problématiques auxquelles il fait face.\nChacune de ces questions peut faire l’objet d’un Design Sprint 🤓\nTop 3 des Features attendues Pendant les interviews, j’essaye également de découvrir le Top 3 des features que l’utilisateur attend. Deux Techniques : soit vous posez directement la question, soit vous notez sur un carnet ce que vous pensez et vérifiez avec votre interlocuteur à la fin de l’interview.\nPour finir : Que ce soit à l’oral ou à l’écrit, partagez toutes les infos que vous avez reçues ! Au(x) PO, à l’équipe de design, aux développeurs et à toutes les personnes qui travaillent sur le produit/service 🙂\nN’hésitez pas également à revenir régulièrement à vos notes pour des petites piqûres de rappel lors de vos rush de créa.\nJ’espère que ça vous aidera à designer de supers produits.\nN’hésitez pas à donner votre avis et à faire des remarques dans les commentaires :)\nPortez-vous bien, Designez bien, et Éclatez vous !\n_Tchou _👋\n_Crédit image : Daniel Mitev, _https://dribbble.com/shots/3710372-Job-Interview\n","date":"Sep 27, 2018","href":"https://blog.talanlabs.com/methode-outils-pour-vos-interviews-utilisateurs/","kind":"page","labs":null,"tags":["design","interviews","ux"],"title":"Méthode \u0026 Outils pour vos interviews utilisateurs"},{"category":null,"content":"Je vous propose ici une piste pour améliorer un peu l’atelier du Squad Health Check, pour aider l’équipe à se concentrer sur les choses qui comptent pour elle, grâce à une technique venue du Management 3.0.\nLa semaine dernière, je vous parlais déjà du Management 3.0 dans un article (en anglais) concernant mon désaccord sur les Kudos.\nCette fois-ci, je vous propose au contraire d’utiliser une technique venue du Moving Motivators, un atelier Management 3.0, pour améliorer un Squad Health Check.\nC’est quoi le Squad Health Check ? Si vous ne connaissez pas encore le Squad Health Check, c’est une exercice présenté par Spotify pour évaluer l’état d’une équipe en partant du principe que les “matrices de maturité” ne sont pas vraiment adaptées et que parler de “maturité” est un peu infantilisant.\nJe ne compte pas trop m’attarder sur le déroulé de l’exercice de base ici, je vous invite à lire l’article explicatif d’Henrik Kniberg : Origine du Squad Health Check.\nC’est quoi le Moving Motivators ? Le Moving Motivators est un exercice proposé par le Management 3.0, qui permet grâce à quelques cartes de mettre des mots sur ce qui nous motive vraiment et à quel point nous sommes affectés par ces “motivateurs”.\nLe but du “jeu” est de classer les motivateurs proposés (ou d’autres) par ordre d’importance du point de vue du participant. Cet ordre peut et devrait être remis en cause. On peut ensuite dire, pour chacun des motivateurs, si nous sommes contents de l’état actuel dans notre travail (par exemple). Cela peut aider à savoir si un changement de poste va influer en pire, ou en mieux, sur nos motivations les plus importantes.\n   Généralement, j’explique à mes interlocuteurs comment “jouer” le jeu en le faisant avec eux, mais aussi en les incitant à le faire seuls, plus tard, afin d’être le plus honnête possible.\n Et si on liait les deux ? En faisant récemment un Squad Health Check avec des équipes, il m’est apparu important, voire indispensable, de savoir ce qui avait le plus d’impact sur l’équipe.\nQue ce soit les “feux rouges”, comme les “feux verts”.\nSavoir quoi faire Je vous propose donc lors d’une prochaine session de Squad Health Check, de rajouter une phase Motivators :\n Après avoir arrangé les cartes comme d’habitude, vous demandez à l’équipe, de déplacer silencieusement les cartes : de la plus importante à la moins importante 2. Comme pour un atelier d’Extrem Quotation, laissez les participants arranger les cartes, les changer, le re-changer, etc. 3. Au bout d’un petit moment, cela devrait se stabiliser. Si quelques cartes ne trouvent pas de place, alors discutez-en rapidement pour trouver un consensus  Cette phase peut être réalisée avant de faire l’exercice normal du Squad Health Check, ou bien après, au choix. De mon expérience, cela importe peu.\nCela permet donc de concentrer les actions les plus pressantes sur les cartes qui ne sont pas “feu vert” les plus importantes pour votre équipe en priorité.\nVous vous rendrez aussi peut-être compte que toutes les cartes importantes pour l’équipe sont “au vert”.\nAnticiper un changement Tout comme avec le Moving Motivators, vous pouvez alors anticiper l’impact d’un changement sur l’équipe. Par exemple, si pour votre l’équipe, le “support” est peu important, alors vous savez que réduire un peu plus ce support pour améliorer autre chose sera moins impactant que, par exemple, leur demander de rogner sur la qualité du code.\nAviez-vous déjà pensé à cette amélioration du Squad Health Check ? Peut-être avez-vous déjà fait ce changement dans l’exercice naturellement ? Si oui, j’aimerai bien avoir votre feedback, et si ce n’est pas le cas, pensez-vous essayer ?\nEst-ce que vous voyez dans cette démarche une problématique ? Ou quelque chose qui irait contre l’idée derrière le Squad Health Check ?\n","date":"Sep 26, 2018","href":"https://blog.talanlabs.com/atelier-squad-health-check-motivators-combo-de-la-mort/","kind":"page","labs":null,"tags":["atelier"],"title":"Atelier : Squad Health Check Motivators combo-de-la-mort !"},{"category":null,"content":"Il y a un vrai débat au sein de la communauté agile sur la pertinence des estimations et principalement de ce qu’il en est fait. Je vous donne ici un premier exemple d’analogie qui pousse à réfléchir à vos estimations et pourquoi pas, à remettre le principe en question.\nRappel : 🤔 que dit le manifeste agile ? Le manifeste agile ne parle pas d’estimation. Il n’en fait même pas mention.\nPar contre, il y a des principes et valeurs qui nous permettent de réfléchir à ce que nous pouvons faire des estimations :\n  La valeur : L’adaptation au changement plus que le suivi d’un plan\n Les principes :  * Accueillez positivement les changements de besoins même tard dans le projet * La simplicité – c’est-à-dire l’art de minimiser la quantité de travail inutile – est essentielle    En effet, d’après ces points, nous sommes incités à accepter le changement et à minimiser le travail inutile. Nous pouvons donc en déduire que pour éviter du travail inutile, nous devrions essayer de ne pas estimer du travail qui ne sera peut-être jamais à faire, car nous aurons adapté notre plan entre temps. Ou alors y passer le moins de temps possible.\nExemple 1 : 🛒 le backlog est une liste de course Lorsque vous partez faire les courses, savez-vous exactement combien de temps vous allez mettre avant d’être de retour chez vous ?\nSi vous connaissez le magasin, vous pouvez imaginer votre plan.\nQue vous le connaissiez ou non, vous pouvez vous baser sur votre historique. C’est-à-dire le temps que vous passez d’habitude à faire vos courses dans ce magasin ou dans un magasin similaire, avec un bon indice de confiance.\nCertains se fichent du temps qu’ils vont mettre, ils y passent la journée. Personnellement ce n’est pas vraiment ma tasse de thé, donc j’essaye de faire au plus efficace.\nImaginez que vous soyez aussi dans ce cas et que vous souhaitiez aller au ciné ensuite. Vous devez absolument estimer le temps de vos courses pour savoir à quelle séance de cinéma vous pouvez donner rendez-vous à vos amis (si vous n’avez pas d’amis, trouvez-en pour le bienfait de l’exercice).\nSe préparer, planifier, estimer Vous décidez donc d’une séance, et trouvez donc que vous avez 1h pour faire vos courses. Vous venez donc de vous imposer une contrainte de temps.\nVous savez ce que vous voulez manger (vos user stories), et pouvez en déduire votre liste de produits à acheter (tâches techniques), c’est votre backlog de sprint, votre scope.\nSi vous connaissez le magasin, votre liste est peut-être ordonnée par rayon (les fruits d’abord par exemple, les grandes surface les mettent souvent à l’entrée pour qu’on ait un sentiment de fraîcheur des produits dès notre arrivée dans le magasin, ce sentiment se propagera alors inconsciemment à tous les rayons).\nL’enfer du magasin (le sprint) Une fois sur place, vous y trouverez une ou plusieurs problématiques, de type pas de place dans le parking, vous perdez 10 min à vous garer ; les rayons ont été changés ; un ou plusieurs produits sont en rupture de stock ou encore le magasin est bondé.\nPar exemple, au fur et à mesure que vous prenez des produits, vous pouvez découvrir un besoin (une envie de poisson par exemple 🐟).\nVotre estimation tombe à l’eau 💦 Vous avez tout pris en compte, sauf … ce que vous n’avez pas pris en compte.\nC’est tout le problème avec les estimations, nous estimons d’après les informations que nous avons au moment où nous estimons. Ces informations se complètent et évoluent dès que nous commençons à agir.Vous devez alors faire un choix : restreindre votre liste (scope) ou donner plus de temps à vos courses (revoir votre estimation pour l’affiner avec plus d’info).\nPlus de monde = plus de partages (dans le mauvais sens) Voyant les problèmes en arrivant, vous auriez aussi pu ajouter de la main d’œuvre en appelant quelqu’un pour s’occuper de la moitié de la liste. Mais attention, l’ajout de personnes n’est pas toujours synonyme de gain de productivité, car vous allez devoir passer du temps à expliquer votre besoin à ce nouvel arrivant.\nSi vous ne partagez pas correctement votre besoin (ne prendre que du bio, éviter telle ou telle marque, etc.), vous risquez de vous retrouver avec des produits d’une qualité qui ne correspond pas à vos critères.\n⚠️ Et rogner sur la qualité, ça, on ne veut pas. N’est-ce pas ? 🤨\nC’est fini ? N’oubliez pas la Definition of Done ! Vous avez estimé le temps que vous souhaitez accorder à vos courses, vous êtes rentré chez vous. Est-ce fini pour autant ? Non, trop souvent nous oublions de compter tout ce qu’il faut faire pour considérer une tâche Done.\nJ’encourage toujours les équipes avec lesquelles je travaille à prendre en compte la Definition of Done dans l’estimation de leurs tâches ou dans la capacité à prendre dans le sprint.\nIci, nous avions oublié le temps qu’il fallait pour ranger les affaires dans les placards, qu’on en profite aussi pour vérifier lesdits placard, et nous débarrasser des produits périmés, par exemple.\nNous devons faire un choix : ranger vite les produits, un peu n’importe comment et ne pas faire de ménage dedans, c’est à dire rogner sur la qualité qui nous importe, ou bien clôturer notre timebox (sprint) sans rien avoir fini et faire l’effort et rater la séance de ciné pour repartir sur une nouvelle timebox nous permettant de finir correctement notre activité.\nEt bien sûr si nous rognons sur la qualité quand même, nous créons de la dette technique. Je vous invite à lire mon article sur l’importance de visualiser cette dette.\n⏩ Compter la dette technique dans la vélocité ou pas ?\nNous verrons dans un prochain article une autre vision de l’estimation, en la comparant à un GPS.\nPhoto by Bernard Hermant on Unsplash\n","date":"Sep 10, 2018","href":"https://blog.talanlabs.com/estimons-lestimation-part-1-la-liste-de-course/","kind":"page","labs":null,"tags":["estimations","Scrum"],"title":"Estimons l’estimation part 1 : la liste de course"},{"category":null,"content":"Avez-vous déjà pensé à utiliser la gamification ou les jeux de société au travail ?\nDans certaines sociétés cela peut être mal vu, pourtant c’est un outil très puissant sur plusieurs aspects. Prêt pour une partie ?\nGrand fan de jeux de société depuis toujours, je me suis vu lors d’une de mes missions jouer tous les midis avec des collègues qui étaient dans l’open space.\n Cela m’a permis de découvrir d’autres collègues avec qui je n’avais pas l’habitude de travailler mais aussi de mettre des prénoms sur des visages.\n Le contact entre nous est devenu plus simple et la demande d’aide pour différents sujets plus facile, tant d’un point de vue technique qu’organisationnel.\nActuellement Scrum Master, j’aime à utiliser des jeux de société au travail pour renforcer la cohésion de l’équipe mais pas que…\nSur votre lieu de travail ?!? Jouer au travail c’est bien, mais pas n’importe comment.\nIl ne faut pas non plus que l’open space ou les salles utilisées deviennent des lieux de récréation bruyantes et incontrôlables (même si c’est fois l’euphorie l’emporte). L’intérêt que j’y vois est sain. Cela permet de découvrir des personnalités en dehors du cadre de travail (même si la plupart du temps nous sommes dans les locaux), d’intégrer et d’unifier plus facilement une personne / une équipe, et enfin de faire une vraie coupure avec le travail (faire une vraie pause quoi).\n Il n’est pas nécessaire de faire cela tous les midis, mais régulièrement dans un premier temps, cela permet à l’équipe de se réunir et de penser vraiment à autre chose. Les personnalités de chacun se dévoilent un peu plus et ça devient un outil pour un Scrum Master pour mieux connaître les personnes avec qui il travaille, et pouvoir ainsi mieux appréhender certaines problématiques au sein de l’équipe.\n Un outil ! Les jeux au travail pour se détendre et en apprendre un peu plus sur votre équipe c’est bien, mais est-ce qu’il serait possible d’utiliser cette énergie pour améliorer le quotidien de votre équipe pendant les sprints ? À mon sens oui. Je n’ai pas encore trouvé la solution pour gamifier les journées de mes équipes mais j’espère trouver une piste bientôt.\nEn attendant, je m’inspire de multiples jeux, et expériences vécues pour faire des icebreaker intéressants pendant mes rétros. Gamifier et varier ces icebreakers peuvent également vous permettre d’en découvrir un peu plus sur votre équipe et comment elle se comporte en groupe dans un cadre un peu hors boulot.\n La dernière fois que j’ai fait un icebreaker où il fallait écrire 3 vérités et un mensonge sur soi, j’ai découvert des choses intéressantes, que je n’aurais sûrement pas découvertes sans ce jeu. Tout le monde a fait l’effort de trouver des choses à dire sur lui-même sans trop rentrer dans le perso. L’exercice a plu et chacun a pu en découvrir un peu plus sur les autres membres de l’équipe.\n Je me dis donc que gamifier dépasse le jeu en lui même, cela devient pour moi un outil pour faire faire des choses ou faire dire des choses à mon équipe tout en s’amusant.\nRien de mieux que d’apprendre en s’amusant !!!\nEn bref La gamification au travail est en soi un bon outil comme on a pu le voir, à la fois pour découvrir d’autres aspect sur vos collaborateurs, et aussi pour unifier une équipe plus facilement.\nJ’encourage fortement la gamification sur tous les aspects au travail, pour avoir une meilleure ambiance de travail, pour lever la tête du quotidien et faire vraiment un break, découvrir d’une manière ludique vos collègues et enfin apprendre en s’amusant, car à mon sens on apprend plus facilement de manière ludique !!!\n","date":"Sep 5, 2018","href":"https://blog.talanlabs.com/la-gamification-au-travail/","kind":"page","labs":null,"tags":["gamification"],"title":"La gamification au travail"},{"category":null,"content":"Vous êtes un développeur, un Lead Dev ou CTO et travaillez sur tout type de projet logiciel ? Comment vérifier la santé du code et s\u0026rsquo;assurer que le produit développé ne soit pas malade ? Quels sont les enjeux et que faire s’il l\u0026rsquo;est ?\nVous êtes agile mais vous n’en voyez toujours pas les bénéfices ? C’est peut-être normal, vous oubliez une partie complémentaire : l’artisanat du code, ou software craftsmanship.\n 📖 Tandis que l’agilité nous aide à nous assurer que nous construisons la bonne chose, les pratiques techniques nous aident à nous assurer que nous construisons la chose bien.\n Comment mener le diagnostic pour connaître la santé du code ? Des outils d’analyse statique de code tels que SonarQube peuvent vous amener des premiers indices. Ceci dit, il n’est pas toujours évident d’interpréter ses rapports.\n Sur le code le plus malade que j’ai pu voir jusqu’à présent (en phase terminale, incurable), SonarQube attribue une note de A, soit la meilleure note. Il s’avère que la note est donnée est calculée à partir d’une estimation : la dette technique du projet comparée à l’estimation du coût pour tout réécrire à partir d’une feuille blanche. Dans mon cas, la complexité de l’application était telle qu’il est, selon SonarQube, plus simple de résorber la dette technique que de tout recommencer. Dans les faits, la complexité introduite n’avait pas lieu d’être, et dès lors, on se rend compte que le calcul de la note peut artificiellement être influencée par la façon d’écrire le code.\n Avec un bon budget, il est également possible de commander un audit de code réalisé par un cabinet externe.\n 👉 La qualité logicielle passe par différentes notions que j’aborde dans un précédent article : Les notions derrière la qualité logicielle, ce qu’il faut savoir\n Quels sont les symptômes ? En général, il n’y a pas besoin d’être le nez dans le code pour savoir s’il est en bonne santé. Ça se sent ! Quelques symptômes :\n Vous n’avez pas assez de tests unitaires ou pas du tout (on parle alors de code legacy) !  Vous avez des tests automatisés de haut niveau instables et lourds à maintenir Vos évolutions sont très coûteuses Des technologies utilisées sont méconnues d’une majeure partie de l’équipe de développement Des régressions font fréquemment leur apparition L’architecture et la logique sont complexes Il existe une dette technique à faire pâlir la Grèce Des outils ont été mis en place pour contourner les problèmes Des frameworks ont élu domicile au cœur du système alors que vous aimeriez en changer     👉 Si vous êtes le responsable produit et travaillez avec une équipe de développement, quelques uns de ces points peuvent vous aider à connaître l’état du produit. Restez proche des équipes de réalisation !\n Alors docteur, c’est grave ? Plusieurs ordonnances vous sont proposées :\n ne rien faire  résorber la dette progressivement repartir d’une page blanche    Ne rien faire Il s’agit en fait d’une fausse solution, car les évolutions seront de plus en plus coûteuses, et les bugs de plus en plus difficiles à corriger, les budgets illimités n’existant pas. Le risque de départ des développeurs n’est pas à prendre à la légère. Avec tout départ, c’est un peu de connaissance du projet et de l’ambiance dans l’équipe qui partent aussi. La règle à appliquer dans ce cas est malgré tout la règle du “Boy scout” : Laisse ton code en meilleur état que tu ne l’as trouvé.\nRésorber la dette progressivement La tâche sera longue et difficile, mais peut s’avérer beaucoup moins coûteuse que de tout recommencer. Il n’en reste pas moins qu’il s’agit d’un sacré challenge qui peuvent en motiver certains, et en décourager d’autres. Selon moi, cela va principalement dépendre de l’environnement de travail et de l’angle d’attaque. Le plus souvent, un pourcentage de production sera consacré à la résolution de la dette. Cela peut également passer par la création de fiches techniques pour refactorer telle ou telle partie du code, même si je préfère en général effectuer le refactoring selon les besoins de l’équipe et les évolutions confiées.\n Exemple : Si je dois corriger un bug ou faire une évolution dans une méthode de 100 lignes, je pourrais en profiter pour ajouter des harnais de tests dans un premier temps, puis simplifier et mieux découper cette méthode pour respecter le principe de responsabilité unique (SRP).\n Repartir d’une feuille blanche La solution préférée des développeurs, mais pas des acheteurs ! Si annoncer à son client que le code est dans un tel état qu’il faut le recommencer est très délicat quand il s’agit de son propre développement, cela peut être plus facile lorsque le développement et la maintenance de l’application sont confiés à une toute autre entreprise.\n 💡 Et, de toute évidence, si on recommence à zéro, on en profite pour repartir sur une stack technique à jour et dans l’état de l’art !\n Cette solution n’est pas sans risque, elle est très engageante. Entreprendre de repartir d’une feuille blanche et s’arrêter au milieu faute de budget est à l’opposé de ce qu’il faut faire. On se retrouverait alors à gérer deux applications en parallèle avec pour chacune sa complexité.\nSi vous êtes tenté de repartir d’une feuille blanche, vous serez probablement amené à imaginer au moins deux façons de faire.\nPremière idée : reconstruire à côté On serait tenté de construire la nouvelle application à côté et de ne la livrer qu’une fois prête, mais il est bien entendu déconseillé de procéder ainsi, surtout si l’application actuelle est déjà en production. Cela serait contraire à l’esprit agile, qui consiste à créer de la valeur et avoir des retours rapides : l’effet tunnel n’est pas à négliger et on n’est pas sûr d’obtenir quelque chose d’acceptable à l’arrivée. Dans le lexique médical, on pourrait comparer cette reconstruction à la grossesse. On ne découvrira qu’au bout de 9 mois à quoi ressemble le bébé, et, espérons le, s’il est viable.\nSeconde idée : reprendre petit à petit des fonctionnalités Je privilégie cette idée, mais attention à la méthode ! Sachant que l’application legacy est amenée à être jetée (normalement), il est indispensable de ne pas en être dépendant.\n 👉 C’est l’inverse qui doit être fait : l’application legacy doit dépendre de la nouvelle, sinon, que se passera-t-il lorsque nous voudrons jeter la première ?\n L’architecture fonctionnelle et technique doit être revue en apprenant des erreurs commises précédemment. Un des impératifs de cette refonte est de garder le nouveau système propre. Il est hors de question de propager la “maladie” du legacy vers cette nouvelle, elle s’y installerait partout à terme.\n 💡 Malheureusement, ce n’est pas toujours possible de faire ainsi, et ce que je préconise dans ce cas est de développer un adaptateur, bien isolé du reste, pour faire office de couche isolante entre ces deux zones. La maladie ne doit pas se propager !\n La reprise des fonctionnalités peut se faire lors d’évolutions ou d’ajouts de fonctionnalités. L’existant va alors appeler le nouveau service au lieu de l’ancien.\n Exemple : Si je dois enrichir la fonctionnalité de recherche d’informations, plutôt que d’adapter le service actuel avec tous ses défauts, je mets en place un nouveau service refait à neuf dans un nouveau composant isolé et le rend disponible pour que l’existant le sollicite (pas de dépendance !).\n Dans le lexique médical, cette solution serait plutôt comparée à l’apprentissage donné par une personne âgée à un enfant, qui prend de plus en plus de responsabilités et déleste la personne âgée.\nLe vaccin, ça existe ? Il existe différentes façon d’éviter que la maladie se déclare. Les symptômes cités doivent être détectés au plus tôt et il faut s’attaquer à plusieurs axes d’amélioration en parallèle. Les boucles de retour doivent être les plus courtes possibles, que ce soit avec le client, l’utilisateur ou par le système de build.\n 👉 Si vous ne l’avez pas encore lu, je vous conseille de lire un de mes précédents articles qui aborde une partie de ce sujet : Livrer de la qualité tout en restant productif\n Les bonnes pratiques de développement doivent mises en place dès le début et être suivies tout au long du développement du produit. Plus on laisse traîner, plus il sera difficile de s’en tirer sans en laisser des plumes.\n L’image d’une construction est souvent prise pour expliquer l’évolution du code. Cependant, ce n’est pas l’image la plus fidèle, je préfère celle du jardinage. Il faut voir le code comme étant un être vivant, qui évolue, d’une manière des fois imprévue, et qu’il faut régulièrement entretenir. Le développeur serait alors un jardinier, qui arrose l’arbre et coupe des branches pour lesquelles il était à l’origine et pour lesquelles il assurait la maintenance.\n L’équipe de développement doit être responsabilisée mais aussi sensibilisée aux problèmes de qualité de code. Les katas, sessions de revue de code sont d’excellents moyens pour y remédier.\n 👉 En effet, en plus de permettre d’améliorer la qualité de l’application à court et à long terme, la revue de code permet de faire grandir chaque personne, que ce soit le relecteur ou le relu. J’y ai d’ailleurs consacré tout un article, alors n’hésitez pas à le lire : Travailler efficacement en équipe avec la revue de code\n La sensibilisation à la qualité passe par tout le monde !\n 📖 “In order to increase business agility, and have a good return on investment, keeping code quality high is paramount” (Sandro Mancuso, dans son livre The Software Craftsmanship, Professionalism, Pragmatism, Pride)\n Et vous ? Avez-vous rencontré des symptômes cités ? Envisagez-vous une résorption de la dette ou un refonte intégrale de l’application ? Comment vous-y prenez-vous ? Je serai ravi d’avoir vos retours en commentaire !\nDans un prochain article, j\u0026rsquo;essaierai d’aborder des méthodes pour minimiser le risque d’échec des projets et commencer un projet sur de bonnes bases.\n","date":"Aug 30, 2018","href":"https://blog.talanlabs.com/allo-docteur-mon-code-est-malade/","kind":"page","labs":null,"tags":["Qualité","Santé du code","Software Craftsmanship"],"title":"Allo docteur, je crois que mon code est malade"},{"category":null,"content":"Durant la période d\u0026rsquo;été, l\u0026rsquo;activité est souvent moindre, on peut alors passer du temps sur les sujets disons \u0026ldquo;moins prioritaires\u0026rdquo;, voir rattraper son retard. Au mois d\u0026rsquo;Avril dernier, j\u0026rsquo;ai assisté à une présentation à Devoxx France qui m\u0026rsquo;avait bien plu, animée par Ane Diaz de Tuesta et Amelie Benoit : \u0026ldquo;Le sketchnote ou la prise de note visuelle et graphique\u0026rdquo;. J\u0026rsquo;ai donc décidé de tester (enfin) cette pratique.\n#FacilitationGraphique Le sketchnote, c\u0026rsquo;est une prise de notes visuelle sous forme de dessins, de schémas, de graphismes et de texte. C\u0026rsquo;est une manière efficace de s’approprier une information. Elle se fait sur une grande feuille blanche et permet de faire le lien entre les idées et d\u0026rsquo;augmenter la visibilité de l\u0026rsquo;information, de mieux la comprendre, d\u0026rsquo;être plus précis et plus synthétique.\nL\u0026rsquo;avantage que j\u0026rsquo;y vois, c\u0026rsquo;est le résultat :\n avoir un support facilement partagable  c\u0026rsquo;est fun et très ludique    Je me suis lancé sans être un grand dessinateur et ça donne cela : Combiner texte et pictogramme n\u0026rsquo;est pas trop difficile mais avec un peu d\u0026rsquo;exercice, on obtient vite des résultats très sympas ;)\nUn peu de lecture  lafoliedusketchnote.com  sketchnotes-facile.com rohdesign.com sketchnote-love.com/ pinterest et autres réseaux sociaux awsgeek     ","date":"Aug 27, 2018","href":"https://blog.talanlabs.com/jai-teste-le-sketchnote/","kind":"page","labs":null,"tags":["sketchnote","ux"],"title":"J'ai testé le sketchnote !"},{"category":null,"content":"J\u0026rsquo;imagine que pour la plupart d’entre vous, le terme \u0026ldquo;qualité logicielle\u0026rdquo; est flou, mais saviez-vous qu’il existe une norme internationale qui le définit ?\nIl s’agit de la norme ISO/IEC 25010:2011. Étudions-la ensemble.\nLes notions principales autour de la qualité logicielle La qualité logicielle passe par différentes notions, dont vous avez sûrement déjà entendu parler, au moins pour certaines. Elles sont au nombre de huit, elles-mêmes composées de sous-notions :\n la capacité fonctionnelle  les performances la compatibilité l’utilisabilité la robustesse la sécurité la maintenabilité la portabilité    Cas d’exemple Pour illustrer l’intégralité des notions, plaçons nous dans le cas d’une application de vente en ligne (e-commerce) comme vous en utilisez déjà très certainement. Chaque notion sera suivie d’un exemple correspondant à ce cas.\nLa capacité fonctionnelle La complétude fonctionnelle (Functional completeness) L’ensemble des fonctions couvrent toutes les tâches spécifiées et les objectifs de l’utilisateur\n L’utilisateur doit pouvoir réaliser toutes les actions attendues, de la recherche à l’achat final, en passant par l’ajout dans un panier.\n L’exactitude fonctionnelle (Functional correctness) Le produit ou système met à disposition les résultats corrects avec un degré de précision nécessaire\n Si je cherche des livres des années 80, j’ai bien la liste des livres des années 80.\n La pertinence fonctionnelle (Functional appropriateness) Les fonctions facilitent l’accomplissement de tâches et d’objectifs précis\n L’ajout d’un article dans le panier se fait sur les pages attendues, l’utilisateur se voit proposer des articles souvent achetés en même temps et l’enchaînement des actions fait tout son sens.\n Les performances Le comportement temporel (Time behaviour) Les temps de réponse, de traitement et de débits d’un produit ou d’un système, lors de l’exécution de ses fonctions, satisfont aux exigences\n La recherche de produits doit afficher les résultats en moins d’une seconde.\n L’utilisation des ressources (Resource utilization) Les quantités et types de ressources utilisés par un produit ou un système, dans l’exercice de ses fonctions, satisfont aux exigences\n Je souhaite qu’un cluster seul puisse supporter une charge de 1 000 utilisateurs en simultané.\n La capacité (Capacity) Les limites maximales d’un produit ou d’un paramètre du système répondent aux exigences\n Je souhaite que mon système utilise au maximum 50Go de données par mois.\n La compatibilité La co-existence Un produit peut exécuter ses fonctions de manière efficace tout en partageant un environnement et des ressources en communs avec d’autres produits, sans impact négatif sur tout autre produit\n L’application de e-commerce doit co-exister avec l’application de back-office permettant de faire les comptes de l’entreprise qui tourne sur le même serveur.\n L’interopérabilité (Interoperability) Deux ou plus de systèmes, produits ou composants peuvent échanger des informations et utiliser les informations échangées\n L’application de e-commerce doit dispatcher la commande aux services de livraison.\n L’utilisabilité L’aptitude à la reconnaissance (Appropriateness recognizability) Les utilisateurs peuvent reconnaître qu’un produit ou un système est adapté à leurs besoins\n Les utilisateur utilisent l’application pour acheter des articles dont ils ont besoin.\n La facilité d’apprentissage (Learnability) Un produit ou un système peut être utilisé par des utilisateurs spécifiques pour atteindre des objectifs précis d’apprentissage de l’utilisation du produit ou du système avec efficacité, efficience, absence de risque et de satisfaction dans un contexte d’utilisation spécifique\n L’utilisateur est accompagné tout le long de son parcours sur l’ensemble du site, il comprend comment aller jusqu’à l’achat.\n L’opérabilité (Operability) Un produit ou un système a des attributs qui le rend facile à utiliser et à contrôler\n Je peux contrôler l’intérieur de mon panier, retirer des articles ajoutés par erreur, visualiser le prix final après éventuelles réductions.\n La protection contre les erreurs utilisateur (User error protection) Un produit ou un système est protégé contre les erreurs issues des utilisateurs\n Si l’utilisateur saisit une chaîne de caractères invalides, l’application doit le détecter et retourner un message d’erreur.\n L’esthétique de l’interface utilisateur (User interface aesthetics) Une interface utilisateur permet une interaction agréable et satisfaisante pour l’utilisateur\n Les résultats des tests UX/UI montrent que les informations importantes pour l’utilisateur de manière claire et agréable.\n L’accessibilité (Accessibility) Un produit ou un système peut être utilisé par des personnes ayant le plus grand nombre de caractéristiques et de capacités pour atteindre un objectif spécifié dans un contexte d\u0026rsquo;utilisation spécifique\n Les résultats des tests UX/UI montrent que l’application est suffisamment simple pour pouvoir être utilisée par des personnes de plus de 60 ans.\n La robustesse La maturité (Maturity) Un système, un produit ou un composant répond aux besoins de fiabilité en fonctionnement normal\n L’application est stable et opérationnelle.\n La disponibilité (Availability) Un système, un produit ou un composant est opérationnel et accessible au besoin\n L’application est accessible en ligne 24/24, 7/7, à un taux de disponibilité de 99,9%.\n La tolérance aux pannes (Fault tolerance) Un système, un produit ou un composant fonctionne comme prévu malgré la présence de pannes matérielles ou logicielles\n Si un serveur tombe, un autre prend le relai en moins d’une seconde. Les erreurs logicielles ne font pas tomber le matériel.\n La récupérabilité (Recoverability) En cas d\u0026rsquo;interruption ou de défaillance, un produit ou un système peut récupérer les données directement affectées et rétablir l\u0026rsquo;état souhaité du système\n Le serveur ayant pris le relai a gardé les articles dans les paniers de tous les utilisateurs actuellement connectés.\n La sécurité La confidentialité (Confidentiality) Un produit ou un système garantit que les données ne sont accessibles qu\u0026rsquo;aux personnes autorisées à y avoir accès\n Le gérant de l’application a des accès à des données que les autres utilisateurs ne peuvent pas obtenir de quelque façon que ce soit.\n L’intégrité (Integrity) Un système, un produit ou un composant empêche l\u0026rsquo;accès non autorisé à des programmes informatiques ou des données, ou la modification de ceux-ci\n Seuls les accès via l’application de e-commerce peuvent modifier le contenu du panier des utilisateurs.\n La non-répudiation (Non-repudiation) Des actions ou des événements peuvent être prouvés avoir eu lieu, de sorte que les événements ou les actions ne peuvent être répudiées plus tard\n La validation de l’achat et du paiement ne peuvent être contestés.\n La responsabilité (Accountability) Les actions d\u0026rsquo;une entité peuvent être tracées à l\u0026rsquo;entité de façon unique\n Chaque action sur l’interface contient l’identité de son auteur.\n L’authenticité (Authenticity) Preuve de l\u0026rsquo;identité d\u0026rsquo;un sujet ou d\u0026rsquo;une ressource\n L’application garanti que c’est bien cet utilisateur qui a passé la commande.\n La maintenabilité La modularité (Modularity) Un système ou un programme informatique est composé de composants distincts, de sorte qu\u0026rsquo;un changement sur un composant a un impact minimal sur les autres composants\n L’ajout d’une nouvelle brique d’analyses des tendances des achats n’impose pas la refonte totale de l’application.\n La réutilisabilité (Reusability) Un composant ou une brique peut être utilisé dans plusieurs systèmes ou dans la création d\u0026rsquo;autres composants ou briques\n Le module de choix de date peut être utilisé partout, aussi bien dans le profil de l’utilisateur pour sa date de naissance que pour choisir sa date de livraison à la fin de son panier.\n L’analysabilité (Analysability) Efficacité avec lequel il est possible d\u0026rsquo;évaluer l\u0026rsquo;impact sur un produit ou un système d\u0026rsquo;une modification envisagée à une ou plusieurs de ses parties, ou de diagnostiquer un produit pour détecter des défaillances ou des causes de défaillance, ou identifier des parties à modifier\n Si je veux modifier la fonctionnalité d’ajout d’un article dans le panier, je sais exactement tout ce que cela impacte. Les erreurs intervenues lors de son utilisation sont consultables dans un journal qui me permet d’y remédier.\n La possibilité de modification (Modifiability) Un produit ou un système peut être modifié efficacement sans introduire des défauts ou dégrader la qualité du produit existant\n Le code est suffisamment bien construit et toute évolution n’entraîne pas de régression dans les fonctionnalités existantes.\n La testabilité (Testability) Efficacité et efficience avec lequel des critères de test peuvent être établis pour un système, un produit ou un composant et des tests peuvent être effectués pour déterminer si ces critères ont été respectés\n Je dispose de différentes façons pour tester efficacement le code, que ce soit avec des tests unitaires, d’intégration, de bout en bout ou de performance.\n La portabilité L’adaptabilité (Adaptability) Un produit ou un système peut être adapté de manière efficace et efficiente à des matériels, logiciels ou autres environnements opérationnels ou d\u0026rsquo;utilisation différents ou en constante évolution\n Le site web peut être utilisé depuis un smartphone, peu importe son OS et sa résolution.\n La facilité d’installation (Installability) Efficacité et efficience avec lequel un produit ou un système peut être installé et/ou désinstallé avec succès dans un environnement spécifié\n L’application peut être déployée sur une machine peu importe son OS ou sa configuration en moins de quelques minutes, voire secondes.\n La remplaçabilité (Replaceability) Un logiciel ou un produit peut en remplacer un autre spécifiquement pour le même usage dans le même environnement\n Je dois pouvoir remplacer la sous application d’achat par une autre conçue dans une toute autre technologie sans grands impacts sur le reste de l\u0026rsquo;écosystème.\n Bilan Ces huit notions permettent d’y voir un peu plus clair par rapport aux attentes de qualité logicielle. Lesquelles sont les plus importantes pour vous ? Et lesquelles appliquez-vous ?\nSources Norme ISO/IEC 25010:2011\n","date":"Aug 23, 2018","href":"https://blog.talanlabs.com/notions-derriere-la-qualite-logicielle/","kind":"page","labs":null,"tags":["Qualité","Software Craftsmanship"],"title":"Les notions derrière la qualité logicielle, ce qu’il faut savoir"},{"category":null,"content":"Vous vous êtes peut-être rendu compte que les Géants \u0026ldquo;Cloud Providers\u0026rdquo; se sont posés au mois de Juin 2018 à Paris pour leurs grandes messes annuelles. Les objectifs sont multiples lors de ce type d\u0026rsquo;évènement et se ressemblent énormément pour Google et Amazon:\n faire connaître les dernières fonctionnalités ou les derniers services de leurs plateformes pouvoir échanger avec d’autres clients sur les usages et les bonnes pratiques de chacun.  Google Cloud Summit Le 5 Juin 2018, Porte de Versailles, Google accueillait l\u0026rsquo;IT de manière générale. A but commercial (évidement), nous avons exploré de nouvelles idées et les dernières technologies pendant un événement qui réunit clients, partenaires, développeurs, décideurs IT et ingénieurs de Google pour construire l\u0026rsquo;avenir du cloud. Google a investi 30 milliards de dollars ces dernières années sur sa plateforme Cloud pour devenir leader.\nAu fil de nombreuses conférences ayant comme intervenants des acteurs de poids du marché français et mondial, comme Total, Airbus, Dailymotion, BNP Paribas, Natixis, La Redoute, Google a ré-exposé sa stratégie d’implantation et de croissance sur un marché français et mondial du cloud qui reste encore timide, avec seulement 5% de taux de pénétration.\nIl ne fallait pas se tromper, ce n\u0026rsquo;était pas le show dédié à kubernetes (KubeCon) et nous n\u0026rsquo;avons pas eu le droit à la démonstration de \u0026ldquo;OK Google, déploie-moi kubernetes avec 8 noeux\u0026rdquo; (voir). Kubernetes n’est plus piloté par Google mais a rejoint la CNCF (Cloud Native Computing Foundation). La thématique la plus récurrente à mon sens était le Machine Learning (ML). De nombreux speakers, nous ont fait des retours d\u0026rsquo;expérience de comment ils traitent efficacement leurs données pour créer des produits plus intelligents, augmenter leurs productivités et réduisent leurs coûts. Le Machine Learning semble un acquis tellement Google semble déployer des outils pour faciliter sa prise en main, son adoption et son utilisation. C’est le cas notamment avec AutoML \u0026ldquo;Vision\u0026rdquo;. La célèbre compagnie de Mountain View est également très fière de son projet \u0026ldquo;Open-Source\u0026rdquo; TensorFlow, une solution d’apprentissage automatique avec plus de 100K étoiles sur github\u0026hellip;\nIl y a eu plus de 20 sessions visionnaires, stratégiques, techniques et de RetExp autour de la Google Cloud Platform pour :\n Moderniser son infrastructure Créer de l\u0026rsquo;Intelligence à partir de ses données Accélérer les développements d\u0026rsquo;applications (CI/CD) Transformer la collaboration entre des équipes (G Suite)  L’écosystème et les partenaires Google Cloud sont clairement identifiés : pas moins de 30 sponsors étaient présents lors de cet évènement, chacun présentant leurs compétences et des démos technologiques.\nAWS Summit Le 19 juin au Palais des Congrès, AWS accueillait 4500 participants pour son show parisien avec sa punchline : \u0026ldquo;il n\u0026rsquo;est plus question de quand mais comment aborder la transition numérique ? Vous faites partie de ceux qui ont décidé d\u0026rsquo;agir ?\u0026rdquo;\n80% des entreprises du CAC40 utilisent AWS aujourd’hui, en production ou uniquement pour des tests. Est-ce la raison pour laquelle AWS propose désormais une offre 100% hébergé en France ?\nRetrouvez la Keynote d\u0026rsquo;ouverture de Dr. Werner Vogels, CTO, Amazon.com :\n[embed]https://www.youtube.com/watch?v=h8tG1hwlsvI[/embed]\nLe Machine Learning à l\u0026rsquo;honneur L\u0026rsquo;offre Amazon est bien fourni en terme de machine learning. AWS propose ainsi une stack complète pour le machine learning, avec les frameworks les plus connus sur le marché (Caffe2, CNTK, Pytorch, TensorFlow, Keras, Gluon), et Amazon Sagemaker comme plateforme centrale. Autour de ce socle, AWS a ajouté un certain nombre de services :\n Rekognition (image et video) Polly (text to speech) Transcribe (speech to text, support du 16KHz et 8Khz) Translate (12 correspondances de langages) Comprehend (analyse des sentiments, entités, langues, sujets, chiffres dans un texte) Lex (apps de conversation)  Il faut passer à du machine learning en temps réel selon Werner Vogels, et pouvoir exploiter les données d’hier et d’aujourd’hui pour prédire ce qui va se passer demain.\nBases de données Vogels a enfin mis l’accent sur le récent lancement de la version Serverless d’Aurora qui permet à une application composite de s’appuyer sur des services de base de données à la demande démarrable et arrêtable à la volée. Aurora Serverless vise à satisfaire les besoins des applications cycliques ou devant faire face à des pics de charge imprévisibles.\nDes grands comptes convertis au cloud Raphaël Viard, CTO de la SNCF est monté sur scène en digne représentant des grandes entreprises françaises. D’ici 2020, la SNCF va migrer 60% de son parc applicatif vers le Cloud. Contentsquare (plateforme SAAS d’analyse du comportement) et Euronext, (principale place boursière de la zone euro) ont également servi d\u0026rsquo;ambassadeur de marque d\u0026rsquo;AWS.\nAlors ?? Cloud ou pas Cloud ?\n","date":"Aug 20, 2018","href":"https://blog.talanlabs.com/juin-mois-futur-du-cloud/","kind":"page","labs":null,"tags":["Cloud"],"title":"Juin, le mois pour construire le futur du Cloud"},{"category":null,"content":"Ce mardi 26 Juin se tenait à Paris, au pied de la Tour Eiffel, la troisième édition du Paris Container Day (https://paris-container-day.fr). Cette conférence avait pour but de partager une vision de l’écosystème des conteneurs d’aujourd’hui et de demain. Cette année, le thème de la conférence était « Vivre avec l’orchestration ». La question aujourd’hui n’est plus d’utiliser des conteneurs et un orchestrateur, le défi est maintenant de les déployer à grande échelle et d’adapter nos pratiques pour vivre avec l’orchestration.\nCette année, le PCD (Paris Container Day) a permis aux 450 participants de suivre des conférences passionnantes réparties selon 3 niveaux de connaissances/expertises : Débutant, intermédiaire et avancé. Autant dire que faire son choix était assez compliqué.\nLa keynote surprise ! La première demi-heure de cette journée qui s’annonçait chargée a été surprenante dans le bon sens du terme. Un orchestre à corde est monté sur scène. Vivre avec l\u0026rsquo;orchestration. Quelle meilleure façon d\u0026rsquo;expliquer un concept qu\u0026rsquo;avec un exemple concret en images et en musique ?\nLes comparatifs ont été nombreux entre les 2 situations. On a parlé du rôle du compositeur et de l’orchestrateur dans l’organisation, de la façon dont on travaille en groupe ou individuellement, de son empathie et de savoir reconnaître quand le groupe n’a pas besoin d’un maître d’orchestre… La matinée commençait bien sur les airs de musique classique…\nQuand la sécurité parle de conteneurs … C’est Liz Rice (@lizrice) qui a ouvert le bal des « conférences techniques ». Cette ingénieure de chez AquaSecurity nous fait le constat que les acteurs de l’univers des conteneurs sont préoccupés par la sécurité. Le monde a changé, la durée de vie moyenne d’un conteneur est de 2,5 jours, on est loin de nos applications « legacy » livrées quelques fois par an… En tant qu’acteur emblématique de la sécurité dans les écosystèmes Docker, Liz Rice nous a rassurés (pour ceux qui auraient encore des doutes). Si on veut appliquer un patch une image pour quelques raisons que ce soit, la seule façon de faire est de relancer le pipeline de CI/CD ! Evidemment, il a été abordé les thématiques comme les images policies intégrés dans les pipelines, les contrôles des configurations de la sécurité des serveurs et la protection au « runtine » des conteneurs….\nClairement, aujourd’hui, (peut-être) pour rassurer l’audience, Liz Rice a conclu par les avantages en terme de sécurité de l’utilisation des clouds natives :\n Décomposition et segmentation des problèmes par l’ajout de couches de défense Le déploiement continue par la fréquence de renouvellement (shorter attack windows) Et les bonnes pratiques de la communauté ! (Vérifiez les vulnérabilités de vos conteneurs -\u0026gt; https://github.com/aquasecurity/microscanner)  Le ML avec k8s, c’est possible ! David Aronchick (@aronchick) est venu parler de Machine Learning (ML). Ce product manager a travaillé pour quelques projets sympas Chez Google (Google container engine, kubernetes, …). Aujourd’hui, il est le co-fondateur de KubeFlow. Ce projet a pour vocation de faciliter le déploiement des écosystèmes de ML, qui sont à ce jour assez complexes. Vous expérimentez TensorFlow ? Vous avez un cluster kubernetes ?\nCe projet est pour vous…\nKubernetes ? Aujourd’hui Joe Beda disait en 2014 que « tout chez Google tourne dans des conteneurs ». Il est (entre autres) à l’origine du premier « commit » sur le projet open-source d’orchestration de conteneur nommé Kubernetes. C’est une version « hybride » du célèbre Google Borg, le système de conteneur de Google. On sait aujourd’hui que Google démarre deux milliards de conteneurs par semaine.\nKubernetes (k8s) avance très vite, pour être up-to-date, c’est assez compliqué (publication de nombreux patchs de sécurité, de feature). Au moment où j’écris cet article, je me demande si je dois installer la v1.10 ou la v1.11-rc (release candidate). On peut dire que le projet est super vivant. Mais ne faut-il pas regarder la concurrence ? avec DC/OS ou Nomad ?\nFiabilité et complexité opérationnelles Alexandre Beslic (@abronan), un ancien de la Docker Core Engine Team est venu nous présenté une vision high-level de sa vision des orchestrateurs. Il a posé une question intéressante : « Utilise-t-on le produit adapté à nos besoins ? ». Kubernetes par exemple permet de gérer jusqu’à 10K nœuds, mais qui a besoin de gérer un parc aussi grand ? (hormis les grandes entreprises). Il y a-t-il aujourd’hui des solutions pour de petits clusters entre 1 et 50 nœuds ? Swarm et Nomad peut-être ? Alexandre Beslic a fait une comparaison de l’architecture technique de ces orchestrateurs pour mettre en évidence les algorithmes de gestion de la persistance de l’état des systèmes. Nous aurions pu nous croire sur une introduction autour de la BlockChain, mais non… On a parlé de « systèmes centralisés » vs « systèmes décentralisés », de consensus et de fortes consistances. Sa présentation intéressante nous a emmené vers son projet (mantissa) ou comment refondre le système actuel et le remplacer par un système basé sur le concept de « consistance éventuel » (Causal Consistency), pour ne plus assurer l’ordre exact mais seulement la présence de l’information… etcd (la base de données distribuées utilisée avec k8s) n’a plus qu’à bien se tenir…\n Strong consistency : strongest guarantee Causal (+) consistency : strongest guarantee below linearizability Strong eventual consistency : updated independently without coordination Eventual consistency : weak consistency model  Quelques projets utilisent déjà le CRDT (Conflit-free Replicated Data Type _aka _Strong Eventual Consitency) comme CosmoDB (Microsoft), AntidoteDB, Bolt-on (Cassandra), \u0026hellip;\nLes tests E2E chez Meetic Nous aurions pu débattre sur la pyramide des tests, sur le fait d’avoir plus de tests unitaires, que de tests de comportement et que de tests end-to-end (E2E), mais ce n’était pas le sujet… Sébastien Le Gall et Sébastien Lavallée nous ont présenté comment ils ont automatisé l’exécution de leurs tests end-to-end (E2E) avec k8s dans un monde distribué avec des caches, des microservices, des brokers\u0026hellip; Evidemment, les stacks sont à reconstruire intégralement à la volée avec leur orchestrateur mais le plus intéressant a été la gestion multiple des namespaces (un par utilisateur pour la scalabilité et le parallélisme). Intéressant, dans un système distribué, il y a forcément des composants interdépendant (un process peut attendre une base de données…) et pour gagner du temps, quand on lance des batteries de tests, k8s offre des options de synchronisation au démarrage :\n init-container : mise en attente d’un conteneur pods life-cycle : synchro  Google Container Tools David Gageot de Google Cloud a commencé sa session par « Qui aime reconstruire son image docker, la tagger et la pousser dans k8s ? puis recommencer indéfiniment ? personne… ». Il n’avait pas faux. Il nous a présenté quelques outils pour développer efficacement dans un monde de conteneurs grâce aux projets Google Container Tools (https://github.com/GoogleContainerTools)\n Skaffold : un outil de skaffolding en ligne de commande pour faciliter la vie des dev k8s Distroless : des images de base pour construire vos images plus légères et plus sécurisées (parfois vraiment mieux conçu que les versions \u0026ldquo;alpine\u0026rdquo; des produits de base) Bazel : pour construire Docker sans Docker Kaniko : pour construire du « docker » sans avoir accès à la socket docker (« merci ma CI ») Petit tuyau de David : pensez à ajouter le répertoire .git dans votre fichier .dockerignore (et tous les fichiers ou répertoires inutiles au build), cela améliore les performances du build docker qui envoie l’ensemble du répertoire au démon docker pour traitement…  Conclusion de la journée Ce fut une journée bien chargée. Mais ce qui m’aura le plus fait plaisir, c\u0026rsquo;est les retours d’expériences sur Nomad, l’orchestrateur de déploiement made by HashiCorp (aussi connu pour sa galaxie de produit : Terraform, Vault, \u0026hellip;). Il est compatible avec tous les grands cloud providers : AWS, GCP, Azure, Digital Ocean. Original, il ne se limite pas seulement aux conteneurs, il gère aussi des applications standalones. Nomad est également capable d’optimiser l’utilisation serveur afin d’exploiter toute la capacité disponible plutôt que de créer plus d’instances. La question est : « aura-t-il une chance de résister face au mastodonte de Google ? »\n","date":"Aug 8, 2018","href":"https://blog.talanlabs.com/pariscontainerday2018/","kind":"page","labs":null,"tags":["container","conteneur","Kubernetes"],"title":"Paris Container Day 2018"},{"category":null,"content":"Safe Map (source : https://www.scaledagileframework.com)\nSAFe a été créé par Dean Leffingwell en 2011 afin d\u0026rsquo;accompagner les entreprises dans le déploiement de l\u0026rsquo;agilité à l\u0026rsquo;échelle, à travers un framework structuré. L’agilité à l’échelle consiste à encadrer par une méthode la multiplication en volume des principes Agiles, ceux de SCRUM notamment.\nPour développer ce framework, son auteur s\u0026rsquo;est appuyé sur l\u0026rsquo;ensemble des bonnes pratiques issues du lean, de la culture Agile et des REX des transformations d\u0026rsquo;entreprises.\nPremier contact, première impression La documentation de SAFe est riche, librement mise à disposition à travers le site web du Framework. Pour faciliter la navigation et la découverte de ce modèle, tout est accessible via une carte. Cette carte représente l\u0026rsquo;ensemble du système mais révèle aussi toute la complexité de SAFe.\nIl est très agréable de disposer d’une carte pour contenir l’ensemble du framework. Cependant ce premier contact me laisse une impression mitigée car l\u0026rsquo;objectif mis en avant par son auteur est louable mais lorsque je regarde la carte, ma première pensée est \u0026ldquo;Des processus et des outils plus que les individus et leurs interactions\u0026rdquo;. De plus on n’a pas une carte, mais trois, en fonction de la couverture de SAFe que l’on souhaite appliquer.\n\u0026ldquo;9 principes pour préserver le modèle\u0026rdquo; Conscient de la complexité d\u0026rsquo;une mise à l\u0026rsquo;échelle de l\u0026rsquo;agilité, de la multitude de contextes auxquels les acteurs de la transformation sont confrontés au quotidien, l\u0026rsquo;auteur a défini des principes qui servent de barrières.\nCes principes sont des gardiens du modèle dans le cas où les pratiques recommandées par SAFe ne peuvent pas être appliquées. Ces barrières définissent les latitudes dans lesquelles nous pouvons adapter le framework.\nCes 9 principes sont:\n Adopter une vision économique Appliquer la pensée systémique Assumer la variabilité, préserver les options Construire progressivement avec des cycles d\u0026rsquo;apprentissage rapides et intégrés Poser des Jalons permettant de tester et aligner régulièrement le système de travail Visualiser et limiter les Work in Progress, réduire les tailles de lots et gérer les longueurs de file d\u0026rsquo;attente Appliquer une cadence et se synchroniser avec la planification inter-domaine Déverrouiller la motivation intrinsèque des collaborateurs Décentraliser la prise de décision  A leur lecture, rien de révolutionnaire, ni de choquant, que du bon sens au final, non ?\n\u0026ldquo;4 valeurs pour orienter les comportements\u0026rdquo; En plus de ces 9 principes, le créateur de SAFe pose 4 valeurs pour orienter les comportements des acteurs :\n L\u0026rsquo;alignement : Repose sur des objectifs métiers de niveau entreprise afin que l\u0026rsquo;ensemble des équipes agiles puissent faire face aux évolutions rapides du marché, comme par exemple l\u0026rsquo;émergence d\u0026rsquo;un concurrent. L\u0026rsquo;alignement ne peut pas reposer sur l\u0026rsquo;opinion combinée des équipes agiles. Cet alignement se fait du niveau stratégique jusqu\u0026rsquo;au niveau des équipes qui exécutent le contenu des programmes. La qualité : Elle ne peut pas être décidée à rebours, elle doit être intégrée dès le départ afin d\u0026rsquo;éviter des retards. Cette qualité intégrée se décline autour de 4 axes : Software, Hardware, Système d\u0026rsquo;intégration, Conformité. La transparence L\u0026rsquo;exécution du programme  \u0026ldquo;1 mindset pour les gouverner\u0026rdquo; Pour gouverner ce modèle, il est nécessaire d\u0026rsquo;avoir un certain mindset (état d’esprit) qui est lui aussi défini par l\u0026rsquo;auteur. Ce mindset est formé de deux ensembles.\nLe premier ensemble s\u0026rsquo;inspire de la Maison du Lean créée et définie par Toyota, baptisé ici : SAFe House Lean. Elle se compose des éléments suivants :\n  Un toit qui définit l\u0026rsquo;objectif : fournir la valeur client maximale dans les délais les plus courts possibles tout en offrant la meilleure qualité possible aux clients et à la société dans son ensemble\n  4 piliers qui soutiennent l\u0026rsquo;objectif :\n Pilier 1 : Respect de la culture et des personnes Pilier 2 (Flux) : Établissement d’un flux de travail continu qui prend en charge une création de valeur continue, basée sur un retour d\u0026rsquo;informations et des ajustements constants. Un flux continu permet une livraison de valeur plus rapide, des pratiques de qualité intégrée efficaces, une amélioration constante et une gouvernance fondée sur des preuves Pilier 3 (Innovation): Encourager la rencontre avec des utilisateurs pour vous nourrir de leurs problèmes afin d’innover Pilier 4 (Amélioration continue) : Encourager l’apprentissage et la croissance grâce à une réflexion continue et à des améliorations des processus    Des fondations qui s\u0026rsquo;appuient sur le leadership\n  Le deuxième ensemble n\u0026rsquo;est autre que le manifeste Agile.\n\u0026ldquo;4 niveaux pour structurer\u0026rdquo; En complément de ce cadre déjà conséquent, l’auteur a organisé des pratiques recommandées à travers des niveaux dont nous pouvons faire le parallèle avec les strates de l’entreprise.\nL’objectif de ces niveaux : permettre d’aligner l’ensemble des forces selon les stratégies d’investissement prises à la tête de l’entreprise.\nCet alignement s’effectue par le lien fait grâce aux livrables et aux événements de chaque niveau qui permettent la circulation dans les deux sens : Bottom up et Top Down. A chaque niveau, un objectif, des rôles, des évènements et des livrables sont définis.\nCes niveaux sont :\n Le niveau « Portfolio » que nous pouvons rattacher à la strate « Top Management » de l’entreprise. C’est le lieu où l’on met en cohérence la vision de l’entreprise, avec les stratégies d’investissement pour enfin définir des Portefeuilles qui contiennent des Epics de niveau Business [NDR : !!!] Le niveau « Program » que nous pouvons rattacher à la strate « Middle Management » de l’entreprise. C’est le lieu où les EPiCs de niveau Business sont découpés pour donner naissance à un ou plusieurs Programme(s) de développement. C’est le coeur du Système de SAFe, là où la valeur se crée de façon itérative par l’intermédiaire de l’ Agile Release Train (ART). Le niveau « Team » que nous pouvons rattacher à la strate « Opérationnelle » de l’entreprise. C’est le lieu où les équipes agiles alimentent l’ART par leur développement. Le niveau « Large Solution » est une extension du niveau « Program » dont l’objectif est d’organiser la synchronisation de plusieurs ART.  Chacun de ces niveaux possède son propre objectif dans la fusée SAFe mais avec une articulation identique pour tous : un objectif défini, incluant des rôles, des évènements, des livrables.\n\u0026ldquo;12 Evènements pour l\u0026rsquo;animer\u0026rdquo; Le maintien de l\u0026rsquo;alignement avec les objectifs stratégiques définis au niveau du portfolio se fait grâce à de nombreux événements répartis par niveau. Chacun a bien évidemment un objectif précis.\nNiveau Team  Iteration Planning : Moment où l\u0026rsquo;Agile Team détermine les objectifs, le nombre de stories et d\u0026rsquo;enabler de la prochaine itération. La CapEx de l\u0026rsquo;équipe détermine le nombre de stories de l’itération. Iteration Review : Inspection et démo de l\u0026rsquo;incrément à la fin de l\u0026rsquo;itération, les feedbacks reçus alimentent le Team Backlog. Iteration Execution : Moment où l\u0026rsquo;Agile Team développe l\u0026rsquo;incrément avec une synchro quotidienne de 15 min. Iteration Retrospective : Se passe en fin d\u0026rsquo;itération, moment où l\u0026rsquo;Agile Team examine ses pratiques et cherche des voies d\u0026rsquo;améliorations Backlog Refinement : Organisé 1 à 2 fois dans l\u0026rsquo;itération pour affiner, réviser et estimer les stories, enablers présentes dans le team backlog Innovation and Planning Iteration : Sert de tampon d\u0026rsquo;estimation pour la réalisation des PI Objectives et fournit du temps consacré à l\u0026rsquo;innovation, à la formation continue, PI Planning et Inspect and Adapt events  Niveau Program  PI Planning : Événement de planification, d\u0026rsquo;une durée de 2 jours, réunissant l\u0026rsquo;ensemble des acteurs intervenant dans l\u0026rsquo;ART System Demo : Présenter l\u0026rsquo;ensemble des fonctionnalités développées dans la dernière itération par toutes les Agile Teams de l\u0026rsquo;ART Inspect \u0026amp; Adapt : Phase d\u0026rsquo;amélioration continue qui s\u0026rsquo;appuie sur les feedbacks obtenus lors du System Demo  Niveau Solution Program  Pre- et Post-PI Planning : Phase de préparation et de suivi des résulats de l\u0026rsquo;ART Solution Demo : Présenter l\u0026rsquo;ensemble des fonctionnalités développées par tous les ART qui composent la solution Inspect \u0026amp; Adapt : Phase d\u0026rsquo;amélioration continue qui s\u0026rsquo;appuie sur les feedbacks obtenus lors du Solution Demo  Niveau Portfolio : Aucun \u0026ldquo;1 Practice pour souder\u0026rdquo; Il y a un élément important qui se situe dans le niveau program mais qui n\u0026rsquo;est pas décrit comme un événement lui-même. Il s\u0026rsquo;agit de l\u0026rsquo;Agile Release Train ou ART. C\u0026rsquo;est le coeur du niveau program, celui qui soude tous les évènements des niveaux program et team.\nUn ART:\n regroupe plusieurs équipes travaillant sur une même Itération de PI Planning cadence la livraison de valeur de l\u0026rsquo;ensemble des teams qui le composent délivre la valeur sur le rythme d\u0026rsquo;itération le plus faible. C\u0026rsquo;est à dire si une team travaille sur des sprints de 2 semaines et qu\u0026rsquo;une autre travaille sur des sprints de 3 semaines, le rythme le plus faible est 3 semaines  \u0026ldquo;13 rôles pour le contrôler\u0026rdquo; Pour donner vie et contrôler ce framework, il faut des rôles à incarner. Il y en a 13 définis, répartis sur les différents niveaux de SAFe.\nNiveau Team  Agile Team : 5 à 11 personnes (Dev team, PO, et SM) ayant la capacité et l\u0026rsquo;autorité pour définir, construire et tester une story ou un enabler de la solution dans une itération. Dev Team : Équipe interfonctionnelle composée de développeurs, de testeurs et autres spécialistes Product Owner : Responsable de la définition des Stories et de la priorisation du Team Backlog. Est le seul à pouvoir décréter \u0026ldquo;Done\u0026rdquo; une Story Scrum Master : Servant leader et coach, il aide à éliminer les obstacles, facilite les évènements d\u0026rsquo;équipe  Niveau Program  System Architect / Engineer : Petite équipe interdisciplinaire qui définit l\u0026rsquo;architecture globale du système, aide à définir les besoins non fonctionnels (NFR), détermine les principaux éléments et sous-systèmes et aide à concevoir les interfaces et les collaborations entre eux. Product Management : Représentent la voix interne du client, travaillent avec les clients et les Product Owners pour comprendre et communiquer leurs besoins, définir les caractéristiques du système et participer à la validation. Ils sont responsables du Program Backlog. Release Train Engineer : Scrum Master pour L\u0026rsquo;ART. Facilite l\u0026rsquo;optimisation du flux de valeur à travers le Program en utilisant divers mécanismes, tel que KanBan program, Inspect \u0026amp; Adapt, PI Planning Business Owner : Stakeholders responsables techniquement et commercialement de l\u0026rsquo;adéquation au marché et du ROI de la solution développée par un ART  Niveau Solution Program  Customer : Acheteur ultime de la solution. Il intervient comme stakeholder lors du PI Planning Solution Architect/Engineering : Petite équipe qui définit une vision technique et architecturale commune pour la solution en cours de développement Solution Management : Voix interne du client qui travaille avec lui pour comprendre les besoins, créer la vision et la feuille de route de la solution, définir les exigences et guider le travail à travers Kanban Solution Solution Train Engineer : Servant Leader et coach qui facilite et guide le travail de l\u0026rsquo;ensemble des ART qui composent la solution Supplier : Organisation interne ou externe qui développe et fournit des composants, des sous systèmes ou services  Niveau Portfolio  Lean Portfolio Management : Groupe de personnes qui organisent et pilotent les portefeuilles d\u0026rsquo;investissement Epics Owners : Responsables de coordonner les Epics PortFolia à travers le system Portfolio Kanban Enterprise Architect : Personne qui aide à fournir l\u0026rsquo;orientation technique stratégique  \u0026ldquo;20 artifacts à délivrer\u0026rdquo; Un cadre, des niveaux, des events, des rôles mais on délivre quoi ? Une vingtaine d\u0026rsquo;artefacts sont définis, décrits et répartis eux aussi par niveau\nNiveau Team  Story : Véhicule transportant les exigences du client. Utilisé par les équipes pour produire de la valeur au sein d\u0026rsquo;une itération Enabler stories : Initiative technique et architecture nécessaires pour réaliser les stories Iteration goals : Résumé de haut niveau des objectifs commerciaux et techniques que l\u0026rsquo;équipe agile accepte d\u0026rsquo;accomplir dans une itération Team Backlog : Contient les stories et enablers. La majorité est identifiée lors du PI Planning et Backlog refinement Team PI Objectives : Résumé des objectifs métiers et techniques spécifiques qu\u0026rsquo;une Agile team souhaite atteindre dans une itération  Niveau Program  Features : Fonctionnalité dont un stakeholder a besoin. Elle contient les hypothèses de bénéfices et les critères d\u0026rsquo;acceptance. Elle est dimensionnée pour tenir dans un ART Program Epics : Premier découpage des Business Epics afin qu’elles rentrent sur un seul ART Program Backlog : Zone d\u0026rsquo;attente pour les fonctionnalités à vernir. On compte un program backlog par ART Program Kanban : Gestion du flux des features pour assurer un pipeline de livraison continue PI Objectives : Résumé des objectifs métiers et techniques spécifiques qu\u0026rsquo;un ART souhaite atteindre dans le prochain IP Architectural Runway : Contient le code, les composants et l\u0026rsquo;infrastructure technique nécessaires pour mettre en œuvre les fonctionnalités  Niveau Large Solution  Capabilities : Ensemble de \u0026ldquo;haut niveau\u0026rdquo; qui donnera naissance à des Features et des Enablers qui alimenteront ensuite chacun des ART Solution Epics : Premier découpage des Business Epics afin qu’elles rentrent sur plusieurs ART Nonfunctionnal Requirements (NFRs) : Définit les exigences autour de la sécurité, de la fiabilité, de la robustesse, de la performance, etc… Solution Backlog : Zone d\u0026rsquo;attente des Capabilities avant leur split pour incorporation dans un ou plusieurs ART Solution Kanban : Gestion du flux des capabilities pour assurer un pipeline de livraisons continues  Niveau Portfolio  Business Epics : Nouvelle idée business Enabler Epics : Initiative technique et architecture nécessaires pour réaliser les Business EPICS Strategic themes : Items qui permettent de faire le lien entre la stratégie d\u0026rsquo;entreprise et les portefeuilles Portfolio backlog : Zone d\u0026rsquo;attente contenant les Business EPICs et Enabler Epics avant leur découpage en Solution Program ou Program  Conclusion Au vu de la taille exceptionnelle de la documentation, on peut s’interroger sur la priorité donnée à l’efficacité de la méthode. Ma longue lecture d\u0026rsquo;une bonne partie de la documentation disponible et dont vous avez un résumé au dessus ne m\u0026rsquo;a pas enlevé l\u0026rsquo;impression que j\u0026rsquo;avais à la découverte de la carte de la méthode : \u0026ldquo;Des processus et des outils plus que les individus et leurs interactions\u0026rdquo;.\nCette impression subsiste pour plusieurs raisons :\n  Il y a tellement d\u0026rsquo;éléments définis pour atteindre l\u0026rsquo;objectif de l\u0026rsquo;agilité à l\u0026rsquo;échelle que cela donne l\u0026rsquo;impression :\n qu\u0026rsquo;il n\u0026rsquo;y a aucune latitude pour s\u0026rsquo;adapter au contexte et à la culture de l\u0026rsquo;entreprise dans laquelle on souhaite l\u0026rsquo;implémenter que l\u0026rsquo;auto-organisation n\u0026rsquo;est pas la bienvenue même si elle est maintes fois citée dans le framework    La méthode crée un éloignement entre les clients et les créateurs de valeurs (l'Agile Team) : les product Owners ne semblent pas avoir un accès direct aux clients, ils passent par les program owners et les EPics Owners :\n Les clefs de l’agilité et du DevOps sont donc perdues. Les clefs du Fast Fail sont aussi perdues : l\u0026rsquo;agilité a pour vocation d\u0026rsquo;aider au Fast Fail, puisque rien ne se passe jamais vraiment comme cela devrait. Or avec ses différents niveaux de délégation, SAFe va freiner le côté fast du Fast Fail, du fait de l\u0026rsquo;augmentation du temps de traversée des couches entre le développeur et le porteur de l\u0026rsquo;Epic concernée. Restera le fail qui sera la responsabilité de l’équipe de développement.    Le time to market pour délivrer reste lent car il faut s\u0026rsquo;aligner sur la team la plus lente pour livrer un incrément. Or dans une ère numérique où il est important de délivrer vite de la valeur cela peut déranger : on aurait préféré d’autres moyens pour minimiser le time to market, comme par exemple un apprentissage par coaching ou partage de bonnes pratiques, un outillage pour s’en tenir strictement à un maximum de 2 semaines de sprints, etc.\n  Cette lecture m\u0026rsquo;a donné l\u0026rsquo;impression d\u0026rsquo;un greenwashing des organisations existantes avec la quantité de rôles définis : on prend les organisations existantes dans les grandes entreprises, celles-là mêmes qui avaient échoué à mener leurs grands projets et s’étaient senties dépossédées de leur rôle avec l’apparition de l’agilité et du management 3.0, on les tague “SAFe” et hop! elles sont agiles, elles sont à l’échelle, elles sont légitimes.\nAu cours de mon expérience professionnelle, j’ai rencontré beaucoup de défauts de la mise à l’échelle, ils sont liés par exemple :\n à l’éloignement des acteurs : SAFe n’en tient pas compte à la gestion de différents environnements entre équipes (backlogs, source repositories, PIC) : SAFe suppose que tout est déjà harmonisé à la mise en place de Scrum multipliée par autant de Feature Teams : SAFe considère que c’est maîtrisé et vient rajouter un nombre conséquent de rôles et outils qui va complexifier la mise en place, sans nous dire comment choisir si on ne veut implémenter qu’un extrait de la méthode au cadencement des sprints : SAFe s’aligne sur le moins véloce au respect du time to market pour délivrer le produit : en plus du point précédent, SAFe met les fonctionnalités dans des trains, en supposant que l’assemblage, via la méthode Kanban, sera satisfaisant à la recherche de réduction de la complexité : dans un grand nombre de situations que j’ai pu rencontrer, la mise à l’échelle ne devait être qu’un remède, pas une fin en soi. La méthode devrait commencer par cette indication “si vous avez tout tenté pour éviter la mise à l’échelle, nous vous proposons ces quelques pratiques à considérer comme un mal nécessaire”.  Pour finir, comme l\u0026rsquo;indique le Chaos Report de Standish Group et par mon expérience, les projets les plus vastes sont ceux qui aboutissent le moins, mais au vu des mots clés qui composent la méthode (Scrum, Lean, Kanban, Scale, Epic, \u0026hellip;), vous aurez la garantie d’avoir mis toutes les chances de votre côté grâce à SAFe. Votre management (2.0) ne vous reprochera rien, tout était prévu. Mais est-ce que tout s’est passé comme prévu ?\nNDA : Lorsque je me suis plongé dans la gargantuesque documentation de SAFe pour écrire cet article, j\u0026rsquo;avais déjà un a priori négatif sur ce Framework. J\u0026rsquo;ai cependant essayé de retranscrire ici ma compréhension du modèle en tentant de rester impartial.\n","date":"Jul 31, 2018","href":"https://blog.talanlabs.com/safe-cest-quoi/","kind":"page","labs":null,"tags":null,"title":"SAFE, c'est quoi?"},{"category":null,"content":"Il nous arrive très souvent de devoir écrire des scripts ou bien du code directement sur les machines virtuelles de développement (parce que notre système d\u0026rsquo;exploitation ne supporte pas Docker\u0026hellip; par exemple). Ce n\u0026rsquo;est pas que je n\u0026rsquo;aime pas utiliser les éditeurs comme VI, VIM, emacs ou nano mais j\u0026rsquo;ai tout de même mes habitudes de développeurs ! Mon serveur ubuntu n’a pas d’interface graphique, je m\u0026rsquo;y connecte en ssh avec putty (en mode console) :(\nLes raccourcis VI/VIM (www.viemu.com)\nMon OPS avisé m\u0026rsquo;a présenté une solution (vraiment pas toute jeune) mais efficace ! Le déport d\u0026rsquo;affichage avec SSH : \u0026ldquo;X11 Forwarding\u0026rdquo;. Cela permet d\u0026rsquo;exécuter des applications graphiques du genre VSCode, Chrome ou autre en déportant l\u0026rsquo;affichage de serveur Linux sur mon poste Windows au travers d\u0026rsquo;une connexion SSH.\nVoici comment cela se passe :\nJe démarre mon IDE depuis mon terminal putty sous windows. L\u0026rsquo;exécution de l\u0026rsquo;application graphique (l\u0026rsquo;IDE) se fait sur le serveur Linux qui est incapable de l\u0026rsquo;afficher mais il sait générer l\u0026rsquo;affichage qui transitera au travers de la connexion SSH à l\u0026rsquo;aide du client SSH et graphique adéquate. Une fois les informations reçues par notre client, Xming (le système de fenêtrage x11) va s\u0026rsquo;occuper de l\u0026rsquo;interprétation de ces informations et orienter ces informations vers un affichage graphique. Ce processus, qui peut sembler complexe expliqué comme cela, est en fin de compte assez simple à mettre en place.\nComment le paramétrer ? Coté server : xvfb (X Virtual frame Buffer), un serveur X « virtuel »\n$ sudo apt-get install xvfb xdm xfonts-base\n$ sudo apt-get install twm \u0026ldquo;xfonts-100dpi*\u0026rdquo; xterm\nActiver X11Forwarding dans la configuration du serveur SSH distant:\nvi /etc/ssh/sshd_config X11Forwarding yes AllowTcpForwarding yes X11DisplayOffset 10\nCoté Windows : Démarrer une connexion SSH avec un client SSH (Putty) autorisant le X11 Forwarding et s\u0026rsquo;assurer que le client Xming est démarré sur le poste client.\n Xming X-Server ( http://sourceforge.net/projects/xming/ ) mobaXterm (https://mobaxterm.mobatek.net/  Il ne vous reste plus qu\u0026rsquo;à installer VSCode (par exemple), c\u0026rsquo;est un IDE gratuit, très léger, simple et rapide, entièrement réalisé à l\u0026rsquo;aide de TypeScript, Electron et Chromium) sur votre VM Linux (https://code.visualstudio.com/docs/setup/linux) et de le démarrer sur votre machine virtuelle :\n$ code .\nProfitez et améliorez votre productivité !\n","date":"Jul 9, 2018","href":"https://blog.talanlabs.com/vim-cest-bien-mais-vscode/","kind":"page","labs":null,"tags":["DevOps","vscode","x11"],"title":"VIM c'est bien mais je préfère VSCode ! Merci X11 Forwarding"},{"category":null,"content":"J\u0026rsquo;ai eu l\u0026rsquo;occasion de connaitre le CRiP par l\u0026rsquo;intermédiaire de l\u0026rsquo;un de mes clients où j\u0026rsquo;opère en mission en ce moment. Ce club, le CRiP (www.crip-asso.fr), Club des Responsables d’Infrastructure et de Production est une association loi 1901 (association à but non lucratif). Elle compte, parmi ses adhérents un certain nombre de nos clients, des grands comptes, entreprises et administrations, qui utilisent des technologies de l’information. Ses membres, confrontés aux mêmes défis financiers, technologiques et organisationnels, échangent et partagent leurs expériences au sein d’un cercle de confiance.\nCRiP : l\u0026rsquo;entraide et le partage S’entraider via le réseau social qu\u0026rsquo;il propose, c\u0026rsquo;est le principal objectif du CRiP. L\u0026rsquo;association se défini comme un lieu de partage d’expertise et de connaissance sur les domaines techniques innovants. Les adhérents éligibles (les ESN ne sont pas autorisées à adhérer), ce sont des grandes entreprises et administrations utilisatrices des technologies de l’information ou des professionnels des infrastructures des SI. Les Entreprises sont représentées la plupart du temps par leur CTO Chief Technical Officer et, parfois, par leur CIO Chief Information Officer.\nLes membres du CRiP comprennent les CTOs, parfois les CIOs, mais aussi les responsables d’infrastructure et de production qui leurs sont rattachés dans les domaines des Data Centers, Systèmes et OS, Stockage, Réseaux et Télécoms, Sécurité, Poste de Travail, Gouvernance et Process ainsi que les directeurs de projets Cloud Computing, Mobilité, Big Data et Social Media.\nLe CRiP organise très souvent des conventions et conférences soit au niveau national, soit régional (Sud Ouest, Hauts-de-France, Auvergne-Rhône Alpes, PACA, Grand Est, et Grand Ouest) et elles sont exclusivement réservées à ses membres.\n\nSi votre organisation n\u0026rsquo;est pas encore adhèrente, allez voir sur le site de www.crip-asso.fr\n","date":"Jul 2, 2018","href":"https://blog.talanlabs.com/crip-club-responsables-infra-prod/","kind":"page","labs":null,"tags":["DSI"],"title":"CRiP : Club des Responsables de l’Infrastructure et de Production"},{"category":null,"content":"Cet article s’insère dans notre série « Chaos Engineering ».\nLe contexte Les architectures applicatives actuelles s\u0026rsquo;éloignent de plus en plus d\u0026rsquo;une structure en blocs monolithiques, pour s\u0026rsquo;orienter vers des architectures basées sur de la composition de services et structurées en systèmes distribués, notamment par l\u0026rsquo;utilisation des micro-services.\nLes applications basées sur ces architectures proposent des fonctionnalités provenant de l\u0026rsquo;interaction de leurs composants, et de la bonne collaboration de l\u0026rsquo;ensemble des composants.\nLes composants de ces architectures peuvent se compter par centaines, ce qui apporte des problématiques de gestion applicatives de plus en plus liées aux systèmes distribués, et acquièrent des propriétés similaires à celles des systèmes complexes.\nUn système complexe peut être défini de plusieurs façons d\u0026rsquo;après le prisme dont il est observé, nous considèrerons les quelques propriétés essentielles suivantes, utiles pour la suite:\n il est composé d\u0026rsquo;un grand nombre d\u0026rsquo;éléments en interaction et ce, de manière simultanée le comportement d\u0026rsquo;un système complexe est très difficile à modéliser, même en connaissant parfaitement chaque élément de ce système. Le comportement est émergent, car il est issu des différentes interactions entre les éléments le composant. l\u0026rsquo;action d\u0026rsquo;un composant peut avoir un effet sur son propre état, sur l\u0026rsquo;état d\u0026rsquo;autres composants et sur l\u0026rsquo;état global du système par propagation la connaissance d\u0026rsquo;une partie du système ne permet pas de déterminer l\u0026rsquo;état global du système  Les fonctionnalités proposées par une application sur l\u0026rsquo;interaction de composants à l\u0026rsquo;intérieur d\u0026rsquo;un système complexe seront donc systémiques, et dépendantes du bon fonctionnement et de la bonne coordination des différents composants.\nMalheureusement, la moindre faille peut avoir des conséquences lourdes sur la bonne fonctionnalité de ces systèmes (voir propriétés ci-dessus), et il est très difficile, voire impossible de modéliser l\u0026rsquo;ensemble des conséquences pouvant émerger de la faille d\u0026rsquo;un composant (failles en cascade, goulot d\u0026rsquo;étranglement) et/ou de l\u0026rsquo;orchestration de différents composants (\u0026ldquo;retry storms\u0026rdquo;).\nLes tests existants (unitaires, intégration, techniques) permettent de tester la bonne fonctionnalité de composants isolés, ou en intégration simple, mais restent très limités dans la possibilité de tester la robustesse d\u0026rsquo;un système complexe à l\u0026rsquo;échelle réelle car ils restent déterministes.\nUtiliser un environnement autre que l\u0026rsquo;environnement réel peut aussi entraîner des biais qui fausseront les observations et la possibilité de les transposer dans la réalité.\nOn pré-supposera finalement que le fait d\u0026rsquo;observer le système réel n\u0026rsquo;a pas d\u0026rsquo;impact conséquent sur le comportement de ce système.\nQu\u0026rsquo;est-ce que le chaos engineering? Le chaos engineering est une approche mise en avant par Netflix en 2008, afin d\u0026rsquo;augmenter sa confiance en sa capacité de fournir des flux vidéos et services associés à des millions de personnes à partir d\u0026rsquo;une architecture distribuée complexe et hébergée dans un cloud public.\nLe chaos engineering adopte une approche proactive d\u0026rsquo;expérimentation au niveau de l\u0026rsquo;environnement de production, afin de détecter les faiblesses d\u0026rsquo;un système, en utilisant une méthodologie proche de l\u0026rsquo;étude des systèmes dynamiques par simulation. On va donc étudier la résilience du système et sa capacité à s\u0026rsquo;adapter à différents problèmes.\nDans l\u0026rsquo;étude des systèmes dynamiques, comme par exemple dans les simulations multi-agents dynamiques à grande échelle [2], la méthodologie est la suivante :\n Définition des variables d\u0026rsquo;observation saillantes du système, permettant de mesurer ses performances ou l\u0026rsquo;état \u0026ldquo;normal\u0026rdquo; de fonctionnement. Comme éléments de mesure, des variables d\u0026rsquo;observation liées aux propriétés du système que nous voulons tester (temps de réponse, ..) sont utilisées. Identification de l\u0026rsquo;état stable (fonctionnellement adéquat) du système, sur une période donnée, qui correspond à son comportement dit \u0026ldquo;normal\u0026rdquo;. Cette notion d\u0026rsquo;état stable peut être une agrégation de plusieurs phases d\u0026rsquo;observation dans les mêmes conditions initiales de départ et sur la même durée, afin d\u0026rsquo;atténuer les bruits ponctuels. Identifier les perturbations au niveau des composants ou du système distribué pouvant entraîner l\u0026rsquo;échec des fonctionnalités (panne de composant, rupture des communications inter-composants, etc..), issus d\u0026rsquo;évènements réels observés, qui constitueront autant de moyens d\u0026rsquo;expérimentations. Avec les mêmes conditions initiales que pour l\u0026rsquo;identification de l\u0026rsquo;état stable, introduire un \u0026ldquo;choc\u0026rdquo; lié à une perturbation ou faute identifiée à un instant t et observer les conséquences par rapport au comportement du système dans son état stable.  On se focalisera donc sur la notion d’observation du comportement au niveau de la fonctionnalité nominale (état stable) et les divergences observées lors de l\u0026rsquo;introduction des chocs au niveau systémique (\u0026ldquo;controlled failure-injection\u0026rdquo;).\nL\u0026rsquo;hypothèse de base est que le système gardera un état proche de l\u0026rsquo;état stable malgré l\u0026rsquo;introduction des chocs. Si cette hypothèse est vérifiée, alors on peut avoir confiance dans la robustesse du système. Si elle ne l\u0026rsquo;est pas, alors le système souffre de fragilités qu\u0026rsquo;il sera nécessaire d\u0026rsquo;identifier et de prendre en compte.\nCertaines précautions de bon sens sont quand même à garder à l\u0026rsquo;esprit et le choix des chocs à injecter en environnement de production fait partie intégrante de la responsabilité du chaos engineer, vu que ces chocs peuvent créer des pannes graves sur le système et le service fourni.\nIntroduire les chocs : les chaos monkeys L\u0026rsquo;introduction de chocs et l\u0026rsquo;analyse des conséquences étant une tâche non triviale, le chaos engineering met aussi en avant l\u0026rsquo;automatisation de ces tâches par des agents du chaos aussi appelés les \u0026ldquo;chaos monkeys\u0026rdquo;.\nUn chaos monkey est une interface, derrière laquelle seront implémentés les comportements associés aux chocs voulus, et qui seront appliqués aux périodes ou intervalles sélectionnés. Netflix fournit sur github une implémentation disponible dans les références de cet article [3].\nUn chaos monkey permettra par exemple de tuer un processus associé à un micro-service, ou l\u0026rsquo;ajout d\u0026rsquo;un blocage sur un firewall entraînant une disruption des interactions lors de l\u0026rsquo;orchestration de services.\nAu fur et à mesure des expérimentations, l\u0026rsquo;équipe en charge du chaos engineering pourra disposer d\u0026rsquo;un ensemble de monkeys (ou simian army par Netflix) qui permettront de vérifier les différents aspects de résilience du système.\nAutour du chaos engineering Le chaos engineering n\u0026rsquo;est pas une démarche isolée et s\u0026rsquo;inscrit dans la collaboration (DevOps) des équipes de développement et des opérationnels qui disposent de points de vue complémentaires sur la compréhension des comportements systémiques. Ces équipes se concertent pour identifier les causes de disruption et identifier les chaos monkeys à mettre en place lors des phases d\u0026rsquo;expérimentation.\nUn point intéressant remonté par B. Gakic lors de la présentation Devoxx 2018 [1][4], est que ces équipes font aussi partie intégrante du système complexe. Les expérimentations chaos engineering permettent aussi d\u0026rsquo;identifier les faiblesses au niveau des équipes par rapport à leur maîtrise lors des remontées de pannes en production.\nAfin de se concerter et permettre aux équipes de s\u0026rsquo;exercer, le concept de \u0026ldquo;GameDay\u0026rdquo; (ou drills/randori d\u0026rsquo;après votre méthodologie/discipline sportive préférée) a été introduit par Netflix et repris par les équipes de Oui-Sncf [5] en introduisant de la gamification. Cela a permis aux équipes de renforcer leur cohésion et leur compréhension.\nRéférences Ci-dessous quelques références pour mieux se documenter :\n Liste de ressources liées au chaos engineering Meetup Chaos Engineering Paris  Références article  [1] https://www.youtube.com/watch?v=WTT2GJquAWY [2] http://www.mcs.anl.gov/~leyffer/listn/slides-06/MacalNorth.pdf [3] https://github.com/Netflix/chaosmonkey [4] https://fr.slideshare.net/BenjaminGakic/devoxx-2018-chaos-engineering [5] http://days-of-chaos.com/  Vous avez aimé cet article ? Découvrez ou redécouvrez l\u0026rsquo;autre épisode de la série « Chaos Engineering » :\nDevOps Night #3 (https://blog.talanlabs.com/devops-night-3-le-meta-meetup-devops/)\n","date":"Jun 22, 2018","href":"https://blog.talanlabs.com/le-chaos-engineering/","kind":"page","labs":null,"tags":["chaos","chaos engineering","monkey"],"title":"Le chaos engineering"},{"category":null,"content":"Cover by Daniel Cheung on Unsplash\nLe Product Owner technique, est généralement un ancien développeur ou chef de projet technique.\nDans ce cas là, il est possible que ce “PO-technique” définisse les histoires en y expliquant comment réaliser la fonctionnalité.\nAprès tout, en tant que technicien, son expertise technique est reconnue et attendue par l’entreprise. Par sa direction même, comme pour justifier son évolution.\nIl est alors important pour le Scrum Master de révéler la situation au grand jour, avant que le problème ne fasse un effet « Scrum ça marche pas pour nous ». Comme je l’explique dans mon talk Être agile, plutôt que de faire de l’agile, je n’ai jamais vu une vraie équipe Scrum échouer un projet (une bonne équipe agile échoue souvent =\u0026gt; culture de l’échec, mais là je parle de l’échec du projet).\nEn décrivant comment faire, le PO technique retire la possibilité pour l’équipe de développement d’inventer, de créer, de s’approprier le produit ou la fonctionnalité. Il déresponsabilise les développeurs. Après tout, si ça ne fonctionne pas, ce n’était pas leur choix. L’équipe doit alors s’engager à livrer quelque chose qu’ils ne maîtrisent pas forcément.\nIl est donc important pour **le Scrum Master d’accompagner ce PO technique **à devenir un PO fonctionnel. Tout cela passe par l’apprentissage, pas si simple, de l’écriture d’histoires, de découpage, etc …\nEt si cela ne fonctionne pas, car n’est pas PO qui veut, ou que ce PO technique est un … “Proxi-PO technique”, il est alors préférable de retirer cette personne de l’équipe, d’aller chercher le vrai PO, de lui inculquer les enjeux de son métier et pourquoi il doit abandonner son bureau et se mêler à l’équipe. “LE PRODUIT N’EN VAUT-IL PAS LA PEINE ?” est une question à poser à la moindre contestation (voir l\u0026rsquo;excellent Scrum Life #17 de JP Lambert pour apprendre à faire les yeux tristes du Scrum Master).\n","date":"Jun 21, 2018","href":"https://blog.talanlabs.com/quand-le-po-technique-dit-comment-faire/","kind":"page","labs":null,"tags":null,"title":"Quand le PO-technique dit comment faire"},{"category":null,"content":"Le choix des méthodes agiles est désormais dans les moeurs de la plupart de nos clients. Les initiatives DevOps se multiplient également. Mais où en est on de la prise en compte de la sécurité de nos applications par les équipes ?\nLors de la conférence DEVOXX 2018, Erik Lenoir nous a partagé les bons réflexes à avoir en terme de sécurité pendant la phase de développement. Je vais vous en retranscrire l\u0026rsquo;essentiel.\nOWASP (Open Web Application Security Project) Le projet OWASP est :\n un projet à but non lucratif, il a pour vocation de promouvoir les bonnes pratiques pour obtenir un monde plus sécurisé agnostique à la technologie (PHP, .net, Java, \u0026hellip;) et s\u0026rsquo;intéresse à toutes les stacks technologiques quelles qu\u0026rsquo;elles soient contribué sans contrepartie par une communauté active d\u0026rsquo;expert en sécurité.  Top 10 Le TOP 10 de l’OWASP est un projet visant à maintenir à jour la liste des 10 risques de sécurité et vulnérabilités les plus critiques affectant les applications Web. Le classement 2017 est disponible sur le site de l\u0026rsquo;OWASP (https://www.owasp.org/index.php/top10) sachant que le précédent datait de 2013 :-(\nCe classement a pour but d’informer sur l’existence de ces vulnérabilités et de fournir des guides simplifiés sur les bonnes pratiques pour s’en prémunir. Ces guides s’adressent aux développeurs, architectes, chef de projets, managers…\nVoici les évolutions apportées par le millésime 2017 OWASP Top10 2017\nLes risques liés à l’Injection (SQL, NoSQL, LDAP, code, système, \u0026hellip;), la gestion de l’authentification et de la session sont et restent en tête du classement. Par contre, deux risques sortent de ce top 10 :\n CSRF (Cross Site Request Forgery), ce type d’attaques est en baisse, les framework actuels disposant de protections à ces attaques. Redirection et renvois non validés, ce point a été sorti pour laisser entrer les nouveaux sujets qui sont plus pertinents.  On notera l\u0026rsquo;arrivée de \u0026ldquo;Insecure deserialization\u0026rdquo; et de l\u0026rsquo;incroyable \u0026ldquo;Insufficient Logging \u0026amp; Monitoring\u0026rdquo; !\nOWASP Top 10 Proactive Controls 2018 Il est toujours appréciable de se souvenir des quelques règles de bases à appliquer quand on développe une application.\n Define Security Requirements Leverage Security Frameworks and Libraries Secure Database Access Encode and Escape Data Validate All Inputs Implement Digital Identity Enforce Access Controls Protect Data Everywhere Implement Security Logging and Monitoring Handle All Errors and Exceptions  La SECU dans mon pipeline ou la sécurité en Continu Il n\u0026rsquo;est pas donné à tout le monde d\u0026rsquo;avoir une vision des risques dans son ensemble. Pour cela, il faudrait en avoir une entière compréhension\u0026hellip; Par contre, La communauté OpenSource et des éditeurs vous mettent à disposition tout un panel d\u0026rsquo;outils pour vous défendre (ou du moins vous sensibiliser à la cyber-sécurité) ! Regardons quelques moyens de gérer la vulnérabilité des applications au travers leurs codes sources.\n Une erreur classique est l\u0026rsquo;utilisation d\u0026rsquo;un composant dont les vulnérabilités sont connues\n Analyse des dépendances ou l\u0026rsquo;analyse des dépendance en Continu Coté Java OWASP dependency check (plugin gradle et maven) : Il construit l\u0026rsquo;arbre de dépendances transitif et pour chaque dépendances, il consultera la liste des alertes de la NVD (national vulnerability database) afin de vous prévenir d\u0026rsquo;une quelconque vulnérabilité connue.\nsources : https://github.com/jeremylong/DependencyCheck\nPour les plus consciencieux, vous pourrez casser votre build ! OWASP vous fournit également Dependency Track (https://dependencytrack.org/) un dashboard pour le suivi de vos analyses.\nCoté JavaScript Le monde du javascript n\u0026rsquo;est pas en reste puisqu\u0026rsquo;OWASP a pensé à leurs pléthores des librairies open-source. Sera à votre disposition :\n Dependency Check (Beta) NSP : Analyzes Node Package Manager (npm) package.json files using Node Security Platform. (payant) retireJS (https://retirejs.github.io/retire.js/)  Coté des containers N\u0026rsquo;oublions pas les vulnérabilités des images des containers eux-mêmes. Il y a quelques produits très intéressants à regarder :\n XRAY (https://jfrog.com/xray/) AQUA (https://www.aquasec.com/) Twistlock (https://www.twistlock.com/) CoreOs Clair (https://coreos.com/clair/docs/latest/)  Il existe également des scripts de contrôle de la configuration de vos serveurs. Docker met à disposition Docker Bench Security, un script qui vérifie un bon nombre de bonnes pratiques autour du déploiement des containers Docker (https://github.com/docker/docker-bench-security).\nLes licences ou la conformité en continu DBS (https://www.blackducksoftware.com/) et XRAY (https://jfrog.com/xray/) pourront vous assurer que vous utilisez bien des librairies pour lesquelles vous avez le droit. On oublie souvent de vérifier le niveau et la qualité d\u0026rsquo;une librairie Open-Source.\nAnalyse statique du code Les éditeurs des analyseurs de code s\u0026rsquo;y sont mis aussi. SonarSource fourni des régles de sécurité contre des vulnérabilités (https://rules.sonarsource.com/java/type/Vulnerability). Pensez à les activer !\nCoté javascript (enfin ECMA Script ou ES), on regardera également les linter de securité comme eslint-plugin-security (https://github.com/nodesecurity/eslint-plugin-security).\nMiddlewares Activer l\u0026rsquo;authentification ! Son absence est l\u0026rsquo;une des plus banale erreur que l\u0026rsquo;on retrouve encore. Les équipes de MySQL lançaient jusqu\u0026rsquo;à il y a encore quelques années que la seule faille de sécurité connue pour leur SGBD était le couple d\u0026rsquo;identifiant par défaut (root et pas de mots de passe ;-) ).\nCôté système, les experts vous préconiseront en premier lieu d\u0026rsquo;utiliser un chiffrement physique qui évitera le vol de données en les copiant physiquement ! La commision européenne vous mènera certainement vers un changement profond dans la manière de gérer les données personnelles et de les protéger, vous menant vers le chiffrement logique (ou par colonne).\nLes serveurs Web Certains petits ajustements peuvent aussi vous permettre une avancée significative en matière de sécurité. Ces modifications sont minimes en termes de code et de configuration, mais elles requièrent une bonne analyse et une validation avant d’être implémentées. Il s’agit des headers HTTP.\n HSTS : forcer le passage en https Content-Security-Policy : maîtriser le chargement de sources (et mitiger les XSS /Cross site scripting) en limitant à des domaines / éviter également les scripts inline X-Content-Security-Policy : complément à CSP sur des vieux navigateurs X-XSS-Protection : complément à CSP sur des vieux navigateurs X-Frame-Options : éviter le chargement par iframe X-Content-Type-options : contrôler les types MIME HPKP : stocker les clés de confiance pour détecter les éventuelles compromissions de certificats  Automatiser les Tests d\u0026rsquo;intrusions OWASP fourni le project ZAP (Zed Attack Proxy https://www.zaproxy.org/), un outil gratuit de tests d\u0026rsquo;intrusion qui permet de détecter les vulnérabilités de vos applications web. Il a plusieurs modes d\u0026rsquo;opérations (passif/safe et actif). ZAP est très pratique par son coté \u0026ldquo;scriptable\u0026rdquo;, lequel viendra évidemment compléter votre pipeline de build ;)\nRetrouver la vidéo originale : \u0026ldquo;Sécurité des applications Web les bons réflexes à avoir (E. Lenoir)\u0026rdquo;\n\nConclusion N\u0026rsquo;oubliez que les critères de choix de vos outils de test de sécurité sont :\n supporter votre langage de programmation les types de vulnérabilités qu\u0026rsquo;il peut détecter comprendre les librairies que vous utilisez monitorer (faux positif vs faux négatif) l\u0026rsquo;intégration dans l\u0026rsquo;IDE de dev la difficulté de paramétrage et d\u0026rsquo;utilisation qu\u0026rsquo;il puisse être lancé continuellement et automatiquement ! et le coût de la license  ","date":"Jun 11, 2018","href":"https://blog.talanlabs.com/owasp-2017-pour-nos-pipelines/","kind":"page","labs":null,"tags":["DevOps","secdevops","sécurité"],"title":"Le Guide de survie de OWASP 2017 (pour nos pipelines de CI/CD)"},{"category":null,"content":"Cet article s’insère dans notre série « Chaos Engineering ».\nLe meta-meetup DevOps Le jeudi 31 Mai 2018 s\u0026rsquo;est tenu le méta-meetup \u0026ldquo;DevOps Night #3\u0026rdquo; au sein des tours de la Société Générale à la Défense. Cette réunion était conjointement organisée par 4 grands meetup parisiens autour d\u0026rsquo;un même sujet : \u0026ldquo;le DevOps\u0026rdquo;.\n Docker Paris (www.meetup.com/Docker-Paris/) Kubernetes Paris (www.meetup.com/Kubernetes-Paris/) Paris Serverless Architecture Meetup (www.meetup.com/Paris-Serverless-Architecture-Meetup/) Paris Chaos Engineering Meetup (www.meetup.com/Paris-Chaos-Engineering-Meetup/)  La Keynote : Chaos isn\u0026rsquo;t a pit, Chaos is a ladder Pour cette troisième édition, la keynote a été réalisé par la dernière communauté à avoir rejoint le méta-meetup, je veux parler des animateurs du meetup \u0026ldquo;Chaos Engineering\u0026rdquo; : Christophe Rochefolle (@crochefolle) et Benjamin Gakik (@BenjaminGakic).\nComme à leurs habitudes, les deux \u0026ldquo;Chaos Engineer\u0026rdquo; de oui.sncf, on le chic pour attirer l\u0026rsquo;attention aussi bien des Dev que des Ops autour de leurs activités préférées : \u0026ldquo;semer volontairement le chaos dans une production informatique\u0026rdquo;\n Le Chaos Engineering est la discipline de l\u0026rsquo;expérimentation sur un système distribué afin de renforcer la confiance dans la capacité du système à résister à des conditions turbulentes en production [wikipedia \u0026amp; principlesofchaos.org]\n Heureusement, leurs définitions et leurs présentations sont largement plus \u0026ldquo;friendly\u0026rdquo; que la définition de Wikipédia. Parce que (franchement) qui trouve le suivi de production intéressant ??. Je dirais tout ceux qui ont déjà vu la présentation de Christophe et Benjamin par exemple à Devoxx ou à DevopsRex (sur YouTube). Et pourquoi pas vous, si vous ne les avez jamais rencontrés ?\nMa citation préférée de cette keynote est tirée d\u0026rsquo;un ouvrage de Chaos Engineering, Building Confidence in System Behavior through Experiments (ebook disponible : https://www.oreilly.com/webops-perf/free/chaos-engineering.csp)\n “Introducing Chaos is not the best way to meet your new colleagues, though it is the fastest.” Nora Jones (@nora_js)\n 4 Meetups, un seul choix possible Comme c\u0026rsquo;est un meta-meetup, il y avait donc un choix à faire entre l\u0026rsquo;une des 4 associations. A l\u0026rsquo;image d\u0026rsquo;un hackathon, chacun leur tour a fait un \u0026ldquo;pitch\u0026rdquo; pour attirer évidement le plus de monde. Voici ce qu\u0026rsquo;il y avait au programme de la soirée :\n la communauté Docker proposait sur le thème de \u0026ldquo;Container everywhere\u0026rdquo; un tour d\u0026rsquo;horizon des nouveautés sur les containers dans Azure pour Kubernetes, vous étiez convié à déployer et maintenir un Kubernetes sur le long terme et sur un BareMetal avec kubespray (http://kubespray.io/) le GDG (Google Developper Group) et la communauté serverless proposait une présentation de la solution Serverless de google : les Cloud Functions (https://cloud.google.com/functions/) un workshop Choas Engineering, un atelier interactif et colaboratif qui avait pour thème \u0026ldquo;Incident \u0026amp; Experimentation\u0026rdquo;  Meetup Chaos Engineering #5 : Incident \u0026amp; Experimentation Pour ce cinquième rendez-vous, ils nous ont invité dans les coulisses de l\u0026rsquo;organisation d\u0026rsquo;un jeu qui s\u0026rsquo;appelle le \u0026ldquo;day of chaos\u0026rdquo;. Les règles sont assez simples : \u0026ldquo;Toutes les 30 minutes, des exploitants (ou OPS) simulaient des pannes en pré-production. Les équipes de DEV obtenaient des points en fonction des détections, des diagnostics et des résolutions\u0026rdquo;. Ce type d’événement gamifié permet d’initier les équipes de développement au concept de résilience et au suivi de production.\nNous avons donc été mis dans la peau des exploitants qui doivent trouver des pannes à proposer à nos équipes de DEV. On a donc travaillé sur des pannes réelles et sérieuses qui sont apparues chez les participants.\nLe déroulement est assez simple : en petit groupes de travail, on a commencé par identifier une panne qui a causé de gros problèmes parce que l\u0026rsquo;application n\u0026rsquo;était pas assez résiliente. Pour nos incidents, il a fallu remplir un premier template. On a donné un \u0026ldquo;petit nom\u0026rdquo; à nos pannes parce que c\u0026rsquo;est tout de suite plus amusant de travailler sur \u0026ldquo;VM Vampire\u0026rdquo; que sur \u0026ldquo;une VM consomme toute les ressources et reboote en boucle\u0026rdquo;. On a détaillé le contexte, l\u0026rsquo;environnement technique, les actions qui avaient été prise à la suite de la panne et indiqué si oui ou non nous l\u0026rsquo;avions résolu.\nDans l\u0026rsquo;état d\u0026rsquo;esprit du Chaos Engineering, il faut expérimenter la panne à reproduire ! Alors, ce fut la seconde étape \u0026ldquo;Expérimentation\u0026rdquo;. Toujours sur le même principe, un petit groupe découvre la panne et propose une action à mettre en place. Nous avons commencé par la base : avoir un but, savoir qui sera impacté ? Quoi mesurer et comment le monitorer. On a également pris en compte la cible et le résultat attendu car toutes actions doivent avoir une raison valable ! Le chaos engineering ce n\u0026rsquo;est pas seulement pour s\u0026rsquo;amuser à démolir un système informatique, c\u0026rsquo;est trouver les solutions pour devenir tolérant aux pannes Comme toute les bonnes choses ont une fin, on a fait un tour de table pour présenter les \u0026ldquo;Incidents \u0026amp; expérimentations\u0026rdquo; des tous les groupes de travail. Si vous voulez la synthèse, venez les voir au prochain meetup Chaos Engineering.\nSuivez-les\u0026hellip; Vous pouvez les retrouver à Agile France le 15 et 16 juin ou lors du prochain meetup de Septembre pour parler des patterns de résilliance! Il y a également un groupe sur Medium à suivre (https://medium.com/paris-chaos-engineering-community) et un slack (https://days-of-chaos.slack.fr)\nVous avez aimé cet article ? Découvrez ou redécouvrez l’autre épisode de la série « Chaos Engineering » :\nLe chaos engineering (https://blog.talanlabs.com/le-chaos-engineering/)\n","date":"Jun 4, 2018","href":"https://blog.talanlabs.com/devops-night-3-le-meta-meetup-devops/","kind":"page","labs":null,"tags":["chaos engineering","DevOps","meetup"],"title":"DEVOPS NIGHT #3 : le meta-meetup DevOps"},{"category":null,"content":"Cette année Vivatech organise 4 hackathons TechCrunch! Un vainqueur sera désigné pour chacun des 4 hackathons, et sur les 4 vainqueurs, un grand vainqueur sera sélectionné !\nJ’ai la chance d’être invité en tant que mentor pour le hackathon Gefco/Talan dont le thème est \u0026ldquo;la mobilité de demain\u0026rdquo;. Donc ma mission sera d\u0026rsquo;aider les équipes à présenter leurs idées au mieux. Et nous serons une dizaine de Gefco et de Talan pour vous aider à faire émerger vos idées.\nMon objectif dans ce Hackathon? C\u0026rsquo;est la première fois que je serais présent à un hackathon et que mon job ne sera pas de coder\u0026hellip; Donc mon objectif \u0026lsquo;perso\u0026rsquo; va être d\u0026rsquo;aider les participants du hackathon Gefco/Talan pour que l\u0026rsquo;un d\u0026rsquo;entre eux devienne le grand vainqueur !\nSi ce hackathon sur la mobilité de demain vous intéresse, c\u0026rsquo;est par ici :\n là : https://vivatechnology.com/techcrunch-hackathon-vivatech/  ou là : https://techcrunch.com/events/techcrunch-hackathon-at-vivatech/    Alors je vous donnes rendez-vous Vendredi en début d\u0026rsquo;après-midi Porte de Versaille ! On va coder et on va gagner !\n","date":"May 22, 2018","href":"https://blog.talanlabs.com/talanvivatech/","kind":"page","labs":null,"tags":["hackathon","vivatech"],"title":"Talan@Vivatech : Hackathon"},{"category":null,"content":"Vous êtes développeur ou CTO et travaillez sur une application legacy, souffrant de nombreux défauts et bugs ? Retrouvez le chemin vers l’efficacité en mettant en place quelques bonnes pratiques, comme la revue de code.\nNous avons déjà vu lors d’un précédent article que la mise en place d’une plateforme d’intégration continue permettait d’améliorer la qualité de l’application. Cette fois, nous allons voir comment les développeurs peuvent, avec un peu de rigueur et un minimum de temps à investir, améliorer la qualité à court et à long terme.\nJ’ai eu l’occasion, à travers mes différentes missions, de mettre en place ou d’affiner le procédé de revue de code, quelque soit la taille et la qualité de l’application à mon arrivée.\nMais… Qu’est ce qu’une revue de code ? La revue de code est un processus outillé qui doit se faire juste avant l’intégration du développement à la branche principale. Cette revue de code fait généralement partie de la Definition of Done. De nombreux outils facilitent l’opération de revue de code.\nComment mettre en place un système de revue de code ? Afin de pouvoir effectuer la revue de code de façon efficace et utile, je recommande de :\n partager un code style au sein de toute l’équipe ou des équipes qui interviennent sur l’application_👉 Partager un code style permet de minimiser le nombre de lignes modifiées inutilement et automatiquement par le formateur. Ainsi, seules les lignes véritablement modifiées lors des développeurs seront mises en évidence, laissant moins de chances à une coquille de passer à travers le filet ;)_  laisser n’importe quel développeur de l’équipe de faire la revue, peu importe sa séniorité ou son ancienneté sur le projet. Attention toutefois au cas où un junior relit le code d’un autre junior car les gains ne sont pas au rendez-vous et cela peut-même être contre-productif. Dans le cas où un junior relit le code, il peut être nécessaire qu’un développeur plus expérimenté l’assiste, ou fasse une dernière validation après lui, afin de valider la pertinence des remarques et la qualité du code_👉 Même en tant que développeur senior, votre code a tout intérêt à être revu par un autre développeur. Cela encourage la collaboration, le mentoring et le partage de la connaissance_ mettre en place un environnement de review    La gestion des branches, un pré-requis pour réussir Pour commencer, il faut choisir un gestionnaire de sources facilitant cette pratique. À ces fins, je ne peux que recommander un outil tel que Git qui allège la gestion d’une multitude de branches.\nLe procédé de gestion GitLab Flow, basé pour cette partie sur celui de GitHub, utilise des branches de fonctionnalités.\nCe sont ces branches qui doivent être revues avant d’être intégrées à la branche master.\nWorkflow Avec le fonctionnement en feature branches et merge request (ou pull request sur GitHub et d’autres), la revue du code se fait plus facilement car toutes les remontées, échanges et questions sont localisées au même endroit. Il est également possible d’inclure les remontées automatiques d’outils de la plateforme d’intégration continue.\nLors de la création de la merge request, il est important que l’on retrouve le n° de la fiche d’évolution, correctif, etc. dans le nom de la branche. Le titre ou la description de la merge request doit être assez explicite pour que le relecteur puisse se placer dans le contexte.\nUne checklist peut dans un premier temps permettre de ne rien oublier. Cette checklist, constituée par l\u0026rsquo;équipe, doit être vivante et évoluer avec le temps :\n si elle n’est pas utilisée, débarrassez-vous en  si des items ne sont plus d’actualité, retirez-les essayez de garder la liste courte et priorisée    💡 Quand c’est possible, il peut être intéressant de transposer certaines règles sur SonarQube ou tout autre outil d’analyse du code et par conséquent retirer la règle de la checklist.\nAttention ⚠ Il est important que la phase de revue ne soit pas un goulot d’étranglement : lorsqu’un développement est soumis à revue, un autre développeur doit effectuer la revue au plus tôt. Il ne s’arrête pas dans son propre développement pour ne pas le perturber, mais n’en reprend pas d’autre tant qu’il y a un développement à revoir. On évite ainsi que de trop nombreux développements soient en attente, en bloquent d’autres, et amènent à résoudre trop de conflits.\nSi cette dernière recommandation n’est pas suivie, une situation désagréable se produit souvent en fin de sprint : des fonctionnalités ne sont pas livrées car elles ont été bloquées trop longtemps.\nPrivilégiez la livraison de fonctionnalités pour avoir un retour plus rapide.\nLe retour sur investissement est considérable Certes, effectuer une revue de code peut s’avérer coûteuse en temps, mais c’est rentable :\n partage des connaissances fonctionnelles et techniques  code plus homogène des bugs corrigés avant même qu’ils soient livrés réduire les failles de sécurité    À long terme, le temps passé est rapidement amorti car revenir sur du code sera plus facile. À court terme, cela permet de corriger efficacement les bugs, identifier et anticiper des failles, etc. mais aussi être force de proposition pour proposer de nouveaux développements.\nL’ensemble de l’équipe s’approprie l’application.\nComment effectuer une revue de code ? Dans un premier temps, je conseille d’effectuer la revue de code en lisant la fiche à l’origine du besoin pour connaître la raison du développement. Il est très intéressant d’imaginer comment vous, vous auriez effectué ce développement, notamment pour s’améliorer, ou proposer votre idée du développement. Il est plus facile de voir des failles, des oublis ou même de vous faire penser à de nouvelles choses en procédant ainsi.\nDans un second temps, je conseille de lire les changements et de comprendre les raisons qui ont poussé à opter pour telle ou telle stratégie, sans aide ou informations supplémentaires de la part du développeur. Si vous ne comprenez pas le code, c’est qu’il n’est pas assez clair ou logique.\nIl peut être intéressant d’échanger avec le développeur directement pour lui faire des retours mais, pour ma part, je préfère effectuer la revue de code principalement sur la merge request pour plusieurs raisons :\n laisser une trace, pour que ce ne soit pas oublié lors de la correction (GitLab permet de marquer un retour comme “fait”)  être vu par d’autres membres de l’équipe qui pourraient continuer la revue de code, et éviter ainsi de les priver de discussions importantes ouvrir un fil d’échanges    Si une ligne vous semble étrange ou que vous ne la comprenez-pas, n’hésitez pas à questionner l’auteur. La communication est la clé du succès d’une équipe.\nEnvironnement de revue Lors de mes derniers projets, j’ai eu l’occasion de mettre en place un environnement de revue, sur lequel sont déployées les différentes branches fonctionnelles. L’application ainsi déployée correspond à la nouvelle fonctionnalité développée et/ou au bug corrigé, facilitant son test. Cet environnement peut par conséquent être accessible par un point d’entrée différent pour chaque branche fonctionnelle. Exemple : la branche “user-management” est déployée sur https://user-management.review.myapp.com et GitLab en fait mention sur la merge request\nCet environnement de revue est largement bénéfique à l’équipe : il permet de gagner un temps considérable. Le relecteur ne perd pas son temps à changer son espace de travail, à compiler, déployer, etc. L’environnement est systématiquement créé de la même façon.\n💡 Lors de la création de l’environnement, il est agréable et utile d’y avoir des données pré-initialisées. Avec Liquibase, il est possible de définir un contexte “review” qui permet de jouer des scripts pour ces environnements spécifiquement.\nJe recommande l’utilisation de Docker, avec éventuellement Kubernetes, idéal pour automatiser le déploiement d’un environnement rapidement et de façon identique.\nEt vous, avez-vous mis en place un système de revue de code sur votre projet ? Avez-vous constaté une amélioration flagrante de la qualité de l’application ?\nQuelques liens Auto DevOps Livrer de la qualité tout en restant productif\n","date":"May 7, 2018","href":"https://blog.talanlabs.com/travailler-efficacement-en-equipe/","kind":"page","labs":null,"tags":["Intégration Continue","Revue de code"],"title":"Travailler efficacement en équipe avec la revue de code"},{"category":null,"content":"Vous êtes CTO ou développeur et soucieux de livrer de la qualité tout en restant productif ? C’est possible ! La mise en place d’une plateforme d’intégration continue (PIC) est un excellent début à cela.\nDans cet article, je vais essayer de vous faire mon retour d’expérience sur l’intégration continue, telle que j’ai eu l’occasion de la mettre en place ou vécue sur des projets de toute taille.\n Mais… Qu’est-ce que l’intégration continue ? Décrite maintes-fois et mise en place depuis de nombreuses années, l\u0026rsquo;intégration continue évolue constamment.\nAu départ, l\u0026rsquo;intégration continue se cantonnait à compiler de façon régulière du code afin de valider son bon fonctionnement. De cette façon, l’ajout de code au gestionnaire de sources (Git, ou SVN à défaut) se fait sur de petits changements validés par toute une chaîne et sur lesquels les développeurs ont plus rapidement du feedback.\nL’intégration continue intègre les tests; qu\u0026rsquo;ils soient unitaires, d\u0026rsquo;intégration, end-to-end, etc. mais également des outils d’analyse de code, tels que SonarQube. Tous les tests sont joués et tout test en échec bloque le processus. Des remontées bloquantes ou majeures de SonarQube peuvent également bloquer le processus.\nUne chaîne d’intégration continue classique est composée de plusieurs étapes automatisées :\n Build, vérifie entre autre que le code compile  Tests et analyse du code Packaging, pour mettre les artifacts à disposition Déploiement, sur l’environnement de développement Smoke tests, qui permettent de vérifier que le déploiement s’est bien passé via des premiers tests légers    Comment et quand la mettre en place sur un projet ? Je recommande fortement que la chaîne d’intégration continue se mette en marche après toute soumission de code.\nNéanmoins, sur certaines applications legacy, la construction des artifacts (le build) peut prendre plusieurs heures et il devient dès lors plus compliqué de déclencher toute la suite de façon systématique.\nD’expérience, quand la plateforme d’intégration continue est mise en place tardivement sur un projet, il est plus compliqué de la rendre systématique. On aura alors recours à divers procédés pour gagner du temps : plus de traitements en parallèle, l’utilisation d’images Docker rapides à mettre en place, etc.\nAfin d’éviter d’en arriver là, je conseille de mettre en place une plateforme d’intégration continue dès que possible dans tout projet et de la surveiller. Une architecture de type micro-services permet de garder une taille d’application raisonnable et donc de minimiser les temps requis à construire, valider et déployer.\nObjectifs et gains d’une telle plateforme Au sein d’une équipe, disposer d’une PIC, c’est garantir que toute modification de code ait traversé les mêmes étapes. Nous minimisons ainsi les risques de défaillance, même si le risque zéro n’existe pas encore.\nUn des objectifs est d’automatiser les tâches répétitives à faible valeur ajoutée, ce qui permet à l\u0026rsquo;équipe de développement de dégager davantage de temps et à se consacrer au côté fonctionnel et créatif.\nPour aller encore plus loin dans l’automatisation, il est possible d’étendre l’intégration continue avec du déploiement automatisé sur les environnements de développement ou de démo. Il est rapidement plus rentable de passer quelques heures à mettre en place un système de déploiement automatisé plutôt que de le faire manuellement en quelques minutes régulièrement.\nL’objectif ultime serait de déployer l’application en production de façon totalement automatique, mais cela nécessite la mise en place d’une couche supplémentaire : les tests d’acceptation, validés sur un environnement de qualification iso-prod. On parle alors de déploiement continu.\nAinsi, sur l’une de mes missions, le déploiement sur l’environnement de développement se faisait suite à chaque modification de la branche master, après avoir traversé toutes les étapes de validation. À la fin du sprint, l’application était livrée sur l’environnement de démo via un simple clic dans une interface (GitLab).\nLa qualité par les retours de l’environnement de production Pour obtenir une application de qualité, il est indispensable d’obtenir des retours de la production aussi souvent que possible.\nC’est notamment l’objectif de l’approche DevOps.\nCette pratique culturelle permet de mettre l’accent sur l’agilité et sur des cycles courts de livraison, particulièrement adaptée dans une approche Agile/Scrum.\nL’approche DevOps permet d’améliorer la qualité de l’application en redonnant une partie de la responsabilité du déploiement et d’exploitation à l’équipe de développement, plus à même d’intervenir rapidement et efficacement.\n Prenons l’image d’un restaurant : À qui feriez-vous le plus confiance pour vous donner la liste des ingrédients utilisés à la confection d’un plat ? Au cuisinier, ou au serveur ? En revanche, le serveur est le plus à même à recueillir la commande, à servir la bonne personne. Son rôle ne peut pas être mis de côté.\n C’est l’équipe de développement qui sait de quoi est constituée l’application, comment elle fonctionne, de quoi elle a besoin. L’équipe opérationnelle ne devrait pas être une frontière imperméable aux remontées utilisateurs.\nIl est indispensable de renforcer les liens entre développeurs et opérationnels au plus tôt du cycle de vie de l’application.\nL’approche DevOps, c’est bien plus que de renforcer les liens entre l’équipe de développement et l’équipe opérationnelle, c’est aussi retirer les silos traditionnels entre chaque spécialité et permettre à chacun de collaborer plus étroitement, de renforcer la communication, etc.\nDe ce fait, l’approche permet de réduire :\n le time-to-market, livrer de la valeur plus rapidement, et mieux prioriser les prochains développements  les délais entre les correctifs le taux d’échec    Bien que l’approche DevOps n’inclue pas la plateforme d’intégration continue, une synergie positive se dégage de la mise en place de ces deux pratiques.\nLes outils à la rescousse Pour être efficace, il faut bien s’outiller, c’est-à-dire mettre à disposition les bons outils et les utiliser systématiquement. Par bons outils, j’entends des outils adaptés à l’équipe et au contexte. GitLab, avec la surcouche GitLab-CI me paraît particulièrement adapté, mais d’autres peuvent également faire l’affaire.\n Attention, l’excès d’outils nuit à la bonne productivité. Ils doivent être maintenables, robustes et stables.\n Les liens inter-outils apportent un réel plus dans cette plateforme. Il est par exemple possible de faire figurer les anomalies SonarQube sur GitLab, qui sont alors inclus dans une demande de merge.\nFaîtes profiter les autres équipes en partageant votre PIC ! Cette PIC évolue et dans un contexte agile il est fréquent de revenir dessus pour l’affiner, ajouter des règles et vérifications.\nLes outils c’est bien, mais ils ne suffisent pas Le facteur humain ne doit pas être sous-estimé. La mise en place d’une plateforme d’intégration continue doit se faire en accompagnant les développeurs au changement, surtout s’il s’agit d’une équipe qui n’a jamais fonctionné avec.\nC’est un enseignement que j’ai pu tirer suite à l’une de mes missions, alors que mon rôle était de mettre en place une plateforme d’intégration continue dans une équipe qui n’utilisait même pas encore de gestionnaire de dépendances.\nAfin de maximiser les chances de réussir, il faut au minimum :\n partager une façon de gérer les branches  favoriser les échanges (cf Scrum), à l’oral si possible ou via un outil de messagerie instantané partager les bonnes pratiques mettre en place un système de revue de code dans l’équipe (j’y consacrerai d’ailleurs un prochain article)    La PIC m’a changé la vie, et vous, qu’en pensez-vous ? De quoi se compose votre PIC ? Quels sont les limitations que vous rencontrez ?\nQuelques liens Gérer les branches en utilisant le GitLab Flow Plugin SonarQube pour remonter les anomalies sur GitLab, développé par un collègue\n","date":"Apr 25, 2018","href":"https://blog.talanlabs.com/de-la-qualite-en-restant-productif/","kind":"page","labs":null,"tags":["DevOps","Intégration Continue"],"title":"Livrer de la qualité tout en restant productif"},{"category":null,"content":"Habitué comme beaucoup aux sprints de deux semaines, j’ai pu me rendre compte de l’impact qu’un sprint d’une semaine peut avoir sur le produit et sur l’équipe. Pensez-vous que vous devriez y passer aussi ?\nDéfinition du sprint Voici la définition du Sprint, telle que présentée dans le Scrum Guide :\n Le cœur de Scrum est le Sprint, qui a une boîte de temps (timebox), une durée, d’un mois ou moins au cours de laquelle un Incrément Produit « Fini » fonctionnel et potentiellement publiable est créé.\n Comme je l’ai expliqué dans mon article Scrum : quel est le bon jour pour commencer un sprint ?, le guide nous laisse libre de faire des sprints d’une minute, à quatre semaines. Nous pouvons même faire des sprints de 3, 13 ou 20 jours –mais ce n’est vraiment pas pratique et je le déconseille (voir l’article).\nOn en est où ? Vincent Barrier, CEO de Kagilum éditant le logiciel IceScrum (que je conseille d’essayer), m’a très aimablement fournit la répartition de la durée des sprints parmi les utilisateurs d’IceScrum (en mars 2018) :\n 15% -\u0026gt; 1 semaine\n80% -\u0026gt; 2 semaines\n4% -\u0026gt; 3 semaines\n1% -\u0026gt; autres\n On se rends bien compte de l’écrasante majorité du sprint de 2 semaines.\nLes logiciels de gestion de projet (comme IceScrum ou Jira), proposent d’ailleurs la durée de deux semaines, par défaut. Est-ce que cela influence les équipes à ne pas y réfléchir et à choisir la valeur par défaut ? Possible.\nA priori sur le sprint d’une semaine Lorsque j’ai proposé à une nouvelle équipe de développer notre produit en sprints d’une semaine, il y a eu des inquiétudes.\n Nous n’aurons pas le temps d’avoir un produit fini à chaque itération  Nous passerons trop de temps en cérémonies et préparations Nous ne pourrons pas finir de grosses fonctionnalités    Ce à quoi j’ai répondu :\n Les fonctionnalités embarquées seront moins nombreuses et mieux découpées  Les cérémonies peuvent (et doivent) s’adapter. Il y en a autant, mais elles durent moins longtemps L’intérêt est justement de mieux découper les grosses fonctionnalités    Le résultat Comme le dit Rob Galanakis, si mon équipe travaille en sprints d’une semaine, c’est pour avoir du feedback très rapidement. Si vous travaillez en sprints de trois semaines, alors que nous travaillons sur des sprints d’une semaine, nous avons l’occasion d’avoir trois plus de feedback et trois fois plus d’opportunités d’échouer, d’apprendre et de nous améliorer.\nAu lieu de discuter si nous devrions essayer quelque chose, nous le testons immédiatement, empiriquement. Cela permet la culture de l’échec et d’en réduire considérablement les impacts. Nous éliminons l’effet tunnel car nous acceptons de considérer les éléments du backlog produit (avant leur arrivée dans un sprint donc), comme des options pouvant être remises en cause.\n Les éléments du backlog ne sont que des options possibles, tant qu’ils ne sont pas embarqués dans un sprint\n Pour le product owner, il est presque impossible de négocier l’entrée dans le sprint d’éléments “non-prêts”. Sur un sprint d’un mois, il est tentant d’ajouter une fonctionnalité dont les maquettes doivent arriver “dans quelques jours”. Sur une semaine, le risque est trop grand et il est facile de se dire qu’on peut attendre le prochain sprint.\nPour une équipe qui n’est pas encore à l’aise avec Scrum, c’est aussi un bien meilleur moyen d’apprendre. Plus l’itération est courte, plus vite l’équipe acquiert le processus.\n Si l’agilité repose sur les feedback, alors réduire de moitié la durée de vos sprints vous donne le double de feedback et de chances de vous améliorer. Vous pouvez essayer quelque chose pendant une semaine, vous rendre compte que ça ne fonctionne pas et partir sur autre chose sans mettre une release en péril. Pour un sprint deux fois plus long, c’est deux fois plus risqué.\n Rob Galanakis, CCP Games\nL’impact n’est pas le même pour tous Pour le Scrum Master, le coût de la rétrospective par exemple est au moins le même. L’évènement en lui même dure forcément moins longtemps, mais la préparation nécessite d’être plus précise, pour faire ressortir autant d’améliorations, en moins de temps.\nLe Product Owner se doit d’être particulièrement attentif aux besoins des développeurs. Ils n’ont pas le temps de se dire “on verra ça demain”. “Demain”, c’est 20% du sprint en moins.\nAu final, c’est peut-être l’équipe de développement qui est la moins impactée négativement. Nous ne lui demandons pas de travailler plus vite; Au contraire, nous pouvons lui laisser plus de libertés.\n I\u0026rsquo;m a big fan of moving to one-week sprints It forces the team to size stories small, eliminate waste in their meetings, work collaboratively at the last responsible moment. There\u0026rsquo;s a heap of goodness. … 1/x\n— Allen Holub (@allenholub) March 14, 2018\n Conclusion La durée d’un sprint ne se choisit pas parce que c’est la mode, ou simplement parce qu’on est habitué comme ça.\nÀ chaque nouveau projet, vous devriez vous poser la question sérieusement et la soumettre au reste de l’équipe.\n Pouvons-nous avoir des retours utilisateurs chaque semaine ?  La définition du “fini” nous permet-elle de livrer chaque semaine ?    En effet, l’intérêt premier du cycle court est d’avoir des retours utilisateurs. Si ce n’est pas possible pour vous d’en avoir aussi régulièrement, il faut réfléchir à la pertinence d’aller vers un sprint court.\nCertains le font déjà sans oser se l’avouer Beaucoup d’équipes qui font des sprints de 2 semaines ou plus organisent “un point mi-sprint”, ou par semaine. Pourquoi ? Pour se coordonner et pour se remettre en question. C’est une bonne alternative au sprint plus court, mais il serait bon de se demander pourquoi ne pas le formaliser comme tel.\nOn peut faire plus court Pourquoi ne pas descendre à des sprints d’une journée ? Une multitude de sprints, avec des points réguliers (toute les semaines par exemple), pour se remettre en question et s’améliorer comme on ferait une rétrospective “produit” tous les 6 sprints par exemple.\nEt vous ? Comme on vient de le voir, bien que ce soit plus intense pour le Scrum Master, le sprint d’une semaine a bien des vertus :\n Du feedback très rapidement  Permet la culture de l’échec et d’en réduire les impacts Difficile de négocier l’entrée dans le sprint d’éléments “non-prêts” Plus l’itération est courte, plus vite l’équipe acquiert le processus    Avez-vous déjà essayé le sprint d’une semaine ? D’un jour ? L’utilisez-vous déjà ? Avec quels résultats ?\n","date":"Apr 20, 2018","href":"https://blog.talanlabs.com/sprint-une-semaine/","kind":"page","labs":null,"tags":["Scrum","semaine","sprint"],"title":"Devez-vous passer au Sprint d’une semaine ?"},{"category":null,"content":"Comment les principes agiles peuvent-ils aider à cadrer et à mesurer le succès ou l\u0026rsquo;échec, tout en en tirant des leçons ?\nLa mesure, principe dévié et pourtant essentiel Les indicateurs souvent utilisés au sein d\u0026rsquo;une équipe agile deviennent inutiles au moment même où ceux-ci sont utilisés à des fins de contrôle de la performance humaine :\n La vélocité doit être constante ? Elle le sera. Les éléments du backlog seront alors évalués à la hausse ou à la baisse pour répondre à l\u0026rsquo;attente de la hiérarchie  La vélocité doit augmenter tous les trimestres ? Elle augmentera. Une même fonctionnalité estimée à \u0026ldquo;3\u0026rdquo; d\u0026rsquo;effort en sprint 1, sera estimée à \u0026ldquo;5\u0026rdquo; en sprint 10 le trimestre suivant, toujours pour répondre à l\u0026rsquo;attente de la hiérarchie Pas de dette technique ? Elle sera invisible, toujours pour répondre à l\u0026rsquo;attente de la hiérarchie, comme je l\u0026rsquo;expliquais il y a quelques semaines Le nombre de fonctionnalités livrées doit être constant ? Il le sera, toujours pour répondre à l\u0026rsquo;attente de la hiérarchie, grâce peut-être à la shadow-velocité (voir mon guide à ce sujet)    Les indicateurs sont pourtant essentiels pour l\u0026rsquo;équipe et doivent donc être utilisés uniquement dans un but d'auto-évaluation.\nEt la mesure de l\u0026rsquo;impact, alors ? Les mesures d\u0026rsquo;auto-évaluation de l\u0026rsquo;équipe n\u0026rsquo;indiquent pas si la fonctionnalité réalisée est utile ou non, aussi bien faite soit-elle.\nPour mesurer l\u0026rsquo;impact réel d\u0026rsquo;une fonctionnalité, le Product Owner dispose d\u0026rsquo;un outil fiable et précis : la sonde.\nLa sonde est un élément technique rajouté à une fonctionnalité afin de collecter les données sur son utilisation.\nAméliorer Fort de ces données, il y a ensuite des actions à prendre. Si une fonctionnalité n\u0026rsquo;est pas ou peu utilisée, il y a plusieurs raisons possibles :\n  Est-ce vraiment un besoin des utilisateurs ? Si l\u0026rsquo;idée est née dans la tête d\u0026rsquo;un marketeur sans une étude préalable, il est possible que l\u0026rsquo;utilisateur n\u0026rsquo;ait pas encore l\u0026rsquo;utilité de la fonctionnalité ou la capacité à l\u0026rsquo;exploiter\n  Les utilisateurs sont-ils au courant ? La fonctionnalité a-t-elle été annoncée et expliquée ?\n  **La fonctionnalité est-elle accessible à la bonne cible ? **Si une fonctionnalité est limité à un type d\u0026rsquo;utilisateur précis, il est important que les indicateurs ne prennent en compte que les comportements de la cible.\n  **La fonctionnalité est-elle utilisable ? **Les utilisateurs savent-ils utiliser la fonctionnalité ? Peut-être est-elle trop compliquée ?\n  L\u0026rsquo;échec est important et nécessaire. Si nous savons en tirer une leçon, nous pouvons alors agir.\nPar exemple nous pouvons mettre en place une action marketing pour annoncer et expliquer la fonctionnalité. Nous pouvons aussi organiser des entretiens utilisateur pour mieux les comprendre.\nIl faut aussi savoir tuer une fonctionnalité. Si celle-ci n\u0026rsquo;apporte rien, elle devient une source inutile de code à maintenir, sans valeur.\nS\u0026rsquo;améliorer Grâce à ces indicateurs, il est alors non seulement possible d\u0026rsquo;améliorer le produit, mais aussi de s\u0026rsquo;améliorer soi-même.\nSi la campagne marketing n\u0026rsquo;a pas fonctionné, il faut pouvoir se remettre en question pour ne pas rater la prochaine.\nSi le besoin n\u0026rsquo;est pas là, comment faire pour éviter de lancer en production un élément inutile ?\nSi les sondes n\u0026rsquo;ont pas permis de comprendre pourquoi la fonctionnalité a eu peu de succès, alors que pouvons-nous en tirer comme conclusion ?\nSi vous souhaitez en savoir plus et découvrir d\u0026rsquo;autres outils pour compléter les sondes, vous pouvez regarder du côté de l\u0026rsquo;accompagnement du changement (un prochain article y sera consacré)\n","date":"Apr 5, 2018","href":"https://blog.talanlabs.com/pas-mesurer-pas-ameliorer/","kind":"page","labs":null,"tags":["Agile","Scrum"],"title":"Ce qui n’est pas mesuré ne peut être amélioré"},{"category":null,"content":"Il existe un mauvais comportement, qui est souvent présent dans les équipes qui s\u0026rsquo;engagent dans un sprint avec trop de points —ou trop de stories. En effet, certains membres peuvent ressentir le besoin de devoir faire des heures supplémentaires pour atteindre l\u0026rsquo;engagement qu\u0026rsquo;ils ont pris dans les prévisions. Ils font de la shadow-velocity.\nQu\u0026rsquo;est-ce que la shadow-velocity ? Shadow-velocity est un terme qui m\u0026rsquo;est venu en écrivant \u0026ldquo;Scrum : quel est le bon jour pour commencer un sprint ?\u0026rdquo;. Il décrit le fait qu\u0026rsquo;un (ou plusieurs) membre(s) d\u0026rsquo;une équipe accompli du travail non planifié pendant la semaine, le week-end ou même pendant ses vacances.\nPourquoi ça arrive et pourquoi ça doit s\u0026rsquo;arrêter ? Les raisons pour ce travail supplémentaire vont de \u0026ldquo;je veux commencer/finir une story\u0026rdquo; à \u0026ldquo;je corrige vite-fais un bug sous le manteau\u0026rdquo;.\nQuoi qu\u0026rsquo;il arrive, il existe toujours une bonne raison pour commencer.\nPour garder une vélocité plate Pourquoi ? Pourquoi l\u0026rsquo;équipe voudrait-elle avoir la même vélocité que pour le sprint précédent ? Chaque sprint est différent. L\u0026rsquo;équipe ne travaille pas sur les mêmes tâches et elle a appris tant que chose depuis les derniers sprints, c\u0026rsquo;est un nouveau monde !\nPersonne ne demande à l\u0026rsquo;équipe de garder une vélocité constante de sprint en sprint. Pourtant, si quelqu\u0026rsquo;un le fait, dites-leur qu'un sprint est unique et demander une vélocité constante ne peut que pénaliser le produit.\nPour compenser une prévision incorrecte Si les membres de l\u0026rsquo;équipe ressentent le besoin de compenser une prévision, c\u0026rsquo;est un problème qui doit être discuté avec toute l\u0026rsquo;équipe, dont le Product Owner et même pourquoi pas des parties prenantes. La rétrospective est un bon moment pour relever ce problème, mais ce n\u0026rsquo;est pas le seul moment.\nLes prévisions doivent être ajustées dès que nécessaire. Ajouter ou retirer des story-points est permis et ne devrait pas être une punis.\nEn 2011, le Scrum Guide a remplacé le terme \u0026ldquo;engagement\u0026rdquo; par \u0026ldquo;prédiction\u0026rdquo; en ce qui concerne le travail sélectionné pour un sprint, et ce, pour une bonne raison.\n Un Sprint Backlog est assez complexe pour que l\u0026rsquo;incertitude soit toujours présent, et le sens commun nous dit que nous ne pouvons pas promettre ce que nous ne sommes pas sûr d\u0026rsquo;être capable de livrer. Lorsque nous utilisons le mot \u0026ldquo;engagement\u0026rdquo;, nous pouvons facilement être biaisé devant cette façon de penser devoir-obligation-promesse.\n D\u0026rsquo;un autre côté, l\u0026rsquo;alternative \u0026ldquo;prévision\u0026rdquo; choisie consiste à faire des hypothèses basées sur des informations et des preuves fiables. Ceci est beaucoup plus proche du fonctionnement d\u0026rsquo;une équipe Scrum expérimentée.\n[ \u0026hellip; ]\nIl n\u0026rsquo;est pas inhabituel (ou déraisonnable, franchement) pour les gens du métier d\u0026rsquo;apprendre que l\u0026rsquo;équipe de développement s\u0026rsquo;est engagée à livrer une liste d\u0026rsquo;articles en souffrance et à la prendre au pied de la lettre.\nScrum.org blog\nOn ne fait pas confiance à l\u0026rsquo;équipe #super-munchkin Le « syndrome du superhero ».\nLe seul capable de corriger tel problème, parce que \u0026hellip; \u0026hellip; il/elle est le/la meilleur(e), vraiment. \u0026hellip; il/elle a tout l\u0026rsquo;historique de ce vieux-projet-de-dix-ans. \u0026hellip; il/elle ne pense pas que quelqu\u0026rsquo;un d\u0026rsquo;autre peut le faire aussi bien.\nChoisissez une réponse, ou ajoutez la vôtre.\nhttps://twitter.com/cog_g/status/944687246782947329\nEt ensuite quoi ? Personne ne sera capable de remplacer ce superhero. Le produit dépends complètement de cette personne. Que se passe-t-il si il/elle gagne au Loto un jour et décide de tout quitter sans attendre ?\nCette personne met en péril tout le travail de l\u0026rsquo;équipe. **Il faut vraiment laisser tous les membres apprendre et rater, **aimer —ou détester— le produit. C\u0026rsquo;est agir en équipe.\nIl/elle à simplement, le temps Parfois, l\u0026rsquo;équipe estime les stories avec une valeur trop élevée pour ce qu\u0026rsquo;elles sont réellement. Dans ce cas, lorsque les membres de l\u0026rsquo;équipe terminent le sprint plus tôt que prévu et disposent de temps libre, ils se peuvent se dire pourquoi ne pas corriger un bug de longue durée, un environnement ou ajouter une petite amélioration à une fonctionnalité ?\nUne telle action n\u0026rsquo;ajouterait ou ne supprimerait pas vraiment de la vélocité, mais en ajuste une ou ajoute une nouvelle tâche pour combler le vide. Si ce travail passe inaperçu, c\u0026rsquo;est un manque de communication qui peut conduire à de futur problèmes.\nAjouter un post-it à ce sujet, parlez à l\u0026rsquo;équipe de ce que vous allez faire.\nVous avez un problème à régler Le problème principal est toujours le même: nous, en tant qu\u0026rsquo;individus, devons répondre aux attentes de notre modèle - nos parents, puis notre mentor, puis notre patron. Et cela nous fait faire de mauvaises choses pour leur plaire.\nLa shadow-velocity passe inaperçu, utilise votre énergie et souvent le sentiment d\u0026rsquo;accomplissement est diminué comparé au travail officiel.\nLa communication pendant ce temps est défectueuse et peut mener à plus de problèmes dus aux changements se produisant sans que l\u0026rsquo;équipe sache.\nLe travail supplémentaire se produit et devrait être visible Comment un Scrum Master peut-il aider l\u0026rsquo;équipe à révéler et éviter la shadow-velocity ?\n Cherchez les commits fait en dehors des heures de travail ou pendant les vacances  Essayer d\u0026rsquo;utiliser les filtres de votre outils de gestion de projet pour extraire une liste des tâches réalisées en dehors des heures Soyez attentif, spécifiquement pendant le Daily Scrum, que chaque membre de l\u0026rsquo;équipe est assigné à au moins une tâche **Posez des questions. **Demandez à la fin du sprint, qui a dû travailler sur des tâches non planifiées et, pourquoi.    N\u0026rsquo;hésitez pas à partager votre expérience de shadow-velocity et comment cela à impacté votre produit.\nArticle publié à l\u0026rsquo;origine sur mon blog\n","date":"Mar 9, 2018","href":"https://blog.talanlabs.com/guide-de-shadow-velocity-scrum-masters/","kind":"page","labs":null,"tags":["Agile","Scrum","vélocité"],"title":"Le guide de la Shadow-Velocity pour les Scrum Masters"},{"category":null,"content":"C\u0026rsquo;est une question qui revient très souvent dans les équipes : doit-on estimer et compter la dette technique dans la vélocité ? La réponse n\u0026rsquo;est pas si simple et en tant que Scrum Master, tous les yeux risquent de se tourner vers vous pour une parole sage.\nLa dette technique ? La dette technique représente toutes ces choses que nous n\u0026rsquo;avons pas voulu –ou pas pu– faire.\nDéveloppeur lutant contre la dette technique\nPar exemple, le Product Owner a accepté que des histoires soient considérées comme Done malgré que les critères d\u0026rsquo;acceptation ne soient pas tous validés (anomalie), ou que tous les critères de la définition du done ne soient pas cochés, ou bien encore que le code génère un bug mineur.\n Une anomalie est créée lorsqu\u0026rsquo;une fonctionnalité ne correspond pas à ce qui avait été défini par le Product Owner, comme un critère d\u0026rsquo;acceptation non valide  Un bug, c\u0026rsquo;est un problème qui est créé par une modification du code, souvent en dehors de son périmètre –par influence– alors que le code en lui même, réponds parfaitement à la fonctionnalité et aux critères d\u0026rsquo;acceptation    Toutes ces choses devront être faites, un jour. L\u0026rsquo;équipe **gagne **donc du temps aujourd\u0026rsquo;hui, pour le payer plus tard. Vous créez de la dette technique.\nCompter la dette technique dans la vélocité Certaines équipes estiment et tiennent compte de cette dette dans leur vélocité. Par exemple, si la vélocité moyenne est de 30 points par Sprint, et que l’équipe s’accorde sur le fait résorber l’équivalent de 10 points de dette, alors, ils devront répartir les 20 points restants dans les histoires fonctionnelles.\nLa vélocité de l’équipe devrait donc rester dans leur moyenne\nLa bonne vélocité et la mauvaise vélocité Si la vélocité reste constante, la valeur fonctionnelle ajoutée au produit, diminue. Il est donc important de garder un moyen de différencier les deux. Comme le cholestérol, il parait qu’il y a un bon et un mauvais; là, c’est pareil, il faut donc se surveiller. Vous pouvez profiter du BurnUp chart pour faire apparaître la différence entre les deux.\nVisualiser la dette contenue dans la vélocité / Répartition de la dette dans la vélocité\nIci, bien que la vélocité reste constante, la dette technique est visible et apporte de l\u0026rsquo;information.\nNe pas compter la dette technique dans la vélocité À l’opposé, il est possible de ne pas comptabiliser la dette technique. L’avantage certain est la mise en lumière rapide et immédiate de son impact (paradoxalement).\n1/ La dette n’est pas estimée Ce cas est plus délicat. Bien que la chute de vélocité sera visible et qu’il est toujours possible d’ajouter une note au BurnUp, le Product Owner ne pourra pas se projeter sur l’impact du remboursement et aura donc des réticences à prioriser dans le flou.\nChute de vélocité, incompréhensible\nNous voyons ici une chute spectaculaire de la vélocité de l’équipe (pointillés bleus). Presque rien_ (de valeur pour l’utilisateur)_ n’a été produit pendant une période –la ligne rouge, plate– mais on ne visualise pas la raison.\n2/ La dette est quand même estimée Il est important de montrer pourquoi la vélocité a chuté. Si vous avez estimé la dette sans la compter dans la vélocité, vous pouvez utiliser le type de BurnUp du précédent paragraphe, mais avec cette fois-ci la ligne de la vélocité. C’est la méthode que je recommande.\nIl est simple de faire le rapport avec la dette\nEstimez et affichez Le Product Owner doit être capable d’avoir une vision du volume de la dette en cours, pour :\n **accepter ou refuser **une nouvelle dette en toute connaissance 2. mesurer la qualité du produit actuel 3. savoir quelle quantité de dette il doit prévoir dans le prochain Sprint 4. prévoir le temps nécessaire pour rembourser le tout 5. comprendre l’équipe  Exemple de visualisation Une plaque Lego© collée au mur. La couleur et la taille des éléments peut être en relation avec le type d’élément (un gros bug rouge, une petite fin d’histoire, une moyenne tâche technique \u0026hellip;).\nLa dette s\u0026rsquo;accumule\nLa taille de la plaque support peut être la limite que l’on se fixe pour accepter la dette. On retire les éléments qui sont remboursés.\nTips : une tâche technique ou un bug peut recouvrir un autre élément, pour exprimer une dépendance\nJenga© (lien) est un jeu de construction. Ici, une brique serait ajoutée à chaque élément de dette, quelque soit sa taille. L’avantage est que la structure devient vite instable de part sa hauteur.\nLa brique de trop \u0026hellip; ?\nMétaphore imparable de l’état du produit, la tour va se mettre à vaciller et peut même s\u0026rsquo;effondrer en plein Sprint quand un membre de l’équipe ajoute une brique. On empile la dette qui au début est assez stable et maîtrisée, sans mesurer l\u0026rsquo;ampleur des conséquences. On peut bien sûr retirer les éléments remboursés (en retirant les briques par le haut !).\nLe p’tit bonhomme qui stress© : Un moyen assez simple de visualiser la dette est d’empiler des bouts de post-it (la couleur correspond si possible au type de l’élément) sur un emplacement dédié et défini en taille, que vous aurez imprimé en A3. On décolle les éléments une fois qu\u0026rsquo;ils sont remboursés.\nOn peut y ajouter un peu de gribouillage \u0026hellip;\nPriorisation de la dette dans le backlog Qu’on estime ou pas en points d’histoire, il n’en reste pas moins que chaque tâche de la dette doit avoir sa valeur business pour être priorisée.\nConclusion Il existe vraiment plein de possibilités pour visualiser la dette en toute transparence. N’hésitez pas à partager vos idées.\n","date":"Feb 27, 2018","href":"https://blog.talanlabs.com/visualiser-dette-technique/","kind":"page","labs":null,"tags":["Scrum"],"title":"Comment et pourquoi visualiser la dette technique"},{"category":null,"content":" Hackathon Blockchain \u0026amp; Shop  Casino organisait un hackathon du 16 au 18 Février autour de la blockchain, avec pour thématiques la prédilection sur la traçabilité et les cartes de fidélité.\n Une équipe Talan très cosmopolite et complémentaire s’est constituée : 3 développeurs, 1 designer UX/UI, 1 consulting et 1 expert supply-chain.\n Trois jours intensifs et de nombreuses nuits blanches plus tard, le projet r.eth.race était opérationnel.\n Ce projet est basé sur une blockchain privée Ethereum (à vue de prototypage rapide), avec une Dapp permettant aux différentes parties prenantes d’interagir (usine, magasin, transporteur, consommateur).\n Sur la traçabilité, le prototype de processus était le suivant, via des signatures des différents acteurs à chaque étape :\n  un magasin passe une commande à une usine\n  une usine valide et complète la commande\n  le transporteur nommé par l’usine procède à l’enlèvement et effectue le transport jusqu’au magasin\n  le magasin atteste de la réception de la livraison\n   Enfin, la traçabilité dans notre projet permettait plusieurs actions :\n   effectuer un suivi du lot (par tous les acteurs)\n  permettre à un magasin de connaître le stock restant des produits rappelés\n  procéder à un rappel de produit en ciblant les consommateurs concernés et en leur envoyant un message personnalisé\n   L’idée principale du projet était du point de vue consommateur de limiter l’expérience négative liée à un rappel de produit. Pour cela, un alt-coin était utilisé (des CasinoCoin) afin de dédommager immédiatement les consommateurs lors d’un rappel de produit et leur permettre d’en obtenir encore plus lors de la restitution du produit rappelé en magasin.\n Ces CasinoCoin sont bien entendu convertibles en divers avantages, via une plateforme de conversion, soit sous forme d’argent, soit sous forme d’avantages divers et variés.\n Ci-dessous notre pitch projet :\n    Ainsi que la vidéo de démonstration de l’outil :\n    Hackathon riche en évènements et expérience très intéressante pour notre équipe. Petit rendu du making-of en vidéo.\n    Bien évidemment plusieurs problématiques ont été identifiées qui nécessitent d’améliorer les workflows de traçabilité. Il faut savoir que le code-barre actuellement utilisé (EAN-13) ne permet pas d’identifier le lot associé (la palette par exemple). Première piste envisageable : l’utilisation de l’EAN-128 (code-barre qui permet d’encoder plus d’informations dans le code barre). Mais l’avenir étant plus à l’intégration via les ERP déjà fréquemment utilisés par les acteurs de la supply-chain afin d’automatiser plus encore les transactions.\n Ce sujet ne devrait pas cesser d’être d’actualité dans les mois à venir, les rappels produits étant de plus en plus visibles. Les distributeurs et les industriels vont donc devoir rassurer les consommateurs dans leur capacité à faire face à ce type d’événement s’ils veulent conserver leur confiance.\n ","date":"Feb 24, 2018","href":"https://blog.talanlabs.com/hackathon-casino/","kind":"page","labs":null,"tags":["hackathon","ethereum"],"title":"Retour sur le hackathon Blockchain and Shop organisé par Casino"},{"category":null,"content":"Une histoire compliquée \u0026hellip; N\u0026rsquo;avez-vous jamais été troublé par l\u0026rsquo;utilisation faite des termes comme « épiques » ou « fonctionnalités » ? Certaines équipes les utilisent sans en garder nécessairement la même signification, d\u0026rsquo;autres inventent même leurs propres termes. Cela peut être assez confus pour un nouveau-venu et peut même engendrer des erreurs.\nLe Guide Scrum et l\u0026rsquo;histoire Le Guide Scrum a été écrit de façon assez ouverte pour ne pas contraindre les équipes à utiliser une règle, mais à s\u0026rsquo;adapter. C\u0026rsquo;est bien sûr une bonne chose, mais cela crée aussi des complications quand des équipes ou des blogs/sites, utilisent un même mot comme \u0026ldquo;épiques\u0026rdquo; pour décrire des éléments différents. Scrum ne décrit pas d\u0026rsquo;histoires, d\u0026rsquo;épiques, etc. Scrum décrit des Product Backlog Items (ou PBIs, ou \u0026ldquo;Item du Backlog Produit\u0026rdquo;), qui sont généralement découpés en épiques, histoires, tâches techniques ou bugs dans la plupart des équipes, parce que c\u0026rsquo;est utile.\n Le Backlog produit liste toutes les fonctionnalités, les fonctions, les exigences, les améliorations et les corrections qui constituent des modifications à apporter au produit dans les versions futures. Les items du Backlog produit ont les attributs d\u0026rsquo;une description, d\u0026rsquo;un ordre, d\u0026rsquo;une estimation et d\u0026rsquo;une valeur. Les items du Backlog produit incluent souvent des descriptions de test qui prouveront leur complétude lorsqu’ils sont « Finis ». Le Guide Scrum™\n Tout est PBI !\n L\u0026rsquo;idée d\u0026rsquo;utiliser des user stories (ou histoires d\u0026rsquo;utilisateur) vient d'Alistair Cockburn (un des signataires du Manifest Agile) en 1998, comme il l\u0026rsquo;explique sur son site. Puis en 2001, Ron Jeffries à proposé la formule des \u0026ldquo;Trois Cs\u0026rdquo; pour la création des histoires, utilisant le format adopté dans la plupart des équipes Scrum aujourd\u0026rsquo;hui.\n En tant que \u0026hellip;, je souhaite que \u0026hellip; dans le but de \u0026hellip;\nUne épique est une histoire Telle que décrite dans le livre que je recommande à tous de lire, Essential Scrum par Kenneth S. Rubin, une épique est une histoire, qui est trop grande pour être réalisée dans un sprint.\n une grand histoire, potentiellement de plusieurs semaines à plusieurs mois en taille [\u0026hellip;]. Les épiques sont utiles comme espaces réservés pour de grands pré-requis. Les épiques sont progressivement redéfinies en de petites histoires, au moment opportun. Une épique sera donc découpée en de petites histoires et ne restera pas. Si vous avez 4 histoires représentant ce qu\u0026rsquo;était votre épique, l\u0026rsquo;épique disparaît, remplacée par 4 histoires.\n Les épiques deviennent des histoires\nThème —ou Fonctionnalité Jeff Patton n\u0026rsquo;aime pas trop devoir retirer l\u0026rsquo;épique une fois celle-ci découpée, comme il l\u0026rsquo;explique sur son blog. Je peux comprendre, néanmoins je pense important de la supprimer, pour la clarté. La solution peut alors être de garder un lien entre toutes les histoires d\u0026rsquo;une épique, comme une annotation sur la carte –ou sur la tâche dans le cas d\u0026rsquo;un logiciel de gestion de produit.\n Cette grosse histoire donnait un contexte. C\u0026rsquo;était ma façon simple de penser globalement à ce que les gens faisaient. C\u0026rsquo;était ma façon rapide d\u0026rsquo;expliquer à d\u0026rsquo;autres le sujet. Pour moi, un \u0026ldquo;thème\u0026rdquo; –parfois appelé \u0026ldquo;fonctionnalité\u0026rdquo;– est justement fait pour ça. Le livre de Kenneth S. Rubin définit un thème comme : \u0026hellip; une collection d\u0026rsquo;histoires. Un thème apporte une façon commode d\u0026rsquo;indiquer que des histoires ont quelque chose en commun, comme à faire partie de la même fonctionnalité.\n Histoires par thème\nLa contrainte par le logiciel L\u0026rsquo;outil que vous avez choisi pour gérer votre backlog peut aussi vous avoir conditionné votre vision de l\u0026rsquo;épique. Des logiciels comme Jira ou Yodiz utilisent les épiques pour relier un groupe d\u0026rsquo;histoires entre elles. Pour moi encore une fois, ces épiques sont en fait des \u0026ldquo;thèmes\u0026rdquo;. D\u0026rsquo;autres logiciels n\u0026rsquo;ont pas d\u0026rsquo;a priori, ou, comme IceScrum, ont incorporés la notion de \u0026ldquo;Features\u0026rdquo; (fonctionnalités) pour regrouper les histoires reliées. Le logiciel permet d\u0026rsquo;ailleurs de transformer une histoire en épique et vice versa. Ce qui est - à mon avis - la meilleure façon de faire.\nS\u0026rsquo;adapter à l\u0026rsquo;équipe Bien entendu, en tant que Scrum Master, vous devez vous adapter à ce qui se fait déjà dans l\u0026rsquo;entreprise, ou prendre du temps pour changer en amont (si cela en vaut la peine). Discutez-en avec les intéressés, argumentez. Il est même possible d\u0026rsquo;avoir une définition au sein de l\u0026rsquo;équipe de ce qu\u0026rsquo;est une épique, qui diffère de celle du reste de l\u0026rsquo;entreprise, mais cela aura aussi un coût.\nConclusion: Est-ce juste une question de choix ? Le choix est arbitraire. Quelque soit votre choix, tenez-y vous. Je vous ai donné ma vision –qui est certainement biaisée par la façon dont j\u0026rsquo;ai appris Scrum (dans le livre de Rubin) et de part les meetup auxquels j\u0026rsquo;ai pu participer. Gardez en tête que votre Définition of Ready devrait établir une valeur maximale pour une histoire pour entrer dans le sprint. Si elle est trop grosse, l\u0026rsquo;appeler \u0026ldquo;grosse histoire\u0026rdquo; ou \u0026ldquo;épique\u0026rdquo; revient au même : it shall not pass si elle n\u0026rsquo;est pas** INVEST**. Publié initialement sur const.fr le 29 janvier 2018.\n","date":"Feb 20, 2018","href":"https://blog.talanlabs.com/scrum-tips-differences-entre-epiques-histoires-themes-fonctionnalites/","kind":"page","labs":null,"tags":["backlog","epiques","Scrum"],"title":"Scrum tips: Différences entre épiques, histoires, thèmes et fonctionnalités"},{"category":null,"content":"Vendredi 19 janvier 2018 avait lieu le meetup de l’Asseth (Association Ethereum française), sponsorisé et hébergé par Talan Labs. Il s’agit de l’un des rassemblements les plus importants autour de la blockchain en France.\n Au programme de la soirée : une présentation du projet Colony par Aron Fischer.\n Le speaker  Aron Fischer  Plus de 70 « blockchainers » se sont déplacés pour profiter de la présence d’Aron Fischer, core dev Ethereum. Docteur en mathématiques (université de New York), Aron Fischer contribue à Casper et surtout à Swarm, le système de partage de fichiers décentralisé de la fondation Ethereum.\n   Le sujet  EthCC  La soirée a commencé par l’annonce de la grande conférence EthCC organisée par l’Asseth : 3 jours de talks, conférences et retours d’expérience qui auront lieu en mars.\n Talan Labs sera évidemment sur place et on y présentera d’ailleurs un de nos projets !\n  Colony  Le sujet principal était la présentation du projet Colony. Plateforme basée sur la blockchain et destinée aux organisations ‘open’, Colony entend bien révolutionner la manière de prendre des décisions au sein d’un groupe de travail.\n   Objectif de Colony Son objectif est clair et ambitieux : permettre la création d’entreprises auto-organisées via le code plutôt que la paperasse. Mais sa cible dépasse les entreprises, en s’adressant aussi aux associations à but non lucratif ou aux projets communautaires.\n Depuis quelques jours le code source du projet est disponible et avec un peu de chance, le projet ne devrait pas trop tarder à être livré sur le réseau principal d’Ethereum.\n   Organisation d’une colonie Dans une colonie, on choisit ses tâches, qui rapportent des tokens propres à la colonie, ainsi que de la crédibilité. Cette crédibilité permet de peser dans les décisions prises par la communauté, tout en garantissant la légitimité de chacun : il faut avoir fait ses preuves pour être influent, et pas seulement parler plus fort que les autres…​\n   La vidéo de la présentation Toutes les grandes fonctionnalités de Colony ont été abordées par Aron Fischer dans un talk dense que vous pouvez retrouver dans la vidéo de l’événement :\n      ","date":"Feb 13, 2018","href":"https://blog.talanlabs.com/meetup-asseth-janvier/","kind":"page","labs":null,"tags":["colony","meetup","asseth"],"title":"Meetup Asseth chez Talan : présentation de Colony"},{"category":null,"content":"A l’heure où tout le monde parle de blockchain, peut-être vous êtes-vous intéressés à l’aspect technique du développement sur Ethereum. Et peut-être vous êtes-vous dits que c’était compliqué…​ Des technologies très nouvelles, peu de documentation à jour, peu d’outils à disposition…​\n Mais ça, c’était avant ! Il y a au moins un outil qui sort du lot : Truffle. Framework de développement sur Ethereum créé par Consensys, il se présente tout simplement comme le couteau suisse Ethereum. Truffle permet de compiler du Solidity (le langage des smart contracts Ethereum), tester des smart contracts, les déployer, gérer différents environnements…​\n   Au travers du projet TalanCoin évoqué dans un article précédent, nous avons été amenés à utiliser Truffle. Depuis le début du projet, Truffle a régulièrement été mis à jour, parfois de manière brutale…​ Mais une jolie nouveauté a vu le jour : Ganache. S’il vous reste un peu d’appétit après les truffes de Noël et la galette des rois, faites comme nous : goûtez à Ganache !\n  Ganache est open-source  La recette du successeur de TestRPC est simple : une fois téléchargé, il suffit d’installer ce petit outil, et vous voilà en possession d’une blockchain personnelle légère. Au lancement, une interface claire : 10 comptes créés et alimentés de 100 ETH pour faire vos premières transactions …​ et déployer vos smart contracts.\n  Aperçu de Ganache  On accède en un clic à la liste des blocs déjà minés, mais aussi à la liste des transactions passées et les logs (similaires à ceux de TestRPC). Deux options quant au minage : automatique à chaque transaction, ou continu avec une durée de génération de bloc fixée pour se rapprocher des conditions réelles.\n Accessible par défaut sur le port 7545 (paramétrable), il est évidemment possible de communiquer avec la blockchain en une requête cURL classique :\n $ curl 127.0.0.1:7545 -X POST \\ --data \u0026#39;{\u0026#34;jsonrpc\u0026#34;:\u0026#34;2.0\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;eth_sendTransaction\u0026#34;,\u0026#34;params\u0026#34;:[{ \u0026#34;from\u0026#34;: \u0026#34;0x627306090abaB3A6e1400e9345bC60c78a8BEf57\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;0xf17f52151EbEF6C7334FAD080c5704D77216b732\u0026#34;, \u0026#34;gas\u0026#34;: \u0026#34;0x76c0\u0026#34;, \u0026#34;gasPrice\u0026#34;: \u0026#34;0x9184e72a000\u0026#34;,\u0026#34;value\u0026#34;: \u0026#34;0x9184e72a\u0026#34;}],\u0026#34;id\u0026#34;:1}\u0026#39; --:--:-- {\u0026#34;id\u0026#34;:1,\u0026#34;jsonrpc\u0026#34;:\u0026#34;2.0\u0026#34;,\u0026#34;result\u0026#34;:\u0026#34;0x873cc520a53026128d021aa05d47b9e1b2bc7825d876a95ea68b7cb466fd06df\u0026#34;}   La clarté de l’interface, la capacité d’explorer les blocs un par un, et même les transactions qu’ils contiennent en font un outil parfait de test et de débug au cours du développement d’un smart contract, facilitant indéniablement le travail des aventuriers de la blockchain.\n Le projet Truffle est régulièrement mis à jour, parfois de manière brutale et non rétrocompatible, il en sera peut-être ainsi pour Ganache…​ Il n’en reste pas moins que l’effort de clarté et d’UX au service du développeur en font un incontournable du milieu Ethereum, désormais recommandé au sein de Talan Labs.\n ","date":"Feb 1, 2018","href":"https://blog.talanlabs.com/presentation-ganache/","kind":"page","labs":null,"tags":["ganache","truffle","ethereum"],"title":"Présentation de Ganache"},{"category":null,"content":"En 2017, l’UX (User eXperience) et l’UI (User Interface) ont pris de plus en plus d’importance au sein des entreprises notamment pour résoudre des problématiques liées aux utilisateurs de leurs services et de leurs produits. Parmi les outils utilisés par les Scrum Masters, les Product Owners (Directeur de projet) ou les équipes UX/UI, on retrouve les** jeux d’ice breaker** (brise-glace) qui aident à démarrer une réunion ou un atelier de manière participative. Ces jeux de 2 à 10 mins permettent de démarrer les échanges dans un mode créatif et collaboratif.\nVoici le TOP 3 des jeux brise-glace recommandés par l’équipe UX/UI de Talan Labs pour l’année 2018.\nJeu UX n°1 : L’objet fétiche Matériel prérequis : Un objet que chaque participant amène Objectif : Faire connaissance Participants : Entre 2 à 10 personnes Règle : Amener un objet qui raconte sa personnalité ou son état\nCe jeu est utilisé pour apprendre aux participants à se connaître. Tous les participants amènent un objet qui présente soit une partie de leur personnalité, décrit leur humeur du jour ou bien tout simplement un objet qui leur plait. Ce jeu crée un lien émotionnel entre les participants et permet ainsi de mieux connaître les personnes avec qui ils vont prochainement travailler.\nAstuce : Si certains participants ont oublié leur objet, vous pouvez utiliser les cartes de jeu Dixit où nous trouvons des illustrations abstraites et créatives. Les participants pourront donc choisir une carte selon leur humeur ou leur personnalité pour s’exprimer.\nJeu UX n°2 : Dessine-moi mon visage Matériel prérequis : Un papier et un crayon par participant Objectif : Apprendre à se connaître dans un contexte plus détendu Participants : De 2 à 20 personnes Règle : Jeu en pair (2 par 2). Il faut dessiner le visage de son binôme en 1-2 minutes Option (Difficulté ++) : Même règle, sans avoir levé la main et sans avoir baissé les yeux\nC’est un autre jeu qui peut être utilisé pour** créer la première interaction entre les participants**. Par groupe de deux, chaque joueur regarde son binôme et essaie de dessiner son visage le plus justement possible. Cela permet aux participants d’apprendre à se connaître dans un cadre plus détendu. Le but n’est pas de dessiner comme Leonard de Vinci mais de s’exprimer par le dessin. Ce jeu est aussi utile pour débuter les sessions de créativité.\nJeu UX n°3 : Chifoumi Matériel prérequis : Grand espace où les participants peuvent circuler Objectif : Rigoler Participants : 10 à 30 personnes (dépend de la capacité de l’espace) Règle : Le jeu chifoumi se joue par paire. Celui qui perd, rejoint le gagnant et joue avec une autre équipe. On fait une file des gagnants jusqu\u0026rsquo;à il n’en reste qu’un seul.\nCe jeu peut être utilisé pour se décontracter, notamment dans une grande équipe projet où les tensions peuvent être nombreuses. Contrairement aux autres jeux, celui-ci peut se faire avec beaucoup de participants et permet de créer un environnement convivial.\n","date":"Jan 18, 2018","href":"https://blog.talanlabs.com/jeux-brise-glace-talanlabs-2018/","kind":"page","labs":null,"tags":["jeux","UI","ux"],"title":"Top 3 des jeux brise-glace pour démarrer une réunion"},{"category":null,"content":"Quel est le bon jour pour commencer son sprint ? Quelle est la bonne heure pour le daily scrum ? Et concernant les événements Scrum ; la rétrospective doit-elle se passer juste après la revue de sprint ?\nLe Scrum guide [1] ne donne aucune indication, laissant les équipes s’ajuster, mais avec l’expérience, quel jour alors doit-on commencer son sprint ?\nLe Sprint \u0026amp; les cérémonies qui en découlent Le plus logique, serait de commencer un sprint le lundi. Après tout, c’est le début de la semaine (quoi que c’est très relatif). C’est parfait, la semaine débute, tout le monde est motivé. En plus, on le termine un vendredi : la fin de la semaine marquant la fin du sprint. Facile.\nLundi D’après le professeur Debbie Moskowitz, spécialiste du comportement, bien que le matin soit peu propice, le lundi après-midi est particulièrement efficace. C’est un bon moment pour **le planning et la définition d’objectifs **en général. Mais attention, certain prolongent leur week-end ou leurs vacances.\nMardi Une enquête menée en 2010 par des chercheurs de la London School of Economics, décrit le mardi comme « traitre ». Cela serait le jour de travail le plus déprimant. D’après cette enquête, le lundi notre cerveau baigne encore dans l’euphorie du week-end, alors que le mardi, nous sommes bien entrés en mode travail et le prochain week-end est encore bien loin. Toujours d’après Debbie Moskowitz, mardi n’en reste pas moins la journée la plus productive de la semaine.\nMercredi \u0026amp; Jeudi Toujours d’après Moskowitz, le mercredi et le jeudi sont des journées assez efficaces. Attention néanmoins, mercredi est souvent le jour du télétravail et des enfants. Bien qu’une cérémonie Scrum puisse se tenir à distance, il vaut mieux chercher à l’éviter quand c’est possible.\nVendredi On est déjà plus ou moins en week-end, certains se font un grand week-end qui débutent le jeudi soir, et se prolonge même jusqu’au mardi matin. Et il est très difficile de garder concentrée une équipe un vendredi après-midi pour une rétro, après une séance de review. Une bonne rétrospective demande de l’attention et un engagement. Certains peuvent partir en week-end le soir-même et sont donc pressés de partir le plus tôt possible. Le vendredi n’est pas forcément le meilleur moment pour des longues réunions importantes comme la revue de Sprint et la rétrospective. Le planning du prochain Sprint risque aussi d’être bâclé.\nEn conclusion : commencer son sprint en milieu de semaine Certaines expériences (celles de Rishi Devendra ou de Paulo Dias par exemple) montrent que commencer un sprint en milieu de semaine (le mercredi pour le terminer un mardi suivant par exemple) est une bonne stratégie.\nPour ma part et en en fonction des recherches décrites ci-dessus pour des sprints de deux semaines ou moins (les cérémonies prennent généralement plus de temps pour des sprints plus longs) :\nFaire une review et une rétro le mardi matin (avec des croissants chauds) peut être intéressant. L\u0026rsquo;équipe est assez alerte et concentrée pour tirer le meilleur de ces cérémonies importantes_._ Le lundi est alors un bon moment pour permettre à l’équipe de préparer sereinement sa démo en déployant une dernière fois; le Scrum Master à le temps d\u0026rsquo;élaborer sa rétrospective et le Product Owner peut aussi affiner son prochain sprint avec l’esprit frais et reposé.\nSavoir qu’il reste encore le lundi matin pour finir, cela peut aussi éviter à l’équipe de développement de travailler le week-end (et donc en shadow-velocity [2]).\nCela permet de commencer le sprint le mardi après-midi avec la Planification de Sprint. Une cérémonie importante qui doit être bien faite et donc, doit prendre le temps qu’il faut. Une Planification de Sprint est intense ; l’équipe y laisse de l’énergie et pourrait perdre en efficacité l’après-midi si le planning se déroule le matin.\n Quoi qu’il en soit, parlez-en avec l’équipe et les parties prenantes. Prenez en compte les contraintes de chacun, du pays avec le(s)quel(s) vous travaillez. Et n’hésitez pas à faire un essai. En effet, si les parties prenantes ne travaillent pas le lundi, jour de la démo, celle-ci n’a que peu d’intérêt et le sprint suivant ne sera pas calé sur leurs retours et leurs besoins.\nIdem pour l’heure du daily scrum. Elle doit convenir à tout le monde. Si la moitié de l’équipe arrive 10 minutes en retard trois fois par semaine, alors il convient d’en discuter et de le déplacer d’une demi-heure au moins, ce qui évite le stress d’arriver pile à l’heure, et permet de se poser confortablement quelques minutes avant pour préparer la cérémonie convenablement.\nCas exceptionnels Vous vous retrouverez avec des sprints qui ne pourront pas commencer tel ou tel jour, quelque soit votre choix. Dans ce cas, ne décalez pas votre sprint. Le sprint commence toujours à la date définie, juste après le précédent et se termine toujours juste avant le début du prochain. C’est important pour la vélocité et donc la prédictibilité du système [3]. En cas de jour férié, d’épidémie de grippe ou de voyage d’entreprise, ne changez pas la date de début du sprint, mais déplacez les cérémonies et ajustez la prédiction de l’équipe sur sa capacité, en fonction (moins de jour de développement = moins de vélocité, mais ce n’est pas grave si c’est compréhensible).\n[1] Planification de Sprint, Daily Scrum, Rétrospective et Revue de Sprint sont des événements de Scrum impliquant tout ou partie de l\u0026rsquo;équipe. Scrum Master, Product Owner et Equipe de dévelopement sont les rôles définis par Scrum et sont décrit précisément dans le Scrum Guide.\n[2] Shadow-velocity : terme inventé pour décrire le travail fait le soir, le week-end ou pendant les vacances et donc hors sprint et qui ne sera pas comptabilisé dans la vélocité officielle. Un futur article y sera consacré.\n[3] Le Scrum Guide n’oblige pas à caler un sprint sur 5, 10, 15 ou 20 jours ouvrés. Un sprint peut durer 7 jours, commençant lundi et finissant le mardi de la semaine suivante, mais il est souvent plus simple de garder une régularité. En effet, si les sprints commencent toujours le même jour de la semaine, on ne se pose pas de questions. Sinon, le second sprint de 7 jours ouvrés sera alors du mercredi au jeudi de la semaine suivante, et ainsi de suite, et tout le monde risque de s’embrouiller.\n","date":"Dec 14, 2017","href":"https://blog.talanlabs.com/commencer-sprint-scrum/","kind":"page","labs":null,"tags":["méthodes agiles","Scrum","sprint"],"title":"Scrum : quel est le bon jour pour commencer un sprint ?"},{"category":null,"content":"Google a annoncé le 23 février 2017 avoir réussi à casser la fonction de « hachage » SHA-1. Il s’agit là d’un événement majeur affectant la sécurité de nombreux systèmes allant de la signature de documents aux certificats de sécurité d’un site internet.\n Qu’est-ce que la fonction de « hachage » SHA-1 ? SHA-1 (Secure Hash Algorithm) est une fonction de « hachage » cryptographique.\n Il s’agit d’un algorithme qui associe à une donnée de taille quelconque, une image de taille fixe. En quelque sorte, une empreinte de la donnée d’entrée. Le calcul de cette empreinte est à sens unique : il est possible de calculer l’empreinte d’une donnée, mais pas de retrouver la donnée depuis l’empreinte. Cela signifie donc qu’il est impossible de revenir en arrière une fois la donnée hachée. De plus, une fonction de hachage doit assigner UNE empreinte unique : 2 données d’entrée ne peuvent pas avoir la même empreinte.\n C’est ainsi qu’un message « haché » par SHA-1 change radicalement de signature, même avec une modification mineure. Par exemple le mot \u0026#34;TalanLabs\u0026#34; devient \u0026#34;499dfc6db8ca1f785724cd88ffef5f471e874156\u0026#34; une fois « haché ». En revanche, \u0026#34;Talanlabs\u0026#34; devient \u0026#34;25f51e7cbd92b575adf321cc3b149e76d48e2431\u0026#34;.\n SHA-1 a été conçue par la puissante NSA, autrement dit l’agence nationale de la sécurité des États-Unis, en remplacement de SHA-0 (sortie en 1993) qui était compromise.\n   Qu’est-t-il arrivé à SHA-1 ? Depuis sa sortie en 1995, de nombreux chercheurs ont tenté de compromettre SHA-1, a minima dans des démonstrations mathématiques. C’est ainsi que depuis 2005, nous savons qu’il est en théorie possible d’obtenir une même empreinte pour 2 données différentes en réalisant 263 calculs (soit plus de 9 milliards de milliards…​). Ce nombre astronomique d’opérations est encore à la limite des capacités des super-ordinateurs actuels. Il semblerait que seul un immense réseau distribué à l’échelle mondiale (à l’image du réseau Bitcoin) soit capable de délivrer suffisamment de puissance de calcul pour obtenir une « collision » (deux données d’entrée différentes avec la même empreinte).\n À titre de comparaison, il faudrait environ 12 millions de GPU (puces graphiques) qui fonctionneraient pendant un an pour obtenir cette même « collision ».\n Jusqu’alors, nous ne disposions que de données théoriques, et jamais de cas pratiques. C’est finalement fin février qu’une équipe composée de chercheurs de Google et du CWI (centre de recherche en mathématiques et informatiques néerlandais) a annoncé avoir trouvé une « collision » entre 2 fichiers PDF qui ont la même signature une fois « hachés » par SHA-1.\n  Illustration d’une collision  Même avec les moyens considérables de Google, il n’était pas questions d’aligner des millions de cartes graphiques pour réaliser une telle prouesse. En affinant les calculs théoriques, et en éliminant d’office certains calculs, l’équipe a eu besoin que de 6 500 années CPU (soit 6 500 processeurs pendant un an) ou 110 années GPU (soit 110 cartes graphiques pendant un an) pour obtenir deux documents PDF différents mais avec la même signature.\n   Quelles sont les conséquences de la « collision » de SHA-1 ? Concrètement, l’outil mis au point permet d’obtenir un faux document qui sera identifié comme un vrai document. Autrement dit de faire passer une donnée potentiellement biaisée comme purement véridique et approuvée par son émetteur.\n Alors, quels sont les systèmes potentiellement rendus vulnérables ? Tous ceux qui utilisent SHA-1 comme algorithme de « hachage » pour certifier leurs données…​ Et ils sont nombreux ! SHA-1 est utilisé pour certifier une connexion HTTPS, signer numériquement des documents, authentifier les informations contenues dans un système de sauvegarde (cloud par exemple), mais aussi par un outil de gestion de versions comme Git.\n  Systèmes impactés  Git, outil privilégié pour la gestion des versions du code source de millions de projets informatiques, utilise SHA-1 pour identifier de manière unique chaque « commit » (modification) de code. Autrement dit, lorsque vous récupérez le code source d’un projet, la version que vous téléchargez est identifiée par un « hash » unique certifiant le contenu. Or, si du code malveillant a été injecté par la suite, mais avec une signature de « commit » identique à la dernière en date, nul système ne pourra le détecter et vous vous exposez à des failles de sécurité.\n Autrement dit, sans le savoir nous utilisons tous les jours des algorithmes de « hachage », et la sécurité de nos données et de nos entreprises est fortement liée à leur qualité. Une remise en cause totale de nos habitudes est-elle encore envisageable ? Revenir exclusivement aux signatures sur papier, aux messagers humains et à la sauvegarde centralisée ?\n   Quelles alternatives à SHA-1 ? Heureusement, des alternatives sont possibles. Et la responsabilité en incombe en grande partie aux développeurs, mais aussi aux décisionnaires de toutes les entreprises liées au numérique.\n SHA-1 n’est pas la seule fonction disponible. bcrypt (présentée en 1999), SHA-2 (conçue par elle aussi par la NSA en 2001) et plus récemment SHA-3 (issue d’une compétition en 2012) n’ont pas été compromises. Même si l’on peut imaginer qu’il ne s’agit là encore que d’une question de temps…​\n  Recommandations de Google \u0026amp; CWI  Google et le CWI ne cherchant évidemment pas à compromettre massivement les réseaux mondiaux, ils ont émis une recommandation simple : utiliser SHA-2 ou SHA-3 partout. De plus, ils ont attendu 90 jours avant de rendre publique la méthode utilisée pour arriver à leurs fins, afin de laisser le temps à l’industrie de se mettre à jour. De leur côté, ils garantissent que leurs produits comme Gmail ou Google Drive sont d’ores et déjà protégés. Par ailleurs, leur navigateur Chrome ne considère plus les certificats HTTPS basés sur SHA-1 comme sécurisés.\n Ils fournissent de surcroit un outil en ligne permettant de vérifier si un document fait partie d’une attaque par « collision » ainsi qu’une infographie de référence qui mériterait d’être affichée dans de nombreuses DSI.\n   ","date":"Dec 7, 2017","href":"https://blog.talanlabs.com/collision-sha-1-securite/","kind":"page","labs":null,"tags":["collision","Google","hachage","sécurité","sha-1"],"title":"Collision SHA-1 et sécurité"},{"category":null,"content":"Tous les développeurs le savent, leur métier ne consiste pas juste à produire du code, mais plus globalement à penser un système, imaginer des algorithmes ou réfléchir à l’intégration de leur projet dans un cadre plus vaste, tout en prenant en compte les besoins réels du client, le ressenti des utilisateurs et des budgets parfois serrés.\n L’état de concentration nécessaire pour être efficace, créatif et apporter les meilleures réponses possibles aux problèmes soulevés fait du développement un métier à part entière, loin de l’image véhiculée par les médias ou même Tim Cook (CEO d’Apple) qui considère que c’est \u0026#34;marrant et interactif\u0026#34;.\n   On donne parfois un nom à cette concentration intense : le « flow ». Cet état est bien connu des sportifs ou des musiciens mais il ne faut pas ignorer que dans certaines conditions le travail peut aussi permettre d’atteindre la \u0026#34;Zone\u0026#34; du flow.\n En effet, pour arriver à un tel état, un ensemble de paramètres est nécessaire :\n  Des objectifs clairement définis\n  Un retour d’informations instantané\n  Une activité en adéquation avec ses compétences\n   C’est alors que l’état de « flow » peut se manifester, à travers 4 dimensions bien distinctes et référencées :\n  L\u0026#39;absorption cognitive : sentiment de maîtrise et de contrôle de son activité (pas d’anxiété, pas d’ennui)\n  L\u0026#39;altération de la perception du temps : concentration sur le présent, on ne voit pas le temps passer\n  La dilation de l’égo : perte de la conscience de soi, sérénité\n  Le sentiment de bien-être : motivation intrinsèque, extase, sortie de la réalité quotidienne\n   Et vous alors, avez-vous déjà expérimenté ce sentiment de complétude en développant ?\n Musique \u0026amp; Développement Le cliché du développeur avec le casque vissé sur les oreilles, bien que caricatural, n’en est pas moins une expression de la recherche de concentration. Le casque isole du brouhaha de l’open-space, il permet d’écouter de la musique, mais il est aussi un avertissement sur le fait que l’on ne souhaite pas être dérangé.\n Justement, au sein d’un même open-space, que trouve-t-on dans le casque des développeurs qui écoutent de la musique ? Un rapide sondage chez Talan Labs fait ressortir que tout est bon pour se concentrer : électro, podcasts, bandes originales de films / jeux vidéo, etc.\n  Playlists sur 8tracks radio  Et lorsque l’on cherche sur internet, on trouve des dizaines, voire des centaines de « playlists » destinées aux développeurs. C’est principalement de la musique électronique, facile à écouter. On retrouve aussi de la musique classique, bref des styles musicaux propices à la concentration.\n Pour autant, on trouve parfois des choix très étonnants. C’est ainsi qu’un certain Jake Denison (le genre de développeur qui travaillait pour la NASA à 15 ans) explique qu’il écoute du métal lorsqu’il code. Et ce n’est pas juste pour s’amuser, il y a de vraies explications ! Le rythme rapide permet de réfléchir plus vite, la \u0026#39;lourdeur\u0026#39; du son permet d’aller plus loin dans son état de « flow », l’agressivité générale permet de maintenir la motivation et la complexité stimulerait les connexions neuronales.\n Et vous alors, est-ce que vous écoutez de la musique en développant ? Est-ce que vos goûts détonnent dans votre open-space ?\n   ","date":"Dec 7, 2017","href":"https://blog.talanlabs.com/concentration-musique-developpement/","kind":"page","labs":null,"tags":["Concentration","développement logiciel","musique"],"title":"Concentration, musique \u0026 développement"},{"category":null,"content":"Les mises à jour successives de Windows apportent de la sécurité et parfois de nouvelles fonctionnalités. Globalement, elles apportent une meilleure expérience pour l’utilisateur, rarement des retours en arrière.\n Windows 10 se met à jour automatiquement mais introduit de la publicité dans le navigateur Le système de mise à jour a lui aussi été modifié avec le passage à Windows 10, puisqu’il est désormais plus contrôlable par l’utilisateur (les mises à jour se font d’elles-mêmes).\n Et c’est désormais officiel, Microsoft a intégré la publicité au sein de son explorateur de fichiers. Autrement dit, lorsque vous naviguez dans le contenu de votre PC, vous pouvez rencontrer de la publicité. Actuellement centrée sur l’outil OneDrive (stockage dans le cloud développé par Microsoft), nous pouvons craindre que cette fonctionnalité intéresse très vite les agences publicitaires, en leur offrant un nouveau terrain de jeu.\n  Publicité dans l’explorateur de fichiers Windows  Alors, pour ou contre la publicité au sein même de votre ordinateur ?\n La question ne se pose pas trop, disons-le ! Un ordinateur coûte cher, un système d’exploitation comme Windows aussi. Nul doute que dans un contexte professionnel il est difficile d’imaginer des publicités intrusives à ce point. Et quid des lois définissant l’abus de position dominante ?\n   Comment supprimer la publicité dans votre navigateur Windows Heureusement, la solution de contournement existe déjà, ici en anglais !\n En résumé (et en français) :\n  Aller dans l’onglet \u0026#34;Affichage \u0026#34;de l’Explorateur de fichiers\n  Sélectionner \u0026#34;Option\u0026#34;\n  Sélectionner \u0026#34;Modifier le dossier et les options de recherche\u0026#34;\n  Aller dans l’onglet \u0026#34;Affichage\u0026#34; de cette nouvelle fenêtre\n  Dans les \u0026#34;Paramètres Avancés\u0026#34;, décocher \u0026#34;Afficher les notifications du fournisseur de synchronisation\u0026#34;\n     ","date":"Nov 30, 2017","href":"https://blog.talanlabs.com/windows-10-publicite-explorateur/","kind":"page","labs":null,"tags":["Microsoft","OneDrive","Publicité"],"title":"Windows 10 : de la publicité dans l'explorateur"},{"category":null,"content":"Lundi 13 novembre 2017 avait lieu le meetup de l’Asseth (Association Ethereum française). Sponsorisé et hébergé par Talan Labs, il s’agit de l’un des rassemblements les plus importants autour de la blockchain en France.\n Au programme de la soirée : le débriefing de la Devcon3, la conférence annuelle des développeurs Ethereum qui avait lieu début novembre, à Cancún au Mexique.\n  Devcon3 - La conférence annuelle des développeurs Ethereum  Environ 85 \u0026#34;blockchainers\u0026#34; se sont déplacés pour profiter des retours de Jérôme de Tychey, président de l’Asseth. En 45 minutes seulement, il a réussi à nous faire vivre 4 jours de conférence. Et quelle conférence ! Devcon3 c’est plus d’une centaine de présentations, devant 1 800 personnes venues du monde entier.\n Les sujets abordés lors de Devcon3 Il y a les incontournables, comme la désormais fameuse présentation \u0026#34;Ethereum en 25 minutes\u0026#34; de Vitalik Buterin, cofondateur d’Ethereum. Mais il y a aussi des présentations plus nouvelles, comme Sikorka, un outil qui permet de prouver sa présence physique de manière infalsifiable à travers la blockchain.\n C’était aussi l’occasion de voir un point d’avancement sur les travaux visant à rendre possible l’utilisation d’Ethereum à travers des clients légers, une fonctionnalité encore inaccessible mais très demandée par les développeurs blockchain.\n Ethereum est un environnement conçu par des développeurs, pour des développeurs. Et l’ergonomie de développement n’est pas au rendez-vous, loin de là. C’est ainsi que des outils comme Puppeth ont vu le jour, pour faciliter le déploiement d’une blockchain privée, et donc favoriser la démocratisation d’Ethereum sur les projets. L’outillage se fait de plus en plus complet, comme avec Remix, l’environnement de développement web pour les smart contracts.\n Côté langage, pas de révolution sur Solidity en 2017, le langage contract-oriented d’implémentation de smart contracts, si ce n’est que le support de _struct _comme argument de méthodes approche. Ce qui n’empêche pas les bonnes pratiques de se répandre, et les recommandations en termes de sécurité d’émerger de manière plus affirmée.\n Sur le plan de la communication avec un client, l’écosystème Ethereum se renforce avec un nouveau framework JavaScript : EthJS. Après https://github.com/ethereum/web3.js/, EthereumJS ou encore parity.js, il semblerait que le consensus ne soit pas encore atteint pour élire la référence JavaScript sur Ethereum.\n   Échanger avec la communauté Ethereum Une fois la présentation achevée et après une dizaine de questions, il était temps de reprendre des forces autour d’un buffet.\n C’était aussi l’occasion d\u0026#39;échanger de manière plus informelle autour de nombreux sujets ayant trait à la blockchain, avec un public aux profils variés. Comptables, avocats, développeurs, mais aussi UX designers, Ethereum rapproche toutes les parties prenantes des projets informatiques. Et si ce n’était pas la plus grande réussite d’une technologie encore en plein essor ?\n Toujours est-il que Talan Labs compte bien participer activement au mouvement qui se crée autour de la blockchain et de nouveaux événements sont à prévoir dans les prochains mois !\n Découvrez la vidéo du meetup sur le debrief de Devcon 3 :\n      ","date":"Nov 16, 2017","href":"https://blog.talanlabs.com/meetup-asseth-devcon3-talan-ethereum/","kind":"page","labs":null,"tags":["asseth","blockchain","devcon3","ethereum","meetup"],"title":"Meetup Asseth chez Talan : retour sur Devcon3"},{"category":null,"content":"Le lundi 13 novembre à partir de 18h30 dans les locaux de Talan aura lieu le débrief du Devcon3, la conférence annuelle des développeurs Ethereum qui s’est déroulée du 1 au 4 novembre 2017.\n   Ce meetup est organisé l’http://www.asseth.fr[Asseth], association ayant pour but de promouvoir, développer, et permettre au plus grand nombre d’utiliser la blockchain Ethereum, en partenariat avec TalanLabs. À cette occasion, les équipes de l’Asseth reviendront sur les annonces et les faits marquants de cette dernière édition du Devcon.\n Victime de son succès, les cent places du meetup ont été distribuées en quelques heures.\n Rendez-vous le 13 novembre à 18h30 chez Talan, 21 Rue Dumont d’Urville à Paris.\n ","date":"Nov 9, 2017","href":"https://blog.talanlabs.com/meetup-asseth-talan-debrief-devcon3/","kind":"page","labs":null,"tags":["asseth","blockchain","devcon3","ethereum","meetup"],"title":"Meetup Asseth chez Talan : débrief du Devcon3"},{"category":null,"content":"À l’occasion d’une** formation UX** interne le 11 octobre dernier, un bootcamp (formation) design thinking a été organisé dans les locaux de Talan. Avec pour thème un parc d’attraction en famille, les 35 participants de cet atelier n’ont pas manqué d’idées et d’imagination. Voici le résultat, façon UX, sous forme de sketchnote.\n","date":"Oct 26, 2017","href":"https://blog.talanlabs.com/bootcamp-design-thinking-talan/","kind":"page","labs":["Lab T"],"tags":["Design Thinking","ux"],"title":"Un bootcamp design thinking chez Talan"},{"category":null,"content":"Mardi 10 octobre 2017, Talan accueillait un « meetup » des « HumanTalks », que nous vous avions présenté dans un article il y a quelques mois.\n Au programme de la soirée : 4 « talks » de qualité sur des sujets plus ou moins techniques, un public de près de 70 personnes, principalement des développeurs, mais aussi des designers UX et des responsables des Ressources Humaines.\n 1er talk : Mythes et Légendes de l’UX Présenté par Marc Wabnitz, UX designer chez Talan Labs, ce talk présentait 7 idées reçues autour de l’expérience utilisateur.\n On dit souvent que les utilisateurs ne lisent pas ce qui est écrit sur une page web, mais c’est faux. De la même manière, on a tendance à croire qu’en copiant les géants du web comme Facebook ou Google, un site sera réussi, mais il n’en est rien.\n En 7 points bien ciblés, Marc a su nous convaincre d’aller plus loin que certains lieux communs trop souvent associés à l’UX.\n   2e talk : Le HTTPS : à quoi ça sert, comment on fait ? Présenté par Foucauld Degeorges, qui après un rappel des principes cryptographiques sur lesquels repose HTTPS, nous a présenté l’intérêt d’utiliser un tel protocole sur un site internet. Il a notamment présenté des solutions gratuites et fiables pour faire du HTTPS sur un site.\n   3e talk : Pour que le CSS ne soit plus une corvée Pour vous le CSS est une corvée ? En reprenant les bases et les bonnes pratiques et avec quelques bons conseils et règles de survie, Albéric Trancart a prouvé qu’il est possible d\u0026#39;optimiser l’utilisation du CSS de manière à rendre plus acceptable un langage trop souvent mal vu. Il y a visiblement plus efficace que d’utiliser !important à tout bout de champ…​\n   4e talk : Planification de rendez-vous avec OptaPlanner   Présenté par Richard Hanna. Le support, c’est par ici !\n Quand il a un problème complexe de planification de rendez-vous, Richard utilise OptaPlanner. Outil open-source capable de résoudre des problèmes complexes (ou NP-Complet), il permet d’approcher une solution considérée comme optimale.\n   ROTI : de la qualité avant tout ! Tradition des HumanTalks : le ROTI (Return On Time Invested) qui permet à tous de s’exprimer quant à la qualité des présentations. Une bonne manière pour les intervenants de s’améliorer.\n  ROTI HumanTalks  La soirée s’est achevée dans la bonne humeur autour d’un buffet, l’occasion pour chacun de nouer des liens et de permettre de nouvelles rencontres.\n Plus qu’un mois avant les prochains HumanTalks, ce sera l’occasion de fêter leurs 5 ans le 14 novembre 2017.\n   ","date":"Oct 24, 2017","href":"https://blog.talanlabs.com/humantalks-talan/","kind":"page","labs":null,"tags":["CSS","HTTPS","HumanTalks","meetup"],"title":"Les HumanTalks chez Talan"},{"category":null,"content":"Pour moderniser les traditionnels supports d’aide à la vente de sa force commerciale interne et celle de ses partenaires, BNP Paribas Leasing Solutions a fait appel à Talan Labs pour la conception en mode agile d’une application digitale dédiée. Un projet bouclé en à peine 6 mois.\nDigitaliser la force de vente Filiale de la banque éponyme, BNP Paribas Leasing Solutions (Leasing Solutions) accompagne les entreprises dans leur développement en proposant des solutions locatives et des modes de financement adaptés. Structurée autour de 3 business units, l’activité de Leasing Solutions adresse le marché des actifs roulants (agriculture, manutention, transport…), et d’autres actifs mobiliers (informatique, médical, bureautique) et immobiliers. Baptisée Technology Solutions, cette troisième business unit distribue ses solutions de financement d’actifs technologiques en direct, mais également au travers d’un réseau de distribution.\nEn décembre 2016, dans le cadre d’une réflexion menée avec la direction générale, l’idée de concevoir un outil efficace d’aide à la vente, pour les équipes de vente internes mais également pour les partenaires commerciaux, est lancée afin d’accélérer le développement de la société. Et c’est le mode agile qui est retenu pour l’occasion.\nEn janvier 2017, une « Jam Session » est organisée. Objectif : « isoler » pendant deux jours les parties prenantes du projet, équipes internes métier et IT, partenaires, et représentants de la direction, pour qu’ensemble ils répondent à une question. En l’occurrence, « comment convaincre un partenaire de faire évoluer son activité en passant du mode vente à la location ? ».\nL’idée qui en est ressortie : la conception, à destination des équipes commerciales internes et partenaires, d’une véritable application digitale d’aide à la vente, allant bien au-delà de traditionnels extranets où sont répertoriés fiches produits, éléments de pricing, de stocks, et autres classiques outils marketing. En synthèse, il s’agit réellement de digitaliser la force de vente.\nTalanLabs : prime à l’audace Dès le concept métier défini, une organisation pilotée par l’équipe IT appelée « l’accélérateur de Leasing Solutions » est sollicitée pour trouver le partenaire technique qui participera au projet. L’objectif de départ était de réaliser une application Web Responsive pour s’adapter aisément à toutes sortes de devices. Mais en sortant des sentiers battus, l’équipe Talan Labs a pris des risques, ce qui s’est finalement avéré payant.\nL**’équipe TalanLab** s’est adaptée au développement en mode agile, sur la base de sprints successifs de 15 jours à partir du 1er avril avec un objectif de délivrer le projet le 12 juin, mais a su faire la différence sur les **choix techniques. En effet, **elle a « osé » s’éloigner du cahier des charges préalablement défini en proposant la création de deux environnements distincts : l’un pour iOS, qui équipe l’ensemble des commerciaux en interne, et un second Web Responsive pour tous les autres « devices » utilisés par les commerciaux des partenaires de Leasing Solutions.\n« _C’est étonnant mais en utilisant certaines fonctionnalités natives d’iOS, nous avons finalement pu disposer de deux applications parfaitement fonctionnelles, à iso-budget et dans le temps imparti _» se félicite aujourd’hui Philippe Jouglard, Directeur Marketing \u0026amp; Développement chez Leasing Solutions, pour qui les équipes de Talan Labs ont fait preuve de grande créativité en termes d’architecture technique, ainsi que d’une compréhension rapide et fine des besoins métiers.\nDéployée sur le périmètre pilote le 26 juin 2017 comme prévu, l’application aura nécessité à peine 6 mois de travail, depuis l’idée jusqu’à sa mise en production. En complément du développement, l’hébergement ainsi que la maintenance applicative (TMA) ont été également confiés à Talan Labs.\nAgilité : la preuve par l’exemple Pour Leasing Solutions, l’objectif de cette application est double. Tout d’abord, elle doit aider l’entreprise à ouvrir de nouveaux marchés, jusque-là plus habitués au modèle achat-vente, qu’au modèle à l’usage. Ensuite, le mode projet déployé pour la conception de l’application est aussi l’occasion de démontrer la pertinence du mode agile. « Nous avons été bien aidés en cela par TalanLabs, s’enthousiasme le Directeur Marketing et Développement. _Leur présence constante et la cohésion entre toutes les parties prenantes du projet, qu’elles soient techniques ou métier, ont été décisives. C’est grâce à cela que nous avons été capables de déployer sur un cycle aussi court une application digitale avec un aussi fort contenu fonctionnel _».\nJusqu’en septembre 2017, la dizaine de commerciaux de Leasing Solutions et quelques autres partenaires qui composent l’équipe pilote poursuivent leur utilisation des nouvelles applications avant un debriefing complet, auquel succédera le déploiement général à partir de la fin septembre. Destinée à l’ensemble de la force commerciale de l’entreprise et à celles des quelques 1 500 différents partenaires actifs, l’application devrait compter entre 20 000 et 30 000 utilisateurs d’ici un an.\nEt BNP Paribas Leasing Solutions ne compte pas s’arrêter en si bon chemin. D’autant que dès l’origine l’application a été pensée pour être largement diffusée, et facilement internationalisée avec l’intégration simple de nouvelles langues. « _Dès l’année prochaine, ces deux applications pourront être mises à disposition des autres _business units _de la société, et sur tous les secteurs géographiques que nous couvrons _», conclut Philippe Jouglard.\n","date":"Oct 23, 2017","href":"https://blog.talanlabs.com/bnp-leasing-solutions-digitalisation/","kind":"page","labs":null,"tags":["BNP Paribas Leasing Solutions","FabLab","méthode agile"],"title":"BNP Leasing Solutions digitalise ses outils d’aide à la vente avec TalanLabs"},{"category":null,"content":"_Présentée pour la première fois le 4 octobre au 1er Forum Parlementaire de la Blockchain, la monnaie virtuelle du groupe Talan est en passe de rentrer en phase de production. L’occasion de présenter en détails les tenants et les aboutissants d’un projet et d’une initiative pas comme les autres : TalanCoin !\n   La blockchain, tout le monde en parle, depuis les médias généralistes jusqu’aux startups les plus innovantes. Et pourtant, rares sont les idées à aller jusqu’à la phase de production. C’est pourquoi Talan a souhaité se démarquer et proposer à ses collaborateurs une application réelle de cette technologie en plein essor. C’est ainsi que l’idée d’une monnaie interne est née de l’envie de fédérer le personnel d’un groupe international autour d’un modèle de partage commun.\n TalanCoin, c’est son nom, abrégé en ‘TC’, est une monnaie basée sur la blockchain, et plus particulièrement sur Ethereum. Ethereum est une blockchain largement utilisée, et sa monnaie, l’Ether, est la deuxième plus grosse capitalisation des crypto-monnaies, derrière le Bitcoin. Le principal apport d’Ethereum par rapport à Bitcoin est sa capacité à intégrer de la logique et des règles métier inaltérables puisque stockées sur la blockchain en elle-même. Il s’agit des fameux “Smart Contracts”, concept que nous développerons plus amplement dans un article à venir.\n Pourquoi lancer cette monnaie ? Comme toutes les entreprises, Talan cherche à catalyser la collaboration en son sein. De plus, Talan souhaite renforcer son unité en tant que groupe malgré des entités distinctes et une répartition sur 4 continents et développer une horizontalité bien loin des hiérarchies verticales classiques. Une monnaie interne, commune à tous les collaborateurs, peu importe leur localisation ou leur rôle hiérarchique, permet de trouver un point commun entre tous les acteurs de l’entreprise. C’est aussi une manière de permettre à tous de mieux échanger, y compris entre équipes distinctes, affirmant ainsi une hiérarchie horizontale.\n  Aperçu du Dashboard de suivi de TalanCoin  De plus, en suivant l’évolution des échanges de cette monnaie et les raisons de transfert, Talan Coin permet un feedback immédiat des tendances chez Talan. L’introduction d’un volet de gamification incite par ailleurs à plus d’utilisation encore, à travers l’organisation de challenges.\n   À quoi ressemble Talan Coin ?  Aperçu de la version mobile de TalanCoin  Talan Coin est accessible à tous les collaborateurs du groupe à travers une application mobile (iOS et Android) téléchargeable sur les stores officiels de Google et Apple. L’orientation mobile de Talan Coin garantit une adoption assez large de l’application et permet à tous d’accéder au système. Cela élimine les éventuelles restrictions de connexion pour nos collaborateurs en mission chez nos clients ou tout simplement en déplacement.\n  Aperçu de la version web de TalanCoin  Par ailleurs, de manière à étendre encore la portée et l’accessibilité du produit, une version web, orientée desktop est en cours de finalisation, pour permettre une nouvelle expérience, dans tous les moments d’une journée de travail.\n   Et concrètement, comment ça marche ? Il y a deux types de portefeuilles en circulation : les portefeuilles personnels pour l’ensemble des collaborateurs et les portefeuilles d’équipes, gérés par une ou plusieurs personne(s) identifiée(s) comme leader(s) sur un sujet ou d’une équipe.\n Les portefeuilles personnels peuvent échanger de l’argent avec tous les autres portefeuilles personnels, mais pas directement avec les portefeuilles équipes. Ces derniers permettent de ‘rémunérer’ les collaborateurs oeuvrant au bénéfice d’une équipe.\n Comment gagner des Talan Coins ?   Comme nous l’avons vu précédemment, l’objectif principal de Talan Coin est de favoriser les échanges et le partage au sein de l’entreprise. C’est ainsi que toutes les actions visant à aider Talan et plus largement qui y facilitent la vie quotidienne sont susceptibles d’être récompensées en Talan Coins.\n Les axes clairement identifiés pour le moment sont les suivants :\n   Aider ses collègues (coup de main occasionnel, soutien technique, etc.)\n  Innover (lancer un nouveau projet, une idée qui peut changer les processus internes, etc.)\n  S’impliquer dans la vie du groupe (participation aux communautés, organisation de meetups, rédaction d’articles pour les blogs Talan, etc.)\n  Être sympa (la blague du matin, l’aide spontanée, etc.)\n  Partager ses connaissances (production de formations, organisation d’une présentation technique, mentoring et parrainage, etc.)\n  Fluidifier les process internes (participation à un appel d’offre, cooptation, etc.)\n    Comment donner des Talan Coins ?   La fonctionnalité de transfert est disponible depuis les applications mobiles et web. Une raison est associée à chaque transfert, afin de fournir des indicateurs d’utilisation, mais aussi dans un but de laisser une trace dans les historiques de chacun.\n Ces raisons de transfert permettent aussi de créer autant de catégories de classement dans le volet ‘gamification’. Il est évidemment prévu de faire évoluer cette liste en fonction des besoins qui seront découverts.\n  Comment convertir ses Talan Coins ?   Une participation active à la vie de l’entreprise permet de gagner des Talan Coins, mais encore faut-il pouvoir les utiliser, en autre via une conversion en biens ou services. C’est ainsi que Talan se repose sur 2 boutiques propres à fournir des produits ou services payables en Talan Coin.\n Campus Talan Historiquement, il s’agit de la première ‘boutique’ identifiée comme source d’offres pour Talan Coin. En effet, le Campus Talan héberge déjà l’ensemble des formations internes et externes délivrées aux collaborateurs Talan. Il paraissait donc naturel d’y ajouter de nouvelles offres de formations jusqu’alors non disponibles, pour ouvrir de nouvelles opportunités.\n Il n’est évidemment pas question de rendre payant ce qui était gratuit depuis le lancement du Campus Talan, mais bien d’étendre la portée d’un outil central au groupe Talan.\n  Aperçu du Campus Talan  Chaque leader d’équipe a la possibilité d’ajouter une offre dans le Campus, en spécifiant un tarif en TC, ce qui rend l’offre visible depuis les applications mobile et web. On peut donc imaginer un collaborateur souhaitant se rendre à une formation payante jusqu’alors non prise en charge par Talan et qui ferait une demande à un leader d’équipe. Celui-ci n’aurait donc qu’à acheter une place à cette formation en € (via le processus classique), et la rendre disponible sur le Campus Talan avec un prix en TC associé. Les TC récoltés par la vente de cette offre au collaborateur reviendrait donc au portefeuille équipe, remboursant l’investissement en € initié.\n Un tel dispositif garantit par ailleurs que les offres de formation ne sont plus négociées au cas par cas, mais ouvertes à tous, assurant une plus grande équité et une mise à disposition de tous les collaborateurs des offres.\n  Talan Coin Shop De manière à fournir des offres en plus des formations, nous avons développé une nouvelle boutique adaptée pour vendre des objets et des services. Ce ‘Talan Coin Shop’ permet de notifier l’acheteur et le vendeur du bon déroulement d’une transaction.\n  Aperçu du TalanCoin Shop  Là aussi, les leaders d’équipes peuvent ajouter des offres, composées d’un titre, d’une description, d’une image et d’un prix. Lors d’un achat, le vendeur et l’acheteur sont notifiés par mail une fois la transaction bien déroulée.\n   Talan Coin est une monnaie fondante   De manière à dynamiser l’économie de Talan Coin, nous avons imaginé utiliser un principe de monnaie fondante. Le concept en est assez simple : lorsqu’un collaborateur ne dépense pas l’argent qu’il a reçu au bout d’un mois, il perd un pourcentage de la somme gagnée. Sans trop pénaliser un collaborateur qui serait en congés par exemple, la somme ponctionnée sera ainsi ré-injectée dans le système global, tout en motivant tout un chacun à utiliser, échanger ou dépenser ses Talan Coins.\n En effet, pour nous l’un des indicateurs clés de la réussite du projet Talan Coin réside tout simplement dans le nombre de transactions qui auront lieu au jour le jour, indiquant ainsi l’adoption de la monnaie dans la vie quotidienne de l’entreprise.\n    La suite…​ La réalisation d’un tel projet, résolument novateur, n’a pas été sans embûche, ni sans découvertes et enseignements de toutes sortes. Nous tenons à partager notre expérience et nos connaissances nouvelles avec le plus grand nombre, ne serait-ce qu’au regard de l’aide apportée par la communauté de plus en plus importante de développeurs Ethereum.\n C’est ainsi que nous publierons dans les prochaines semaines le making-of de Talan Coin, agrémenté des anecdotes les plus marquantes, mais aussi des articles plus techniques autour de Talan Coin, et de la BlockChain en général.\n \u0026gt;\u0026gt;\u0026gt; Le site officiel Talan Coin \u0026lt;\u0026lt;\u0026lt;\n   ","date":"Oct 17, 2017","href":"https://blog.talanlabs.com/quest-ce-que-talancoin/","kind":"page","labs":null,"tags":["blockchain","cryptomonnaie","ethereum","TalanCoin"],"title":"Qu’est-ce que TalanCoin ?"},{"category":null,"content":"Le 2 Octobre 2017 s\u0026rsquo;est tenu la seconde édition du DevOpsRex à Paris dans la mythique salle du Grand Rex.\nCette conférence technique 100 % francophone est aujourd\u0026rsquo;hui un des rendez-vous de référence autour du DevOps. Une quinzaine de speakers nous ont partagé leurs retours d’expériences (REX) tous issus de retours concrets en entreprises. Nous avons apprécié leurs maitrises de la méthodologie et leurs recommandations. Car même si le DevOps a ses bénéfices, il a évidemment son lot de contraintes et de limites.\nChez Tarkett, Pauline et Aurore sont revenues sur les principales erreurs commises et leurs approches, aussi bien techniques qu\u0026rsquo;organisationnelles, pour les corriger. Cette alchimie entre les couples Dev et Ops a été mise à l\u0026rsquo;honneur aussi chez PhotoBox qui nous a présenté l’immersion d\u0026rsquo;un Ops parmi les Devs et les avantages à travailler côte à côte au quotidien.\nDe grandes entreprises comme LaPoste, RadioFrance, Cisco, Axa et PMU ont partagés leurs méthodes de travail mise en place et comment leurs transformations ont pu se faire !\nVoyages SNCF, connue pour être une société innovante nous l\u0026rsquo;a encore démontré au travers leur culture d\u0026rsquo;entreprise qu\u0026rsquo;est l\u0026rsquo;excellence opérationnelle. Sur le principe de Chaos Monkey (une application éditée par Netflix qui a pour but de générer des pannes aléatoires en Production, pour forcer les équipes à pouvoir régler toutes formes d\u0026rsquo;anomalies !), les Ops ont imaginé un jeu grandeur nature sur les véritables environnements de pré-production invitant les équipes des Dev à jouer entre eux !\nUn bel exemple de team building sur fond de « gamification » baptisé Days Of Chaos.\nLe point central de toutes les sessions reste l\u0026rsquo;importance des relations humaines ; c\u0026rsquo;est ce qui fait tout l’intérêt du DevOpsRex. Le DevOps n\u0026rsquo;est pas qu\u0026rsquo;une pipeline. Pour atteindre l’équilibre parfait entre Dev et Ops, il faut travailler la culture, la collaboration et le partage.\nSi le DevOps pouvait se résumer dans 4 termes, se serait \u0026ldquo;culture, automatisation, mesure et partage\u0026rdquo; !\n","date":"Oct 6, 2017","href":"https://blog.talanlabs.com/conference-devopsrex-2017/","kind":"page","labs":["Lab V"],"tags":["Agile","culture","DevOps","experience"],"title":"Retour sur la conférence DevOpsRex 2017"},{"category":null,"content":"Quand l’UX se focalise sur la conception centrée utilisateur, le SCRUM offre une méthodologie pour la bonne réalisation de projet. On lui donne aussi le nom de SCRUX.\n_Mais comment faire pour que ces deux univers cohabitent ensemble ? _\nVoici ci-dessous notre retour d’expérience avec une liste des « bonnes pratiques » à mettre en œuvre.\nPhase de lancement (sprint 0 pour les agilistes) La collaboration joue un rôle essentiel dans cette phase et ceci tout au long du projet. En phase de lancement, il s’agit **d’étudier, de partager et de définir **une approche claire avec l’ensemble des parties prenantes du projet (décideurs, équipe projet et futurs utilisateurs).\nRassembler l’ensemble de ces parties va permettre d’assurer une vision exhaustive du produit, formalisée en « backlog » et priorisé selon différentes valeurs :\n valeurs business (objectifs, indicateurs de succès, jalons, impacts)  valeurs techniques (architecture, coûts de développement, réalisation) valeurs utilisateurs (profils, attentes, frustrations, ressentis).    La réussite d’un projet va donc dépendre de l’histoire que l’on veut raconter et de la collaboration entre les parties prenantes et les utilisateurs ; tout est une question d’échange.\nPendant les itérations agiles 1- L’équipe Design doit intervenir avec un « sprint » d’avance Des « sprints » UX/UI viennent s’ajouter en parallèle de ceux des développeurs. Pendant que les développements du sprint N sont en cours, les UX/UI préparent le sprint N+1.\n2- Le sprint se focalise sur une « feature » Une « feature » rassemble plusieurs « user stories » permettant d’apporter de la valeur à l’utilisateur ou au business : c’est une caractéristique de l’application qui peut être divisée en plusieurs fonctionnalités ou tâches à réaliser par l’utilisateur. L’ensemble des « features » représente une vision simplifiée du « backlog » du produit.\nIl est essentiel de travailler par feature pour assurer une cohérence sur la conception : ce sont elles qu’il faut prioriser avant le début des itérations. Les ateliers UX seront ainsi plus pertinents et aucun participant ne se sentira à l’écart des réflexions puisqu’ils seront tous concernés par le même sujet.\n3- Enrichir les rituels SCRUM Pas de révolution ici, il faut seulement enrichir les rituels du SCRUM classique :\n Inclure l’UX Designer dans le « sprint planning » pour identifier les features à travailler  Organiser un atelier design par « sprint » pour construire le squelette, le prototype et tester les scénarios d’usage. Participer au « daily » stand up meeting pour que les membres de l’équipes puissent partager ce qu’ils ont fait, ce qu’ils vont faire et leurs points de blocage. Les développeurs commencent alors leurs sprints avec les résultats des concepteurs. S’intégrer au « sprint » de restitution (« review ») pour partager les propositions à toute l’équipe afin de valider ou d’effectuer un mini-pivot (légères évolutions qui ne retardent pas les prochaines itérations). Les développeurs commencent alors leurs « sprints » avec les résultats des concepteurs. Faire une rétrospective pour faire le point tous ensemble (SCRUM, UX, UI, Dev) afin d’améliorer les prochains « sprints », apprendre de nos erreurs et grandir.    4- Réutiliser des composants Un composant peut concerner une ou plusieurs « users stories ». Les premiers « sprint UX » vont permettre de converger sur des solutions design et des composants réutilisables. C’est un gain de temps considérable, autant pour les UX / UI designers que pour les développeurs.\n**5- Renforcer la vision des utilisateurs ** Le renfort de la vision utilisateur pendant la phase d’itération peut être aussi simple que complexe : tout dépend de sa définition en amont.\nCas 1 - En phase de lancement, la vision produit est claire. Les UX connaissent parfaitement leurs utilisateurs, leurs façons de penser et leurs points de frustrations. Ils n’auront alors aucun mal pour répondre aux questions des développeurs parce que les hypothèses auront été validées en amont. L’enjeux pendant les sprints consistera donc à maintenir et défendre la vision des utilisateurs auprès des commanditaires.\nCas 2 - En phase de lancement, la vision produit n’a peu ou pas été défini. La valeur business et/ou technique a pris le dessus. Dans ce cas, les UX doivent se rapprocher des utilisateurs pour comprendre leur ressenti sur le produit. Un atelier réunissant les commanditaires devra aussi permettre de :\n Comprendre et clarifier la vision des commanditaires  Porter la vision des utilisateurs à partir des recherches menées Convaincre que l’**approche UX **constituent une source d’opportunités considérable pour la suite du projet.    Une fois cette passerelle créée, la vision utilisateur prend de la valeur pour venir se mêler à celle du business et de la technique.\nL’UX et l’agile : entre partage et collaboration Au début et pendant les « sprints », il faut rester à l’écoute des utilisateurs car ce sont eux qui décident de l’avenir du produit. Partager les points de vue et collaborer dans la recherche de solution seront les meilleurs moyens pour construire un produit cohérent et visionnaire.\nCes retours d’expériences seront enrichis au fil de l’eau. Et comme en UX on adore les dessins, voici une illustration de la rencontre entre un monde agile et des utilisateurs.\nPour aller plus loin :\nhttps://blog.talanlabs.com/cat/methodologie/ux/\nhttp://www.agiliste.fr/guide-de-demarrage-scrum\n","date":"Sep 14, 2017","href":"https://blog.talanlabs.com/ux-methode-agile-methodologie/","kind":"page","labs":null,"tags":["DevOps","méthodes agiles","Scrum","SCRUX","ux"],"title":"L’UX et l’agile : une histoire de collaboration"},{"category":null,"content":"TalanLabs participe à la Nantes Digital Week 2017. En partenariat avec l’entreprise VIF, notre équipe vous propose un atelier Design Thinking le 21 septembre de 14h à 17h dans les locaux de VIF au 10 rue de Bretagne à La Chapelle sur Erdre.\nNantes, une ville Labellisée \u0026ldquo;Métropole French Tech\u0026rdquo; organise depuis 2014 des moments de partages et de réflexion sur les enjeux liés au numérique dans le cadre de NANTES DIGITAL WEEK.\nParmi les événements de cette semaine, Talan Labs et VIF proposent leur expertise sur le Design thinking.\nLes UX designers appliquent plusieurs techniques et outils pour améliorer l’expérience globale d’un produit ou d’un service. Le Design thinking regroupe toutes ces techniques avec une approche pluridisciplinaire dans un projet d’innovation.\nPendant cet atelier ludique, vous pouvez **tester cette démarche créative et collaborative qui est utilisée pour la conception de nos projets. **\nInscrivez-vous vite !\n","date":"Sep 7, 2017","href":"https://blog.talanlabs.com/nantes-digital-week-2017/","kind":"page","labs":null,"tags":["Design Thinking","Nantes Digital Week","ux"],"title":"Talan Labs acteur de la 4ème édition de la Nantes Digital Week"},{"category":null,"content":"Cet article fait référence au meetup UX Talan Labs qui s’est déroulé le 11 juillet 2017. Pour cette dernière publication de l’été, nous vous faisons découvrir le monde de l’UX design à travers un exemple concret. Rendez-vous à la rentrée pour un nouvel article sur l’UX et la méthode SCRUM.\nL’UX design, centré sur l’expérience utilisateur, permet de proposer des produits répondant exactement aux besoins et aux attentes des consommateurs. Voici l’exemple d’un produit Orangina qui a su parfaitement intégrer l’UX dans la conception de sa nouvelle canette.\nPhase 1 : l’inspiration pour mieux comprendre les utilisateurs L’inspiration revient à observer le comportement de l’utilisateur pour **comprendre ce que’il fait, pense et ressent **lorsqu’il utilise un produit ou un service.\nPrenons un exemple avec une bonne gorgée bien fraîche d’Orangina. Oups, nous avons oublié de secouer la canette. Le résultat ? “Ah ça m’énerve, je vais m’acheter un Coca”.\nNous avons un utilisateur frustré mais une opportunité de conception bien définie : _les consommateurs d’Orangina oublient souvent de secouer la canette, nous avons besoin de supprimer cette frustration quant ils consomment notre produit. _\nPhase 2 : idéation du produit Le problème posé, il faut maintenant :\n Diversifier les profils et réussir à réunir toutes les personnes qui sont, de près ou de loin, concernées par le produit (décideurs, concepteurs, marketing, graphistes, vendeurs, consommateurs) pour générer de nouvelles idées.  Diverger et faire confiance aux probabilités car plus vous allez générer d’idées, plus vous trouverez des solutions. Chronométrer vos réflexions, changer le contexte, éliminer des contraintes ou raisonner par l’absurde sont autant de techniques qui vont vous permettre en un temps très court d’avoir une liste riche d’idées et d’innovations. Converger, voter la ou les solutions qui répondent au mieux à votre problématique.    Phase 3 : implémentation et conception du produit final Cette dernière étape va permettre d’aboutir à un produit final. On commence par proposer un maximum de prototypes basses fidélités et on se rapproche des utilisateurs pour les tester. Le résultat des tests et les décisions des commanditaires vont permettre de converger vers la solution qui répondra au mieux au problème posé.\nDans le cas d’Orangina, la solution choisit est la suivante : imprimer un message à l’envers pour que le consommateur retourne la canette : une solution peu coûteuse et un message fun pour éliminer la frustration des consommateurs.\nEt vous, avez-vous envie de retourner la canette ?\nEn savoir plus :\nhttps://blog.talanlabs.com/5-elements-qui-fabriquent-lux/\nhttps://nextbillion.net/design-thinking-inspire-ideate-and-implement/\n","date":"Aug 1, 2017","href":"https://blog.talanlabs.com/ux-design-orangina/","kind":"page","labs":null,"tags":["expérience utilisateur","méthodes agiles","ux"],"title":"L’UX en application : l’exemple d’Orangina"},{"category":null,"content":"Les grenouilles de JFrog Artifactory ont organisé le 11 Juillet 2017 le Jenkins Community Day à Paris. Voici un compte-rendu de cet événement et des nouveautés présentées.\nLa communauté Jenkins de l\u0026rsquo;outil le plus connu du monde du continuous integration/delivery et DevOps s\u0026rsquo;est regroupée au cœur du 16e arrondissement de Paris pour la venue de Kohsuke Kawaguchi, le créateur de Jenkins, et un des trois membres dirigeants de la plateforme. Avant de rejoindre CloudBees où il officie désormais en tant que Chief Technology Officer, Kohsuke a fait sa carrière chez Sun Microsystems et Oracle, où il a lancé une variété de travaux et notamment le projet open source qui deviendra par la suite Jenkins.\nKohsuke Kawaguchi (CloudBees) et Arnaud Ladrière (JFrog) © Bruno Lamit Photography\nCloudBees, un spécialiste de l’intégration continue d’applications, a livré en début d\u0026rsquo;année au marché une déclinaison dite « entreprise » de Jenkins, CloudBees Jenkins Enterprise. Cette plateforme a pour vocation de proposer une version certifiée et validée de la souche Open Source et de l’entourer d’une offre de support professionnel.\nLors de cette journée, Cloudbees nous a présenté les derniers nouveautés de Jenkins 2\nLes pipelines As Code Un fichier Jenkinsfile contenu à la racine du projet contient le script Groovy de définition du pipeline. Il est donc versionné avec le reste du code. C\u0026rsquo;est une révolution comparée aux anciennes méthodes dite \u0026ldquo;à papa\u0026rdquo; qui consistaient à ajouter manuellement dans l\u0026rsquo;interface d\u0026rsquo;administration la liste ordonné des plugins qu\u0026rsquo;il fallait ensuite paramétrer.\nLe plugin Blue Ocean La **refonte graphique de l\u0026rsquo;interface **avec le plugin Blue Ocean très pratique pour les projets multi-branches.\nBlue Ocean repense (enfin) l\u0026rsquo;UX de Jenkins :\n Visualisation des pipelines de continuous delivery (CD) qui permet une compréhension rapide et intuitive du status des builds et l\u0026rsquo;acces aux logs Éditeur de pipeline Personnalisation par rôle pour chacun des membres de l\u0026rsquo;équipe Intégration native des branches et des pull requests  Des images Docker pour \u0026ldquo;Slave Jenkins\u0026rdquo; La virtualisation avec Docker est en pleine effervescence sur la planète DevOps ! L\u0026rsquo;utilisation d\u0026rsquo;images Docker pour générer les slaves Jenkins à la demande pourrait révolutionner votre façon de voir les chaines d\u0026rsquo;intégration continue (documentation).\ndockerNode(\u0026lt;image: maven:3.3.3-jdk-8, sideContainers: []) { git https://github.com/wakaleo/game-of-life sh mvn clean test } Et pour finir cette agréable journée, nous avons fait un tour de table sur le futur et le devenir de Jenkins avec les différents speakers (Kohsuke Kawaguchi et Nicolas De Loof de Cloudbees, Arnaud Ladière de JFrog, Guillaume Laforge de Google, Quentin Adam de Clever Cloud pour les plus connus).\nMerci aux grenouilles d\u0026rsquo;avoir organisé cet événement parisien ! Si vous l’avez manqué, il y a le Jenkins World à San Francisco.\n","date":"Jul 18, 2017","href":"https://blog.talanlabs.com/jenkins-community-day-paris-2017/","kind":"page","labs":null,"tags":["Cloud","continuous delivery","continuous integration","DevOps","Docker","Jenkins"],"title":"Jenkins Community Day Paris 2017"},{"category":null,"content":"Android Things , l’OS Google pour la gestion des objets connectés a été lancé en décembre 2016. Ce projet s’inscrit dans la continuité du projet Brillo.\nCertaines APIs d’Android sont absentes, mais en contrepartie des APIs spécifiques à cette version permettent d’exploiter le matériel.\nUne seule application peut être déployée par appareil, les « System apps » n\u0026rsquo;étant pas présentes. Elle sera lancée automatiquement au démarrage.\nDes « developers kits » sont disponibles, comme le Raspberry Pi3 accompagné d’accessoires tels que le Rainbow Hat spécialement conçu pour Android Things, et qui embarque des leds, des boutons capacitifs, des capteurs de pression et de température, un vibreur, un afficheur numérique, des « pins » pour connecter des servomoteurs, etc. Tous ces composants sont utilisables via des APIs et permettent de réaliser par exemple une station météo, une sonnette intelligente mais aussi des produits plus complexes.\nCe SDK (Software Development Kit) permet également d’exploiter Weave (l’infrastructure cloud qui permet de relier les objets connectés aux services Google et d’utiliser Google Assistant) ainsi que Google Cloud Platform. Il est tout à fait possible de visualiser un flux vidéo sur une application mobile, à partir des images capturées par une caméra reliée au Raspberry Pi. . Le site android experiments mets en avant un périphérique permettant de détecter les animaux sauvages, d’enregistrer une image de leur passage ainsi que des relevés environnementaux.\n","date":"May 24, 2017","href":"https://blog.talanlabs.com/android-things-iot-google/","kind":"page","labs":null,"tags":["Android","API","Brillo","Google","Google Things","IoT"],"title":"Android Things : l’IoT par Google"},{"category":null,"content":"La vidéo de la conférence microservices TalanLabs réalisée pendant Devoxx 2017 est en ligne !\nRevivez la conférence de François Berthault, Cyril Deverson et Christophe Thépaut « Cloudifie ton monolithe ! Les microservices pour une architecture comme les grands du web ».\n\nFélicitations à François, Cyril et Christophe pour la présentation et à l’équipe Devoxx pour le montage vidéo.\nRedécouvrez également la série « Les microservices pour une architecture orientée web » de François.\n","date":"May 4, 2017","href":"https://blog.talanlabs.com/conference-microservices-devoxx/","kind":"page","labs":null,"tags":["conférence microservices","Devoxx","Video"],"title":"Revivez la conférence microservices de Talan Labs lors de Devoxx 2017"},{"category":null,"content":"Vers toujours plus d’agilité dans le déploiement des nouvelles technologies Dans l’interview précédente, nous avons évoqué le mode d’organisation agile de Talan Labs. Cette organisation est-elle facilement applicable chez vos clients ? En théorie, oui : la majorité des clients souhaitent de l’agilité ! Dans les faits, c’est plus complexe. Tout d’abord parce que le mode agile n’est pas simple à contracter puisqu’il va à l’encontre des contrats historiques, au projet ou en régie. Ensuite, parce que de nombreux clients, s’ils ont compris que la transformation digitale n’était plus une option, souhaitent se digitaliser « à leur rythme ». Bien souvent pour des raisons d’inertie organisationnelle.\nEn mode agile, des freins peuvent aussi apparaître dans les équipes IT elles-mêmes. En DevOps, la suppression des silos entre le développement et l’opérationnel peut ainsi créer des tensions et des crispations. Dès lors, il faut savoir** accompagner les équipes**, et leur expliquer les implications, dans leur quotidien, des nouveaux modes d’organisation.\nLe constat est d’ailleurs le même du côté de l’AMOA (la maitrise d’ouvrage). La vague technologique touche toutes les strates de l’entreprise, jusqu’à la direction générale. Résultat : les métiers de l’AMOA se dirigent inexorablement vers la notion d’expérience utilisateur. Les équipes AMOA verront leur mission évoluer vers les nouveaux enjeux de l’UX. C’est dans cette perspective que nous cultivons chez Talan en général et particulièrement au sein du Labs des compétences UX. Pour répondre à ces évolutions, nous proposons l’organisation d’ateliers UX à nos clients, afin de les accompagner au plus près dans leurs enjeux de transformations digitales et de nouveaux modes d’organisation et de management.\nQuelles sont les orientations stratégiques de Talan Labs en termes de nouvelles technologies ? Nos priorités sont multiples et s’intègrent aux tendances du marché, tout en restant cohérentes avec** nos savoir-faire**. En ce moment, nous menons par exemple des travaux autour de l’Internet des Objets (IoT). Pas sur les objets ou capteurs en eux même, mais sur l’usage qu’il peut être fait des données récoltées. Car en matière d’objets connectés, il est important de rappeler que l’innovation ne réside pas seulement dans l’objet ou le capteur en lui-même mais la capacité à traiter et à utiliser la donnée. Cet aspect est donc au cœur de nos priorités métiers.\nParmi les autres technologies, nous étudions également les possibilités du Big Data et de ses usages inhérents, tels que le Machine Learning ou encore le Deep Learning. Pour le moment nous sommes en veille active sur le sujet. Il est cependant important de souligner que dans de nombreux cas, une adaptation de technologies de type NoSQL peut suffire au traitement des données volumineuses.\nQuid de la Blockchain ? C’est un sujet passionnant, porté par la direction du groupe Talan. Plusieurs projets sont donc en train de voir le jour en interne. Cependant, la technologie est encore jeune, et de nombreuses questions restent en suspens comme par exemple, l’impossibilité de supprimer un élément de la chaîne une fois intégré.\nQuoi qu’il en soit, les travaux menés nous permettent d’avancer plus largement sur le thème de la sécurité, et en particulier sur les questions de cryptographie. Il s’agit pour l’instant de travaux internes, qui pourraient cependant déboucher prochainement sur des projets clients. L’objectif est d’être prêt pour répondre aux demandes du marché sur le sujet.\nRetrouvez la première partie de l\u0026rsquo;interview de Nicolas Thomas, Directeur Général de Talan Labs 3 questions à Nicolas Thomas - Partie 1 : Un esprit de start-up au service de nos clients.\n","date":"May 2, 2017","href":"https://blog.talanlabs.com/3-questions-nicolas-thomas-agilite-ntic-partie-2/","kind":"page","labs":null,"tags":["blockchain","DevOps","méthode agile","Nicolas Thomas","TalanLabs","ux"],"title":"3 questions à Nicolas Thomas - Partie 2"},{"category":null,"content":"Un esprit de start-up au service de nos clients Talan Labs a 18 mois ! Dix-huit mois après sa création, Nicolas Thomas, Directeur Général de Talan Labs, fait le point sur la vision technologique, organisationnelle et métier de l’entité.\nCet entretien est divisé en deux parties : la première présente la culture et l’organisation de Talan Labs pour répondre aux enjeux de transformation digitale de ses clients et la deuxième aborde les nouvelles technologies et leur déploiement chez les clients.\nQuel regard portez-vous sur les 18 mois écoulés ? Vaste question à laquelle il n’est pas simple de répondre… Dans l’ensemble, nous avons essayé de créer de la cohérence dans une nouvelle entité qui regroupe des équipes habituées au mode projet d’une part, et plus orientées régie d’autre part. Plus largement, le rapprochement de plusieurs cultures d’entreprise est un défi que nous tentons de relever au quotidien.\nPar quels moyens par exemple ? L’implication et la responsabilisation ! Cela passe par exemple par une délégation des entretiens annuels au plus près des collaborateurs : les chefs de projets, associés aux managers, sont invités à opérer eux-mêmes les entretiens de leur équipe. Outre la pertinence de l’exercice du fait de la proximité immédiate du chef de projet avec ses collaborateurs, c’est aussi l’occasion de responsabiliser l’ensemble des équipes et de les engager dans un projet commun d’entreprise.\nPar ailleurs nous sommes convaincus que la passion et la curiosité sont les leviers de la motivation, c’est pourquoi nous soutenons des initiatives internes qui vont des meetup aux conférences en passant par les ateliers, formations ou projets internes. Nous y voyons deux objectifs :\n Identifier les expertises et leader techniques capables de les animées  Créer une vraie dynamique permettant à tous les consultants de s’exprimer en fonction de ses compétences et surtout en fonction de ses appétences.    Enfin, sur les projets nous cherchons à développer la culture autour de l’erreur. Celle-ci consiste à ne pas considérer l’erreur comme un échec, mais au contraire comme un moyen d’avancer. Dans les rétrospectives des problèmes opérationnels nous nous attachons à étudier et partager la réaction et l’implication des équipes à la résolution du problème plutôt que d’identifier le responsable. Cette culture est portée par la solidarité et la bienveillance au sein des équipes qui travaillent main dans la main sur un même projet. Assumer solidairement et corriger les erreurs ensemble, c’est aussi partager les succès !\nUn véritable esprit start-up en quelque sorte ? Exactement ! Je suis d’ailleurs convaincu du bien-fondé de l’organisation horizontale et en équipes des start-ups. Cette culture de l’initiative et la responsabilisation apporte une véritable émulation au sein des équipes, et donc des capacités d’innovation remarquables. L’objectif prioritaire à garder en tête est l’efficacité, quitte à d’ailleurs déroger à certaines règles pré-établies et à des carcans néfastes à la créativité.\nBien sûr, Talan Labs n’est pas une start-up… d’où l’idée de créer une organisation de type FabLab afin de limiter au maximum les barrières à l’innovation. Chacun de ces laboratoires serait alors responsable d’un sujet ou d’un domaine en particulier et fonctionnerait avec l’agilité nécessaire pour faire avancer les projets. Ces nouveaux modes d’organisation inspirent déjà certains de nos clients, de grands groupes industriels ou financiers, qui s’en empreignent afin d’inventer le management de projets de demain.\nRetrouvez la seconde partie de l\u0026rsquo;interview de Nicolas Thomas, Directeur Général de Talan Labs 3 questions à Nicolas Thomas - Partie 2 : Vers toujours plus d’agilité dans le déploiement des nouvelles technologies.\n","date":"Apr 25, 2017","href":"https://blog.talanlabs.com/3-questions-nicolas-thomas-esprit-start-up-partie-1/","kind":"page","labs":null,"tags":["FabLab","méthode agile","Nicolas Thomas","TalanLabs"],"title":"3 questions à Nicolas Thomas - Partie 1"},{"category":null,"content":"Devoxx, l’événement des développeurs logiciels, s’est terminé le 7 avril dernier. Après trois jours de conférences,** TalanLabs**, sponsor platinium de cette 6e édition, fait le bilan chiffré de Devoxx 2017.\nEn quelques chiffres, Devoxx 2017 c’est :\n  **3 **jours\n  **48 **exposants + 12 startups\n  220 conférences dont 192 filmées\n  237 orateurs\n  2 890 participants par jour\n  420 participants au Meet \u0026amp; Greet du jeudi soir\n  7 900 repas et 22 000 bouteilles d’eau\n   de **30 **organisateurs et bénévoles    1 sharing box Talan Labs et 164 .gifs animés partagés\n  **des milliers **de goodies Talan Labs distribués\n  Le paninoxx des organisateurs de Devoxx © TalanLabs\nEt c’est également 1 conférence sur les microservices, « Cloudifie ton monolithe : les microservices pour une architecture comme les grands du web », animée par François Berthault.\nFrançois, Cyril et Christophe avant et pendant la conférence « Cloudifie ton monolithe » ©TalanLabs\nDécouvrez les slides de la conférence ci-dessous et redécouvrez notre série « Les microservices pour une architecture orientée web ».\nMerci à tous d’être venus partager un moment avec nous sur notre stand et un grand bravo à tous les organisateurs Devoxx pour leur disponibilité et leur énergie.\nRendez-vous l’année prochaine pour Devoxx 2018 !\n","date":"Apr 13, 2017","href":"https://blog.talanlabs.com/talanlabs-bilan-devoxx-2017-chiffres-images/","kind":"page","labs":null,"tags":["conférence microservices","Devoxx","sponsor platinium"],"title":"Talan Labs fait le bilan de Devoxx 2017 en chiffres et en images"},{"category":null,"content":"Talan Labs est pour la seconde année consécutive sponsor platinium de l’événement des développeurs de logiciels en France : Devoxx. Du 5 au 7 avril, venez rencontrer les équipes TalanLabs (stand P4) et gagner de nombreux goodies.\nDans le cadre de notre série « Les microservices pour une architecture orientée web », François Berthault, Technical \u0026amp; coding architect chez Talan Labs, animera lors de Devoxx le jeudi 6 avril à 17h10, une conférence intitulée « Cloudifie ton monolithe : les microservices pour une architecture comme les grands du web ».\nVenez parler métier et échanger vos expériences avec toute l’équipe TalanLabs, stand P4.\nDécouvrez le programme de Devoxx 2017 et inscrivez-vous à l’événement dès maintenant.\nPour en savoir plus, consultez le communiqué de presse Devoxx 2017 de Talan Labs.\n","date":"Mar 31, 2017","href":"https://blog.talanlabs.com/talanlabs-sponsor-platinium-devoxx-2017/","kind":"page","labs":null,"tags":["conférence microservices","Devoxx","sponsor platinium"],"title":"Talan Labs sponsor platinium de Devoxx 2017"},{"category":null,"content":"Mathieu Mélé, chef de projet technique chez Talan Labs et Vivien Auguy, ingénieur également chez Talan Labs, nous parlent de la méthode Talan Labs en présentant le projet innovant de blockchain du Groupe Talan baptisé TalanCoin.\nUne méthodologie basée sur la connaissance, la R\u0026amp;D et l’expérimentation La « méthode TalanLabs » est basée sur le partage des connaissances, des tests grandeur nature sur des technologies de pointe et sur une gestion de projet toujours plus** agile**. « Nous avançons avec le client pour l’orienter dans le choix de la meilleure technologie et veillons à la facilité d’utilisation des solutions proposées et leur modularité pour les faire évoluer. Nous pouvons aussi leur proposer des solutions complètement innovantes, lorsqu’ils sont prêts à nous suivre. », explique Mathieu.\nPour maîtriser les nouvelles technologies et présenter des solutions concrètes à leurs clients, les équipes de Talan Labs développent leurs propres projets : « _nous fonctionnons sur le modèle d’un incubateur interne _», détaille Mathieu.\nTalanCoin, un projet vitrine de l’expertise TalanLabs En s’inspirant de ce modèle, Talan Labs travaille - en collaboration avec plusieurs équipes du Groupe Talan - sur un projet de blockchain, technologie qui intéresse particulièrement les entreprises de la finance. Pour ce faire, une monnaie virtuelle, le TalanCoin, a été développée. Elle sera à terme utilisée par l’ensemble des salariés du groupe Talan, via une application mobile. Les salariés pourront ainsi convertir leurs TalanCoins en avantages non-monétaires, comme du temps ou des formations par exemple.\n« _Les clients sont intéressés par la nouveauté mais prudents sur des sujets qu’ils ne maîtrisent pas ou des technologies récentes sur lesquelles il y a peu ou pas de retour d’expérience. Un projet comme le TalanCoin nous permet de montrer au client les possibilités de la technologie, l’utilisation que nous en faisons et notre expertise sur le sujet _», souligne Vivien.\nLe projet TalanCoin implique tous les différents métiers chez Talan Labs. Il change radicalement la manière de travailler des équipes, notamment en favorisant les échanges décentralisés et collaboratifs.\nPour en savoir plus sur le projet TalanCoin, regardez la vidéo de présentation de Medhi Houas, Président du groupe Talan :\n","date":"Mar 30, 2017","href":"https://blog.talanlabs.com/methode-talanlabs/","kind":"page","labs":null,"tags":["blockchain","innovation","projets","Talan","TalanCoin","technologies"],"title":"Mathieu et Vivien parlent de la « méthode Talan Labs »"},{"category":null,"content":"5 points essentiels à comprendre pour s’initier aux microservices Cet article s’insère dans la série « microservices » écrite par François Berthault dans le cadre de la participation de Talan Labs à Devoxx 2017, du 5 au 7 avril prochains au Palais des Congrès de Paris.\nÀ l’heure de la digitalisation et du tout agile, les applications monolithiques semblent inéluctablement vouées à disparaître au profit d’architectures logicielles en microservices. Plus souples, elles répondent aux besoins d’évolution rapides. Mais la philosophie et l’organisation des projets doivent s’accorder. Quelques astuces pour bien démarrer avec les microservices.\n1. Les microservices ou comment éviter l’obsolescence des monolithes Depuis des années, pour ne pas dire des décennies, développer une application se faisait dans la très grande majorité des cas avec une plateforme logicielle unique. On parle alors de monolithes.\nSimples par certains côtés (une seule technologie à maîtriser), ces monolithes sont forcément plus complexes à faire évoluer, tant leurs fonctionnalités sont interdépendantes. Parfois même la correction d’une simple faute d’orthographe peut prendre une journée entière !\nAu contraire, plus souple et plus agile, l’**architecture en microservices **offre toute latitude aux développeurs pour ajouter ou modifier des fonctionnalités, faire évoluer l’application pour améliorer l’expérience utilisateur, etc. Le tout très rapidement et sans risque pour le reste des fonctionnalités. En outre, chaque microservice va pouvoir être développé avec le langage qui correspond le mieux à ses fonctions.\nEn bref, l’architecture logicielle en microservices, c’est l’assurance de disposer en permanence d’une application à jour et en phase avec les attentes des utilisateurs.\n2. Qualité et agilité : les bases attendues d’une architecture en microservices Faire du microservices pour du microservices ne sert à rien. Encore faut-il adhérer aux objectifs inhérents à ce type d’architecture. À savoir tout d’abord et comme on l’a vu, gagner en agilité. En termes de développement comme en termes de maintenance fonctionnelle et corrective.\nEnsuite, gagner en qualité. Le risque d’une application monolithique est en effet une dilution des responsabilités entre l’ensemble des développeurs, et surtout une certaine cécité : l’application étant tellement gigantesque que chaque développeur n’est jamais certain que ses actions ne risquent pas d’endommager le reste de l’application.\nÀ l’inverse, une application développée en microservices gagne véritablement en qualité : plus faciles à documenter, ces derniers seront d’autant plus faciles à maintenir ou faire évoluer individuellement.\n3. Ne pas sous-estimer la complexité d’une architecture en microservices À première vue, on pourrait croire à un système miraculeux tant les avantages semblent nombreux, ce qui est le cas. Mais aussi miraculeux soient-ils, les microservices n’enlèvent rien à la complexité des applications : ils en déplacent simplement le centre de gravité.\nCar pour répondre aux besoins et aux processus métiers, les interactions devront toujours exister, les tests effectués, les déploiements assurés, et les fonctionnalités maintenues, etc.\n4. Apprendre à découper une application On l’aura compris, des modules plus petits facilitent la maintenabilité d’une application. Néanmoins, attention à une découpe trop granulaire qui conduirait à la création de nanoservices !\nEn bref, pour un découpage fonctionnel efficace, il est impératif, encore plus que dans le cadre d’une application monolithique, que l’équipe de développeurs comprennent parfaitement le métier et les usages que doit remplir l’application. C’est à cette condition que le découpage sera juste et cohérent, sans complexifier l’application plus que de raison.\n5. Avec les microservices, il faut penser collaboratif ! En plus de transformer l’infrastructure logicielle d’une application, les microservices vont également modifier la façon d’aborder la création d’applications. Descendue d’une certaine tour d’ivoire, la DSI doit plus que jamais (c’est déjà de plus en plus le cas) s’intégrer aux métiers.\nCar pour bien réussir une migration vers des architectures en microservices, il est essentiel d’étendre à l’ensemble de l’entreprise ce qui fait la force du DevOps en interne à la DSI : une communication et une collaboration toujours plus étroites entre toutes les parties prenantes, et où la maîtrise d’ouvrage est finalement de la responsabilité de chacun.\nVous avez aimé cet article ? Découvrez ou redécouvrez les autres épisodes de la série « Les microservices pour une architecture orientée web » :\n_Partie 1 _Les microservices pour une architecture orientée web n°1 : Définitions et caractéristiques\n_Partie 2 _Les microservices pour une architecture orientée web n°2 : Un changement de point de vue\n_Partie 3 _Les microservices pour une architecture orientée web n°3 : Organisation des équipes pour une projet d’architecture en microservices\n_Partie 4 _Les microservices pour une architecture orientée web n°4 : Simple comme Spring Boot\n_Partie 5 _Les microservices pour une architecture orientée web n°5 : Spring Cloud, le couteau suisse des microservices\n","date":"Mar 27, 2017","href":"https://blog.talanlabs.com/microservices-partie-6-points-essentiels/","kind":"page","labs":null,"tags":["applications","architecture web","DevOps","méthodes agiles","Microservices"],"title":"Les microservices pour une architecture orientée web n°6"},{"category":null,"content":"Spring Cloud, le couteau suisse des microservices Cet article s’insère dans la série « microservices » écrite par François Berthault dans le cadre de la participation de Talan Labs à Devoxx 2017, du 5 au 7 avril prochains au Palais des Congrès de Paris.\nNous avons vu dans l’épisode précédent comment implémenter un microservice grâce à Spring Boot.\nUne nouvelle question se pose : Comment orchestrer et mettre à disposition un parc de microservices qui pourrait être scalable et déployer sur différents serveurs ? Et pour se faire, nous pouvons utiliser Spring Cloud.\n© François Berthault\nAPI Gateway : un design pattern pensé pour le Cloud Spring Cloud propose plusieurs API Gateway simples à mettre en place (NetFlix, Consul et Zookeper).\nIntéressons-nous à celle de Netflix OSS (Open Source Software). Quand la célèbre société de streaming vidéo à débuter sur les plateforme d’Amazon AWS, les équipes de Netflix ont mis au point leur propre API Gateway, répondant notamment aux problématiques de la distribution volatile sur le Cloud (IP dynamique, scalabilité en temps réels, data-center, etc.). Novateur dans le domaine des architectures émergeantes, Netflix OSS offre régulièrement à la communauté open source de nombreux projets (plus d’une cinquantaine) comme Hystrix, Chaos Monkey, Eureka ou Zuul.\nCette API Gateway se compose de deux éléments :\n Eureka : le service d’annuaire (discovery service ou registry service). Les instances d’Eureka peuvent s’enregistrer et les clients peuvent être découverts par les instances.  Zuul : le proxy/routeur intelligent (edge service).    Spring Cloud ajoute une surcouche à Eureka et Zuul pour les rendre transparents et complètement intégrés à l’écosystème Spring.\n© François Berthault\nC’est assez simple à comprendre :\n Étape 1 : Chacun des microservices embarque la librairie discovery-client et va s’enregistrer dans un serveur Eureka au démarrage. Étape 2 : Zuul va lui découvrir l’ensemble des microservices connus par Eureka. Étape 3 : Zuul va mettre en place les routes http vers les microservices, jouer le rôle de load balancer et les protéger avec Hystrix (circuit breaking). Étape 4 : Une fois l’initialisation terminée, Zuul exposera de nouvelles uri, afin de rendre accessible les microservices où qu’ils soient.  Spring Cloud Eureka fournit aussi un tableau de bord de supervision des instances de microservices :\nUn serveur de config pour simplifier les déploiements Afin d’ajouter un service de configuration pour centraliser et déployer en temps réel les configurations des microservices, un simple dépôt Git suffit. Il stockera l’ensemble des propriétés d’environnement et remplacera les multiples fichiers de configuration. Au démarrage, chacun des processus viendront récupérer les informations dont ils ont besoin. Plus besoin de déployer manuellement un fichier de configuration par instance de microservice_._\n© No Fluff Just Stuff : blog.nofluffjuststuff.com\nSpring Cloud est la boite à outil indispensable à vos microservices Nous avions vu la simplicité avec laquelle il était possible de construire un microservice complet grâce à Spring Boot dans l’article précèdent. Maintenant, l’orchestration et la configuration des microservices sont choses faites avec Spring Cloud. Il ne vous reste plus qu’à vous lancer.\nEn complément, voici une vidéo de l’excellente conférence de Josh Long (Spring Advocate \u0026amp; JAVA Champion) lors du Devoxx France 2016 nommé « Bootiful microservice ».\nDes exemples de codes pour votre architecture microservices sont disponibles sur Github. Vous y trouverez également un exemple complet de déploiement de microservices via Spring Cloud (jusqu’à l’orchestration des images Docker avec docker-compose).\nVous avez aimé cet article ? Découvrez ou redécouvrez les autres épisodes de la série « Les microservices pour une architecture orientée web » :\n_Partie 1 _Les microservices pour une architecture orientée web n°1 : Définitions et caractéristiques\n_Partie 2 _Les microservices pour une architecture orientée web n°2 : Un changement de point de vue\n_Partie 3 _Les microservices pour une architecture orientée web n°3 : Organisation des équipes pour une projet d’architecture en microservices\n_Partie 4 _Les microservices pour une architecture orientée web n°4 : Simple comme Spring Boot\n","date":"Mar 23, 2017","href":"https://blog.talanlabs.com/microservices-partie-5-spring-cloud/","kind":"page","labs":null,"tags":["Cloud","framework","Microservices","Spring Cloud"],"title":"Les microservices pour une architecture orientée web n°5"},{"category":null,"content":"Simple comme Spring Boot Cet article s’insère dans la série « microservices » écrite par François Berthault dans le cadre de la participation de Talan Labs à Devoxx 2017, du 5 au 7 avril prochains au Palais des Congrès de Paris.\nMaintenant que l'architecture en microservices n\u0026rsquo;a plus de mystère pour vous, nous pouvons nous intéresser à son implémentation. Et comme nous l\u0026rsquo;avons vu dans un article précédent, il existe de** nombreux frameworks** parmi lesquels vous pouvez faire votre choix. Si par exemple vous ne jurez que par Guice, Dropwizard est fait pour vous. Spring Boot reste tout de même la meilleure solution. C’est le projet qui a rendu à Spring sa simplicité d’origine (et sans XML).\nSpring Boot : le super couteau suisse Si votre ordinateur est équipé d\u0026rsquo;un JDK8 et d\u0026rsquo;un Maven3 (ou d\u0026rsquo;un Gradle), alors vous êtes prêt !\nSpring fournit un site de configuration automatique start.spring.io pour les développeurs Java, Groovy ou Kotlin pressés. Mais prenons le temps d\u0026rsquo;analyser les éléments qui composent un microservice Spring Boot.\nPour rappel, le framework Spring est un conteneur dit léger qui permet d’obtenir une infrastructure similaire à celle d’un serveur d’application Java EE plus lourd. Spring Boot est un framework qui permet d\u0026rsquo;obtenir une application Spring (microservice) avec un minimum d\u0026rsquo;effort.\n\u0026ldquo;POM\u0026rdquo;, \u0026ldquo;BOM\u0026rdquo; et dépendances Le point de départ est le \u0026ldquo;modèle objet projet\u0026rdquo; (appelé POM pour Project Object Model). Il contient une description détaillée de votre projet. La solution la plus simple est un héritage de la dépendance \u0026ldquo;spring-boot-starter-parent\u0026rdquo;. Seulement, vous n\u0026rsquo;aviez pas l\u0026rsquo;intention de réserver cette place unique et privilégiée à Spring Boot, votre équipe possède certainement déjà son \u0026ldquo;parent pom\u0026rdquo;. Pas de panique, Spring a pensé à tout !\nSpring, Jersey et d\u0026rsquo;autres projets commencent à utiliser ce procédé d'import de dépendance. Cette utilisation d\u0026rsquo;un BOM (pour Bill Of Materials) permet d\u0026rsquo;ajouter une dépendance de type POM dans le scope import de Maven. Cette fonctionnalité mal connue est pourtant présente depuis 2008.\nMake JAR not WAR La magie de Spring Boot réside dans le packaging de votre application. C\u0026rsquo;est par défaut un jar \u0026ldquo;self-contained\u0026rdquo; (jar exécutable contenant l’ensemble des dépendances de l’application - aussi appelé \u0026ldquo;fat jar\u0026rdquo;). Le package est effectué par le plugin Maven \u0026ldquo;spring-boot-maven-plugin\u0026rdquo;.\nLes starters Le projet Spring Boot fournit un ensemble de starters (plus de 150 sont disponibles sur start.spring.io). Un starter est un kit de dépendances auto-configurables. Ajouter la dépendance à \u0026ldquo;spring-boot-starter-web\u0026rdquo; et votre projet devient un mini-serveur Tomcat auto-porté dans votre jar, pouvant également exposer des ressources REST, sans la moindre configuration.\nEt si vous ne voulez pas de Tomcat et que votre serveur d\u0026rsquo;application préféré est Jetty, une simple exclusion de la dépendance \u0026ldquo;spring-boot-starter-tomcat\u0026rdquo;, l\u0026rsquo;ajout de** \u0026ldquo;spring-boot-starter-jetty\u0026rdquo;** et le tour est joué.\nCréation d’une application microservice avec Spring Boot Une classe main Subtilement placée dans le plus petit sous-package du code, la classe principale annotée uniquement @SpringBootApplication va automatiquement scruter vos dépendances et activer toutes les fonctionnalités présentes. Dans l’exemple, la présence de \u0026ldquo;spring-boot-starter-web\u0026rdquo;, activera et configurera les briques de base de Spring-Core (dont le \u0026ldquo;scan-package\u0026rdquo;), Tomcat et JAX-RS (pour la partie REST). Cela fonctionne également pour tous les autres starters de spring-boot (du moins si votre classe principale est bien placée).\n **\u0026ldquo;Convention over Configuration\u0026rdquo; **__@SpringBoot\u0026rsquo;s Team\n Un controller REST \u0026ldquo;Spring-mvc\u0026rdquo; est l\u0026rsquo;implémentation par défaut de JAX-RS. Ajoutez la dépendance \u0026ldquo;spring-boot-starter-jersey\u0026rdquo;, si vous préférez celle de Jersey.\nAdieu les fichiers XML, bonjour \u0026ldquo;application.yml\u0026rdquo; L\u0026rsquo;ensemble des propriétés sont disponibles sur l'Appendix Spring BOOT. La liste est longue, n\u0026rsquo;ajoutez dans votre projet que les propriétés dont vous voulez surcharger les valeurs par défaut.\n3, 2, 1 \u0026hellip; compilez, packagez et lancez vous ! Et voilà, votre microservice a démarré.\nQuelques Features indispensables Actuator Actuator vous permet de monitorer et d\u0026rsquo;interagir avec votre application. Il ajoute :\n  des endpoints comme :\n* **/health** : les informations sur l'état de santé, * **/info** : la description de votre application (nom, version, commit, ...), * **/env** : les valeurs des propriétés spring, * **/logfile** : pour voir les dernières logs dans votre navigateur web par exemple (qu'il est pratique de ne plus se connecter sur le serveur et d'ouvrir le bon fichier log), * des métriques sur les requêtes HTTP et sur l'état de la JVM ...    des outils de monitoring comme ApplicationPidFileWriter qui créé un fichier contenant le PID de l\u0026rsquo;application (très utile pour vos OPS) et son équivalent pour le port d\u0026rsquo;exécution EmbeddedServerPortFileWriter.\n  Et si vous ne voulez pas l\u0026rsquo;exposer en REST, Spring Boot vous le permet aussi en Telnet, en JMX et en SSH.\nLa gestion des logs Spring Boot vous fournira un kit complet de gestion des logs (dépendances et configuration de logback) pour aller directement en production. Si votre équipe est « DevOps », que vous indexez vos fichiers de logs afin de les exploiter, Spring ne vous a pas oubliez. L’intégration avec des outils comme Splunk ou ELK (ElasticSearch, Logstash et Kibana) a été mis en place.\nSpring Boot DevTools Cet outil vous permettra de redémarrer automatiquement votre application à chacune des modifications de votre code pendant la phase de développement. Que de temps autrefois perdu à redémarrer son serveur.\nEt le jar se transforma en un bash\u0026hellip; L\u0026rsquo;idéal serait maintenant d\u0026rsquo;avoir un script de démarrage (start/stop/status) afin d\u0026rsquo;éviter les commandes disgracieuses de type \u0026ldquo;java –jar\u0026rdquo; et \u0026ldquo;kill -1\u0026rdquo; pour stopper notre service. Spring a encore pensé à tout puisqu’il package le jar avec un lanceur !\nLa variable **\u0026ldquo;server.port\u0026rdquo; **du fichier de configuration \u0026ldquo;application.yml\u0026rdquo; a ainsi été surchargée par une variable d\u0026rsquo;environnement.\nMême Docker est prévu À l\u0026rsquo;aide d\u0026rsquo;un simple plugin Maven\u0026hellip;\n\u0026hellip;et d\u0026rsquo;un Dockerfile ! Votre application se transformera en une image Docker prête à l\u0026rsquo;emploi.\n``Il ne reste plus qu\u0026rsquo;à lancer votre image\u0026hellip;.\n Quand une développeur et un sysadmin discute\nSpring Boot : un outil simple, parfait pour créer des microservices L\u0026rsquo;outillage de Spring Boot est vraiment impressionnant et comporte de nombreuses autres fonctionnalités comme l\u0026rsquo;intégration de Flyway, d'Elastic Search, de MongoDB, de Kafka, de la sécurité, du projet** Spring Data JPA**, de l'Hateoas, de Logstash et de très nombreux autres.\nL\u0026rsquo;expérience et la maturité de l’éco-système Spring fait ses preuves. Au niveau de la dynamique, des projets et des versions, rien à redire, l’outil est très vivant. D\u0026rsquo;ailleurs, on attend spring-boot 2.0 en version RELEASE pour bientôt !\nProchaine étape : Spring Cloud !\n© Ryan Baxter : ryanjbaxter.com\nVous avez aimé cet article ? Découvrez ou redécouvrez les autres épisodes de la série « Les microservices pour une architecture orientée web » :\n_Partie 1 _Les microservices pour une architecture orientée web n°1 : Définitions et caractéristiques\n_Partie 2 _Les microservices pour une architecture orientée web n°2 : Un changement de point de vue \n_Partie 3 _Les microservices pour une architecture orientée web n°3 : Organisation des équipes pour une projet d’architecture en microservices\n","date":"Mar 14, 2017","href":"https://blog.talanlabs.com/microservices-partie-4-spring-boot/","kind":"page","labs":null,"tags":["framework","Microservices","Spring Boot"],"title":"Les microservices pour une architecture orientée web n°4"},{"category":null,"content":"Avec la user experience (UX), on place l’utilisateur au centre de la conception. C’est lui qui va participer à la définition et à la redéfinition d’un service. Alors comment parler d’UX sans parler du MVP : le « Minimum Viable Product » ?\nQu’est-ce que le MVP et pourquoi l’intégrer dans une approche d\u0026rsquo;expérience utilisateur ? Un MVP fait partie d\u0026rsquo;une stratégie de développement de produit, c\u0026rsquo;est un prototype que l\u0026rsquo;on va utiliser pour valider des hypothèses. C’est Eric Ries, Mentor du Lean Startup qui a fortement contribué au développement de cette approche. Le Lean Startup est tout simplement la méthode que les jeunes pousses utilisent pour gagner en efficacité : on construit, on mesure, on apprend. Et on itère sur cette base !\nL’objectif ? Réduire le Time To Market : gagner du temps avec un minimum de ressources pour réduire les coûts.\n© Incubateur Jean Moulin – Université Lyon 3\nAlors c’est parti ! Vous avez l’idée ? Voici la démarche.\nÉtape 1 : Valider le concept Temps : 5 jours, large.\nRessources : Des parties prenantes, une population d’utilisateurs cible.\n Considérer l’idée comme une hypothèse à valider. 2. Mélanger les utilisateurs au concept grâce à un P.O.C (Proof Of Concept) 3. Valider ou pivoter sur les hypothèses émises à partir des retours utilisateurs  Un exemple ? Dropbox, 0 lignes de code + une vidéo de présentation = 75 000 pré-inscriptions. Job Is Done, il est temps de passer à l\u0026rsquo;action !\nÉtape 2 : Créer le MVP Temps : 5-6 semaines\nRessources : Product Owner, Scrum, UX, UI, Dev.\n Respecter ces 4 critères indispensables pour un MVP approuvé   L’unicité = une valeur ajoutée pour l’utilisateur  La simplicité = une expérience fluide La stabilité = pas de dysfonctionnement L’esthétique = une interface agréable    Prioriser les développements en se posant les questions suivantes : Quel est le besoin sous-jacent associé au produit ? Quel est le moyen le plus rapide et le moins cher qui va me permettre de répondre à ce besoin ? 3. Développer le MVP (4 semaines max) 4. Et surtout… Analyser les feedbacks des utilisateurs, qui constitueront la matière grise des prochains sprints  Un autre exemple ? Alors parlons de Joe Gebbia et Brian Chesky. Ils ont eu l\u0026rsquo;idée de transformer une chambre inutilisée en \u0026ldquo;bed and breakfast\u0026rdquo; pour arrondir leurs fins de mois. C\u0026rsquo;est à l\u0026rsquo;occasion d\u0026rsquo;une conférence organisée à San Francisco que le premier produit voit le jour : quelques photos de leur appartement sur un site, des participants en recherche de logement et Airbnb est né.\nQuel bilan tirer de l’approche MVP ? Utiliser le MVP dans une stratégie UX permet de valider des idées sans risque. Avec un petit bémol tout de même : les faux résultats. Attention à la façon dont le produit est communiqué ou aux métriques que vous évaluerez, au risque d\u0026rsquo;avoir des retours négatifs ou positifs qui vont feront croire que vous êtes sur la bonne ou la mauvaise direction.\n","date":"Mar 9, 2017","href":"https://blog.talanlabs.com/mvp-ux/","kind":"page","labs":null,"tags":["MVP","User experience","ux"],"title":"Quand le MVP se mêle à l’UX"},{"category":null,"content":"Organisation des équipes pour un projet d’architecture en microservices Cet article s\u0026rsquo;insère dans la série \u0026ldquo;microservices\u0026rdquo; écrite par François Berthault dans le cadre de la participation de Talan Labs à Devoxx 2017, du 5 au 7 avril prochains au Palais des Congrès de Paris.\nMême si au démarrage des microservices les équipes de développement peuvent travailler en mode \u0026ldquo;NO OPS\u0026rdquo;, les limitations se feront vite ressentir. Il est essentiel d’impliquer les OPS dans un processus DEV-OPS.\nUne transformation humaine inéluctable Pour mettre en place ce type d’architecture, le “métier” doit aussi avoir une place centrale dans les réflexions. Et c’est par une approche de type DDD (domain-driven-design) que l’application sera fractionnée.\nAttention toutefois : pousser le modèle trop loin reviendrait à créer des nanoservices en lieu et place de microservices !\nUne équipe agile pour une architecture microservices réussie Généralement, les organisations sont composées d’équipes indépendantes et spécialisées (le MOA, les développeurs, les administrateurs de bases de données ou DBA, les administrateurs système et les testeurs). Un microservice a besoin des mêmes compétences impliquées en son sein, afin d’éviter de créer un nouveau monolithe.\n_« Un microservice est un produit et non pas un projet en tant que tel. » _Martin Fowler\nÀ titre d’exemple, le modèle d\u0026rsquo;organisation le plus souvent cité est celui de Spotify.\nEn 2017, vive les microservices ! Avec l\u0026rsquo;avènement du cloud computing comme nouveau paradigme d’hébergement (merci à Netflix et à Amazon !), on a vu apparaître de nombreux frameworks légers adaptés au développement rapide de microservices, comme Dropwizard, Spring Boot, Spotify Apollo, Spark (Java), Kumuluzee (J2EE), Flask (Python), Sinatra (Ruby) ou Vert.x (Polyglotte).\nLes directeurs des systèmes d’information n’ont plus qu’à bien se tenir, l’air des microservices est bien en marche !\nPour ceux qui, en 2016, n’auraient pas encore essayé les microservices, il est grand temps de s’y mettre !\nEt pourquoi pas aller découvrir les architectures sans serveur (serverless) comme AWS Lambda d’Amazon ?\nCette première série d’articles vous aura permis de découvrir les principes fondamentaux de l’architecture en microservices et les conditions nécessaires à réunir avant de commencer.\nSi vos orientations stratégiques sont d’augmenter la scalabité de votre application pour augmenter le nombre de client, d’assurer un service et une expérience utilisateur de haut niveau, d’acquérir de nouveau marché et de rester compétitif, les microservices sont la solution.\nCependant, la route est encore assez longue avant de faire le buzz « microservice ». Commencez par créer un ou deux microservices à forte valeur ajoutée fonctionnelle. Éprouvez différents frameworks s’il le faut. Par la suite et avec de la maturité sur le sujet, vos prochains sujets d’études seront le « service discovery » et ensuite l’« event-driven data management ou CQRS/ES». Affaire à suivre\u0026hellip;\nVous avez aimé cet article ? Découvrez ou redécouvrez les autres épisodes de la série « Les microservices pour une architecture orientée web » :\nPartie 1 Les microservices pour une architecture orientée web n°1 : Définitions et caractéristiques\nPartie 2 Les microservices pour une architecture orientée web n°2 : Un changement de point de vue\n","date":"Mar 6, 2017","href":"https://blog.talanlabs.com/architecture-microservices-partie-3-organisation/","kind":"page","labs":null,"tags":["architecture hexagonale","DevOps","domain driven development","Microservices","projet Agile"],"title":"Les microservices pour une architecture orientée web n°3"},{"category":null,"content":"Parmi les nombreux frameworks JavaScript, AngularJS est indéniablement celui qui a connu le plus de succès. Mis en ligne en 2010 et avec une première version publiée en 2012, Angular n’a eu de cesse de s’améliorer avec une communauté croissante, des besoins qui évoluent et une équipe de développement soutenue par Google. Et puis l’an dernier, tout a changé…​\n   AngularJS devient Angular 2   Les annonces autour de la nouvelle version majeure d’AngularJS, désormais appelée Angular 2, surprennent. C’est en fait un nouveau framework qui se profile, une nouvelle approche, une nouvelle manière de penser. Et évidemment cette nouvelle version n’est pas rétro-compatible avec la précédente…​ Nous n’entrerons pas ici dans les détails qui ont poussé l’équipe a totalement ré-écrire le framework, mais une chose est sûre, un projet en AngularJS n’est pas directement transposable en Angular 2 sans un effort de développement, voire parfois une remise à zéro du projet. Ce qui n’est pas sans poser des problèmes.\n   Pas de version 3 pour Angular La sortie de la première version officielle d’Angular 2 date seulement du 14 septembre 2016, et à peine remis de nos émotions, nous apprenons en décembre qu’une nouvelle version majeure va voir le jour. Angular 3 donc ? Encore un changement radical, une perte de la capitalisation de connaissances, une remise en question totale et un changement de paradigme ? Non. Et non.\n En fait, il va s’agir d\u0026#39;Angular 4. Oui oui, on oublie la version 3. En réalité, il y a déjà une version 3, puisque le module \u0026#39;router\u0026#39; est en 3.x depuis quelque temps. De manière à éviter toute confusion (notamment dans les paquets NPM), la nouvelle version du framework sera la version 4.\n   Promesse de l’équipe de développement : cette fois-ci il n’y aura pas de breaking changes, la nouvelle version sera rétrocompatible avec la 2. C’est une promesse, reste à voir si elle va être tenue ! Pour le moment, les premières versions bêta (4.0.0-beta.4 au 19 janvier 2017) laissent supposer que les changements sont en effet de l’ordre de la mise à jour et de l\u0026#39;optimisation, mais pas de la ré-écriture complète. Plutôt rassurant…​\n   Angular : bilan de la version 4 S’il est bien entendu encore trop tôt pour juger de l’adhésion de la communauté à cette nouvelle version prévue en mars prochain, il est certain que certains points ne peuvent pas être ignorés. Le changement majeur entre AngularJS et la version 2, revenant à en faire deux frameworks totalement différents ne contribue-t-il pas à créer une forme de défiance des clients finaux, frileux à l’idée d’intégrer un framework capable de changer du tout au tout en quelques mois ?\n Et maintenant que l’on parle d’une nouvelle version majeure, sera-t-il réellement aisé de convaincre les décideurs du bien-fondé d’une telle démarche de mise à jour ? Si l’on sait déjà que les changements de version correspondent à de réelles améliorations visant à rendre l’outil toujours plus robuste et adapté aux usages actuels, il n’en reste pas moins nécessaire de garder en tête les conséquences en termes de marketing et d’adhésion de la communauté.\n   Ce qui est certain pour le moment, c’est que le planning des prochaines versions d’Angular est d’ores et déjà en place :\n   Les patchs seront publiés toutes les semaines\n  Des versions mineures seront publiées tous les mois\n  Les versions majeures (et rétro-compatibles) seront publiées tous les 6 mois, soit :\n  Angular 4 - Mars 2017\n  Angular 5 - Octobre 2017\n  Angular 6 - Mars 2018\n  Angular 7 - Octobre 2018\n   À noter que les deux frameworks AngularJS et Angular 2 ont chacun un site et une documentation, bien séparés : angularjs.org pour AngularJS et angular.io pour Angular 2+.\n   ","date":"Mar 2, 2017","href":"https://blog.talanlabs.com/oubliez-angular-3-voici-angular-4/","kind":"page","labs":null,"tags":["javascript","Angular","AngularJS","Google"],"title":"Oubliez Angular 3, voici la version 4"},{"category":null,"content":"Un changement de point de vue Cet article s\u0026rsquo;insère dans la série \u0026ldquo;microservices\u0026rdquo; écrite par François Berthault dans le cadre de la participation de Talan Labs à Devoxx 2017, du 5 au 7 avril prochains au Palais des Congrès de Paris.\nDans la littérature, les microservices sont représentés la plupart du temps par de petits hexagones. C’est une référence à un modèle d’architecture appelé architecture hexagonale, introduit par Alistair Cockburn pendant les années 2000.\nL’architecture hexagonale : clef de voûte des microservices Alistair Cockburn définit ce modèle aujourd’hui appelé Ports \u0026amp; Adapters comme suit :\n“[The Ports \u0026amp; Adapters pattern] a_llows an application to equally be driven by users, programs, automated test or batch scripts, and to be developed and tested in isolation from its eventual run-time devices and databases._”\nHexagonal Architecture and Microservices © Geeks With Blogs\nAttention cependant, il se pourrait que vous arriviez à une solution qui ressemble à ceci :\nAttention à la complexité ! © Ploeh Blog\nContre les pannes et défaillances, la résilience Dans les architectures microservices, la tolérance aux pannes est un pré-requis et non une singularité. La conséquence directe d’utiliser des services comme composants est que l’application doit être pensée pour accepter les appels de service pouvant échouer.\nLes problèmes sont classiques :\n une requête vers un système distant trop longue ; l’interaction avec un service occupé à 100 % de sa capacité ; un client qui renvoie une exception.  Mais une erreur dans un service ne doit pas impacter l’expérience utilisateur. Les applications doivent automatiquement prendre des actions correctives quand une de leurs dépendances est défaillante. C’est ce que l’on appelle la résilience.\nÉchouer pour mieux repartir Un service en échec ne doit en aucun cas dégrader les services le consommant. Compte tenu de ce besoin architectural, les solutions à utiliser sont des combinaisons de :\n timeout réseaux et de retries ; séparation des threads en pool ; utilisation de sémaphores non bloquant (tryAcquire) ; circuit breakers.  Le but ici est d’échouer rapidement afin de garantir la santé du reste de votre application. Une erreur n’est jamais plaisante en termes d’expérience utilisateur mais permettre à son système de se rétablir par lui-même rapidement est une option non négligeable.\nNetflix Hystrix est certainement la librairie la plus célèbre pour la gestion de latence et de tolérance aux pannes. Elle a été conçue par Netflix en 2011 pour isoler les points d\u0026rsquo;accès à un système distant, un service ou à une librairie tierce.\nEn conclusion, le modèle en microservices doit donc faire évoluer le point de vue des développeurs dans la création des applications. Au-delà, c’est aussi l’organisation des projets qui s’en trouve profondément modifiée. C’est le sujet abordé dans le volet numéro 3 de cette série [lien du billet n°3].\nVous avez aimé cet article ? Découvrez ou redécouvrez les autres épisodes de la série « Les microservices pour une architecture orientée web » :\nPartie 1 Les microservices pour une architecture orientée web n°1 : Définitions et caractéristiques\nPartie 3 Les microservices pour une architecture orientée web n°3 : Organisation des équipes pour un projet d’architecture en microservices\n","date":"Feb 27, 2017","href":"https://blog.talanlabs.com/architecture-microservices-partie-2-changement-point-de-vue/","kind":"page","labs":null,"tags":["Alistair Cockburn","architecture hexagonale","Microservices","Netflix Hystrix","resilience"],"title":"Les microservices pour une architecture orientée web n°2"},{"category":null,"content":"Résumé des épisodes précédents…​\n Depuis plusieurs mois la communauté Java est en émoi. Oracle semble se désintéresser de Java EE. Au terme d’une saga résumée dans un article de novembre, c’est finalement lors de la grande conférence annuelle JavaOne que l’entreprise annonce une nouvelle feuille de route pour Java EE, ainsi que le lancement d’un sondage visant à mieux cerner les aspirations et attentes de la communauté.\n Présentation du sondage Oracle Nous l’évoquions : Oracle tente de se réconcilier avec la communauté Java, tout en faisant avancer ses travaux sur Java EE. L’entreprise californienne est passée par un sondage pour mieux comprendre ses utilisateurs, mais aussi pour valider les choix déjà faits. Après 2 mois, ce sont 1693 personnes qui ont soumis leurs réponses. Il est intéressant de noter que la France semble ne pas avoir beaucoup répondu au formulaire comme l’illustre la carte de l’Europe des réponses fournie par Oracle dans son rapport exhaustif…​\n   De manière à mieux décrire les participants au sondage, Oracle y fait figurer leurs années d’expérience. Et avec près de 50 % des sondés ayant plus de 8 ans d’expérience en Java EE, nous pouvons raisonnablement considérer que ceux-ci possèdent le recul nécessaire à une bonne maîtrise de l’environnement. De plus, avec plus de la moitié des participants ayant déjà développé des micro-services, le sondage reflète la réalité du travail des développeurs de l’écosystème Java actuel.\n   Les résultats du sondage Java EE En demandant de noter de 1 (Pas important) à 5 (Très important) les différentes technologies envisagées pour la prochaine version de Java EE, Oracle comptait obtenir un graphe à la lecture aisée et simple, et l’objectif semble atteint avec ce classement :\n   On note donc que (sans trop de surprises) les services REST, la prise en charge de https://fr.wikipedia .org/wiki/Hypertext_Transfer_Protocol/2[HTTP/2] et la gestion des protocoles OAuth et OpenID sont plébiscités par la communauté pour intégrer Java EE au plus vite. Ce résultat n’est pas une surprise dans la mesure où cela colle réellement aux projets actuels, avec des API fournissant des données parfois pour plusieurs applications différentes, ainsi que la \u0026#39;mode\u0026#39; des microservices.\n De plus, l’attente d’une standardisation des configurations d’applications et la gestion des événements soulignent la dimension croissante des déploiements d’application Java dans le cloud.\n En revanche, l’API JMS par exemple semble avoir fait son temps, et la communauté ne voit pas son évolution comme une priorité absolue. De même, un framework standard d’interface web n’est pas nécessaire, dans la mesure où les nouvelles applications ont tendance à se doter d’interfaces web basées sur des frameworks JavaScript.\n   Les conclusions d’Oracle pour Java EE 8 Dans un article de blog, Oracle annonce ses conclusions suite à ce sondage. Et force est de constater que l’avis de la communauté est réellement pris en compte et va influer sur la suite du développement de Java EE 8. En effet, pour satisfaire la demande sur l’évolution de REST, JAX-RS va passer en version 2.1, une servlet 4.0 va voir le jour pour épouser HTTP/2.\n   Les protocoles d’authentification OAuth et OpenID vont avoir des standards en Java EE (mais normalement pas en version 8). De la même manière, dans la mesure où l’intégration de \u0026#39;Configuration\u0026#39; et \u0026#39;Health Checking\u0026#39; risquerait de retarder la sortie de cette version, ces éléments devraient être introduits \u0026#39;plus tard\u0026#39;. Quant à Management, JMS et MVC …​ ils sont retirés de Java EE 8.\n En résumé, Oracle semble réellement avoir pris en compte les avis de sa communauté et modifie sa feuille de route, ce qui parait rassurant. Reste maintenant à voir la sortie de Java EE 8 de manière effective, ainsi que la suite des événements. En effet, avec une telle prise en compte des opinions et une grande réactivité, la communauté peut être rassurée, au moins en attendant les premières annonces relatives à Java EE 9 !\n   ","date":"Feb 23, 2017","href":"https://blog.talanlabs.com/resultats-sondage-java-ee/","kind":"page","labs":null,"tags":["développement","Java","Java EE","JavaOne","oracle"],"title":"Java EE, nouvelle orientation : les résultats du sondage"},{"category":null,"content":"Découvrez le portrait de Marouan, développeur chez Talan Labs, passionné de voyages et de retouches photos.\nD’origine marocaine et après avoir obtenu un Master Développement Logiciel à l’Université Paul Sabattier de Toulouse, Marouan a rejoint l’équipe Talan Labs il y a deux ans. Pour lui, être développeur c’est être polyvalent : « c’est un métier exigent qui crée du challenge pour moi et pour mes équipes ». En retouche photos ou comme en développement, ce sont les détails qui comptent.\nMarouan parle avec fierté de son métier chez TalanLabs : « En deux ans chez Talan Labs, on m’a donné confiance en moi, on m’a donné des responsabilités ! De plus la collaboration avec mes collègues, m’a permis de monter en compétence. Parce que sur le même plateau il y a de la coopération et de l’échange. […] C’est une vraie culture, chez TalanLabs ».\nTalanLabs, société du Groupe Talan, rassemble des expertises technologiques complémentaires (Java/J2EE, Agile, Devops, UX, mobilité, IoT et Cloud) afin de répondre de manière opérationnelle et agile aux enjeux liés à la transformation digitale des entreprises.\nDécouvrez d’autres parcours de passionnés sur le blog TalanLabs.\nRejoignez Marouan et toute l’équipe Talan Labs en postulant à nos offres.\n","date":"Feb 22, 2017","href":"https://blog.talanlabs.com/inside-talan-marouan-developpeur-talanlabs/","kind":"page","labs":null,"tags":["carrière Talan","developpeur","inside Talan","portrait","TalanLabs"],"title":"Inside Talan – Marouan, Développeur chez TalanLabs"},{"category":null,"content":"Définitions et caractéristiques Cet article s\u0026rsquo;insère dans la série \u0026ldquo;microservices\u0026rdquo; écrite par François Berthault dans le cadre de la participation de Talan Labs à Devoxx 2017, du 5 au 7 avril prochains au Palais des Congrès de Paris.\nLa notion d’architecture en microservices est très récente, apparue depuis 3 à 4 ans. Précurseur et véritable gourou sur le sujet, Martin Fowler est la référence en matière de microservices. Il a notamment écrit de nombreux articles sur la componentization des applications logicielles.\nAujourd’hui, les microservices sont de plus en plus utilisés pour construire des applications d’entreprise ; certainement parce que les retours sont très positifs. Il existe pourtant peu d’informations sur le sujet.\nCette série d’articles a pour objectif d’**expliquer les microservices **et d’en comprendre les implications et les usages.\nQu’est-ce qu’une architecture en microservices ? L’architecture en microservices est une approche servant à concevoir une application unique basé sur un ensemble de petits services indépendants. Chaque microservice s\u0026rsquo;exécute dans un processus qui lui est propre et communique via un protocole léger, le plus souvent à base de ressource HTTP (tel que REST -REpresentational State Transfer- par exemple).\nGrâce à une gestion centralisée et minimaliste, chaque microservice peut être écrit dans un langage qui lui est propre. Pour chaque microservice, en fonction de l’objectif auquel il doit répondre, il est donc possible d’aller au plus simple et d’**utiliser les outils et langages les plus pertinents. **L’approche en microservices est donc totalement inverse de l’approche monolithique, certes plus homogène car composée d’une seule et même technologie, mais qui entraîne un certain nombre de problèmes, de réécriture notamment.\nLa componentization, une nécessité pour les microservices Avec les microservices, chaque élément correspond à un service (componentization), ce qui les distingue de l’approche par librairies réutilisables, comme c’est encore majoritairement le cas dans l’industrie logicielle. L’objectif principal est de pouvoir déployer les microservices indépendamment (ce qui devrait réjouir vos OPS) et de respecter le principe de la responsabilité unique.\nQuelques facteurs indispensables à la componentization :\n **cohérence et logique du système **: tout service doit correspondre à une fonctionnalité précise ; spécificité des fonctionnalités : chaque service ne fait qu’une chose et il la fait bien ; **autonomie des services **: chaque service dispose de son propre code, gère ses propres données et ne les partage pas (pas directement en tout cas) avec d’autres services ; **indépendance des microservices **: chaque microservice ne doit pas inclure de couplage fort avec un autre microservice.  Un modèle d’architecture plus solide La gestion de la base de données dans une architecture monolithique et dans une architecture microservices © Martin Fowler\nL\u0026rsquo;architecture microservices favorise la scalabité cubique : contrairement aux applications de type monolithique, ce design apporte de la robustesse à votre application. Le clonage de serveur n’est plus une fatalité. Ainsi, seuls les services ayant besoin d’être étendus sont dupliqués, comme expliqué ci-dessous.\nComparaison entre architecture monolithique et architecture microservices © Martin Fowler\nMicroservices et architecture orientée services (SOA) : même combat ? On pourrait croire que le microservice est une sorte d’architecture orientée service (SOA) déguisée. Si, au premier abord, les deux approches peuvent sembler assez similaires, une différence majeure les distingue :\n le SOA ou service-oriented architecture est un modèle d\u0026rsquo;architecture qui définit des composants applicatifs produisant des services pour d’autres composants via un protocole de communication réseau ; l’architecture en microservices est, au contraire, liée à une application, qui se compose de petits processus indépendants qui communiquent entre eux en utilisant des API (Application Programming Interface), ce qui les rend indépendants des langages.  Vous avez aimé cet article ? Découvrez ou redécouvrez les autres épisodes de la série « Les microservices pour une architecture orientée web » :\nPartie 2 Les microservices pour une architecture orientée web n°2 : Un changement de point de vue\nPartie 3 Les microservices pour une architecture orientée web n°3 : Organisation des équipes pour un projet d’architecture en microservices\n","date":"Feb 21, 2017","href":"https://blog.talanlabs.com/architecture-microservices-partie-1-definitions/","kind":"page","labs":null,"tags":["Cloud","DevOps","microservice"],"title":"Les microservices pour une architecture orientée web n°1"},{"category":null,"content":"Le 3 février dernier, Talan organisait une session vidéo en direct avec des consultants du groupe sur la plateforme Job Teaser. Virginie (Directrice des Opérations chez TalanLabs) et Jean-Baptiste (Consultant Senior chez TalanConsulting), se sont prêtés au jeu et ont répondu aux questions des internautes en direct.\n Cliquez sur l’image pour voir le replay sur Job Teaser\nVirginie et Jean-Baptiste ont parlé de leur parcours et surtout de leur métier au sein du groupe Talan.\nPour Virginie « _l’apport de Talan c’est justement cette proximité que nous avons avec les consultants en amont qui sont en contact direct avec les clients et qui participent à la transformation digitale des entreprises. […] Avoir cette proximité, nous aide à choisir les solutions techniques les plus adaptées _».\nLa Directrice des Opérations note également que chez Talan Labs, « on peut à la fois travailler sur un projet client et sur des projets internes_ innovants_ […] _nous avons travaillé récemment sur un outil d’automatisation des tests. » _Elle évoque également le projet Talancoin dans le domaine de la blockchain.\nConcernant l’ambiance en interne, « Talan, c’est comme une petite famille », pour Jean-Baptiste. Virginie parle d’une ambiance conviviale « avec des gens que l’on connaît ».\nDe nombreuse questions sur le profil des consultants recrutés ont été posées et sur ce point, Virginie et Jean-Baptiste ont insisté sur le fait que tous les profils sont les bienvenus chez Talan et que le recrutement se faisait avant tout sur la personnalité plutôt que sur les diplômes.\nPour avoir toutes les réponses, regardez la vidéo.\nÀ vos agendas ! Talan organise un Happy Hour mercredi 1er mars 2017 de 18h à 20h dans leurs locaux. Venez à la rencontre de nos consultants et échanger avec nous autour d’un verre.\nInscrivez-vous dès maintenant.\n","date":"Feb 14, 2017","href":"https://blog.talanlabs.com/job-teaser-recrutement-consultants-talan/","kind":"page","labs":null,"tags":["Job Teaser","Recrutement","Talan"],"title":"Nos consultants parlent aux internautes du recrutement"},{"category":null,"content":"Parce que la vie d’un développeur ce n’est pas qu’un quotidien routinier face à un écran rempli de code, il est parfois important d’élargir ses idées et de se confronter au travail et aux passions d’autres acteurs de la communauté. C’est ainsi que je vous propose aujourd’hui de découvrir un meetup, les Human Talks.\n Mais au fait, qu’est-ce qu’un meetup ?\n Qu’est-ce qu’un meetup ?   C’est avant tout un événement de réseautage, autour d’un intérêt commun. Il en existe dans tous les domaines, comme le sport, les langues ou l’engagement politique, dans le monde entier. En effet, le site Meetup.com, qui fait office de référence mondiale, recense plus de 23 millions d’utilisateurs dans 180 pays. Quels que soient vos centres d’intérêt, vous êtes obligés de trouver votre bonheur parmi leurs plus de 200 000 groupes.\n Événements incontournables au sein des communautés de développeurs, les meetups permettent de rencontrer ses pairs, de lier de nouvelles relations, mais aussi de découvrir des nouvelles technologies, voire des domaines jusqu’alors inconnus. Bref, les meetups ouvrent l’esprit et étendent notre champ de réflexion, facilitant une veille technologique pas toujours aisée avec un emploi à plein temps.\n     Le meetup des Human Talks J’ai personnellement participé à une douzaine de meetups en un peu plus d’un an, en grande partie aux Human Talks de Paris qui fêtaient leurs 4 ans en novembre. Initiés par Human Coders, centre de formation visant à favoriser les échanges entre développeurs, les Human Talks sont une série de 4 talks (ou présentations) de 10 minutes chacun, se déroulant une fois par mois et hébergés par des entreprises du numérique (Viadeo, Dailymotion, Prestashop,…​).\n Les sujets sont présentés principalement par des développeurs et pour des développeurs, avec des thèmes aussi larges que la programmation, les retours d’expérience, les méthodes de travail…​ Et il n’y a pas que Paris, puisque les Human Talks sont aussi présents à Lyon, Toulouse, Nancy, pour un total de 14 villes.\n     Les 4 ans des Human Talks à Paris Pour leur anniversaire, les Human Talks ont fait les choses en grand ! Avec comme hôte Google France et leurs locaux assez grandioses, ce sont plus de 250 personnes qui se sont retrouvées pour 4 présentations :\n   GraphQL, the new age of API ?\n  www : The Web Will Win\n  Tout ce que vous pensez savoir sur la couleur est faux\n  Hacker-Dev API on the Way\n   Des sujets techniques donc, mais aussi de quoi découvrir un nouveau domaine comme le bug bounty, ou réaliser que les écrans que nous utilisons n’affichent pas toutes les couleurs visibles à l’œil nu. Et après une ultime présentation bonus sur la prise de parole en public, il était temps de tous se retrouver autour de pizzas, bières et cupcakes d’anniversaire, pour débriefer, échanger et rencontrer des développeurs de tous horizons.\n Et vous alors, quand est-ce que l’on vous croise à un meetup ?\n   ","date":"Feb 1, 2017","href":"https://blog.talanlabs.com/presentation-meetup-human-talks/","kind":"page","labs":null,"tags":["Human Coders","Human Talks","meetup","réseautage"],"title":"Présentation d''un meetup : les Human Talks"},{"category":null,"content":"TalanLabs, société du Groupe Talan, rassemble des expertises technologiques complémentaires (Java/J2EE, Agile, Devops, UX, mobilité, IoT et Cloud) afin de répondre de manière opérationnelle et agile aux enjeux liés à la transformation digitale des entreprises.\nTalan Labs vous emmène à la rencontre de Lucille, UX designer, qui vous parle de son métier et de sa passion, le théâtre.\nDe formation Supélec, Lucille est ingénieure en systèmes d’information, mais aussi artiste, metteur en scène et dessinatrice. Elle se tourne très vite vers le design et les recherches cognitives et se spécialise dans la programmation orientée objet (POO). Lucille a travaillé dans des environnements très divers (aéronautique, défense, logistique) et est aujourd’hui consultante UX chez Talan Labs.\n« Comme le théâtre, l’UX c’est aussi raconter une histoire, déclare-t-elle. Talan Labs m’a permis de concilier ma passion et mon travail. […] Ce qui est vraiment passionnant dans ce métier c’est qu’on est encore dans la découverte, il y a des évolutions tous les jours […] il y a sans cesse quelque chose de nouveau à découvrir. »\nLucille travaille actuellement sur des applications destinées aux entreprises en interne pour des grands comptes de transport et logistique.\nDécouvrez d\u0026rsquo;autres parcours de passionnés sur le blog TalanLabs.\nRejoignez Lucille et toute l’équipe Talan Labs en postulant à nos offres.\n","date":"Jan 12, 2017","href":"https://blog.talanlabs.com/au-coeur-de-votre-passion-lucille-ux-designer-chez-talanlabs/","kind":"page","labs":null,"tags":["carrière Talan","portrait","ux designer"],"title":"Inside Talan - Lucille, UX designer chez TalanLabs"},{"category":null,"content":"Peut-être que pour vous la série de l’été était Stranger Things, la petite pépite aux couleurs années 80-90 de Netflix. Mais dans le vaste monde du Java, une toute autre série aux multiples rebondissements a retenu toutes les attentions.\n Pour introduire le récit de cet été de toutes les surprises, un petit historique s’impose…​\n Java, Sun et Oracle L’entreprise Sun est fondée en 1982, et lance Java en 1995. Un programme en Java peut s’exécuter tel quel (ou presque) sur toutes les plateformes (Windows, UNIX, etc.), est plus sécurisé qu’en C++, adapté à la programmation distribuée, supporte le multi-threading, etc. Très vite, Java prend son essor, pour toutes ses caractéristiques, révolutionnaires pour l’époque.\n   La spécification de Java, ‘orientée entreprise’, J2EE 1.2, sort en 1999 et continue d’évoluer jusqu’à la version Java EE 7 publiée en 2013. Mais entre temps, Oracle a racheté Sun. Pour rappel, Oracle c’est 110 milliards de chiffre d’affaires, et des produits ultra-répandus comme Oracle Database ou Weblogic.\n Les craintes de la communauté Open Source sont vite confirmées, Oracle abandonnant par exemple le développement d’OpenSolaris (un système d’exploitation de Sun). L’open n’est pas la priorité d’Oracle, et l’on commence à craindre pour Java. Le procès intenté par Oracle à Google pour son utilisation de Java dans Android laisse à penser que le monde du Java va se refermer sur lui-même et que sa communauté va se réduire. En mai 2016, la Cour Suprême des États-Unis se prononce en faveur de Google, jugeant que son utilisation de Java est licite.\n   Java EE Guardians Au mois de juin 2016, James Gosling, qui n’est rien de moins que le créateur de Java, s’émeut du manque d’intérêt évident d’Oracle pour Java EE et rejoint les Java EE Guardians, un groupement de décideurs, de développeurs, de “users’ groups” et d’entreprises de premier ordre. Leur revendication est simple : ils souhaitent plus de transparence de la part d’Oracle, et l’assurance de la continuation des travaux sur Java EE (8 et plus).\n   Les hostilités sont ouvertes. La communauté fourmille d’idées pour assurer la survie de Java EE, allant jusqu’à proposer le transfert de son support à IBM ou Red Hat.\n Après une période de mutisme, Oracle réagit en juillet. C’est par la voix de son président du développement produit, Thomas Kurian, que l’entreprise affiche son ambition de moderniser cette spécification pour applications professionnelles. L’annonce reste vague et ne convainc pas vraiment.\n Une semaine plus tard, Oracle communique de nouveau sur Java EE 8, en expliquant que cette version sera équipée pour le déploiement dans le cloud, adaptée aux microservices et au multi-tenant (la capacité d’une application à servir plusieurs organisations clientes). Encore une fois, et même si l’effet d’annonce est plus important, les Guardians demandent des détails…​\n   La feuille de route C’est finalement à l’occasion de sa conférence JavaOne 2016, la grande messe du Java, qu’Oracle fait ses annonces les plus intéressantes, et les plus détaillées. Du 18 au 22 septembre, les conférences et ateliers techniques s’enchaînent, avec cette année un accent fort sur le futur de Java.\n   Dans sa volonté de se réconcilier avec la communauté qui se sentait bafouée, Oracle lance une vaste campagne de sondage visant à recueillir les attentes de tout un chacun. Les résultats ne sont pas encore publics, mais nul doute que l’initiative a de quoi rassurer. Oracle compte bien écouter la communauté.\n Quant à la feuille de route : après le report de la publication de Java 9 (prévue pour 2016) à juillet 2017, Java EE 8 est attendu pour la première moitié de 2017. Et, surprise, pour rattraper le décalage entre les versions de Java EE et Java, une version 9 est prévue dès 2018.\n   Deux versions de Java EE en un an, ce serait inédit, et non sans laisser imaginer de nouveaux problèmes. Quel intérêt aurait une entreprise à migrer ses projets de Java EE 7 à 8 puis, immédiatement ensuite à relancer un chantier pour passer à la version 9 ?\n   La suite…​ La réorientation stratégique des objectifs de Java EE vers des problématiques très actuelles (micro-services, cloud, etc.), les échéances resserrées et toutes ces annonces en quelques mois seulement ne sont pas sans soulever de nombreuses questions. Et en réalité non sans laisser planer de nombreuses incertitudes…​\n Alors, bonnes ou mauvaises nouvelles ? Bonnes ou mauvaises décisions ? Il est bien entendu encore trop tôt pour le dire. Mais ce qui est certain, c’est que seule la communauté des développeurs pourra réellement faire de cette nouvelle version une réussite ou un échec, un engouement ou un désenchantement.\n   ","date":"Nov 16, 2016","href":"https://blog.talanlabs.com/java-ee-nouvelle-orientation-bonne-nouvelle/","kind":"page","labs":null,"tags":["développement","Java","Java EE","JavaOne","oracle","sun"],"title":"Java EE, nouvelle orientation : une bonne nouvelle ?"},{"category":null,"content":"LES PREMIERES LEÇONS DE L’IA DANS LES CONTEXTES INDUSTRIELS Une machine pensante comme l\u0026rsquo;homme. Tel est le rêve des uns et le cauchemar des autres. Au-delà des hésitations éthiques et économiques inhérentes à chaque rupture technologique importante, la progression de l\u0026rsquo;intelligence artificielle, déjà présente dans nombre d\u0026rsquo;applications que nous utilisons au quotidien, ne cesse de se renforcer.\nComment cette technologie, à l\u0026rsquo;aube de sa gloire, révolutionne déjà l’industrie, la recherche, le monde médical, le marketing ? La liste est difficilement énumérable tant elle semble se prolonger à chaque seconde.\nA l\u0026rsquo;instar de la prospection pétrolière où les technologies de localisation de nappes ont bousculé l\u0026rsquo;industrie, le nouvel eldorado de l\u0026rsquo;information, que le monde a convenu de nommer big-data, où des gisements de données s\u0026rsquo;amoncellent en quête d\u0026rsquo;interprétation compréhensible par l\u0026rsquo;homme, a besoin de nouveaux outils d\u0026rsquo;exploration.\nSans objectifs d\u0026rsquo;exhaustivité, cet article propose de parcourir les problématiques naissantes inhérentes à l\u0026rsquo;utilisation d\u0026rsquo;algorithmes d\u0026rsquo;intelligence artificielle dans un contexte industriel, comme par exemple, le projet initié par SNCF Réseau auquel le groupe Talan participe actuellement\nIl précisera ensuite le positionnement de tels automates dans l\u0026rsquo;équilibre des tâches entre l\u0026rsquo;homme et les machines classiques, et finira par une ouverture sur la roadmap engagée vers des applications d’ IA complètements autonomes.\nLE MACHINE LEARNING EN QUELQUES PHRASE I. Le Principe Le principe fondateur de cette classe d\u0026rsquo;algorithme est simple, implémenter des modèles mathématiques proches du fonctionnement des réseaux de neurones biologiques permettant, via des « apprentissages », la reconnaissance de concepts (objets dans des images, signatures dans des sons ou autres signaux) avec des degrés de libertés inatteignables via des algorithmes classiques. Des neurones artificiels connectés en réseau (RNA) apprennent donc, soit de manière supervisée, où un expert qualifie une réponse à un stimulus, le logiciel corrigeant sa structure en conséquence pour apprendre ce nouvel élément, ou de manière autonome, non supervisée, où le réseau apprend, dans l\u0026rsquo;état actuel de l’art, à reconnaître de lui-même des redondances plus ou moins complexes dans les entrées.\nComme évoqué dans l’introduction, l’utilisation de ces algorithmes progresse dans de nombreux cas d’usages. Ils peuvent par exemple reconnaitre avec exhaustivité les objets dans une image et donc décrire une scène de manière automatisée en temps réel, rendant accessible la recherche d’instants dans une vidéo par recherche textuelle, par exemple « y-a-t-il une montagne bleue dans cette vidéo ? ». Au-delà de ses fonctions régaliennes de « reconnaissance de formes », que ce soit dans des images ou des sons, cette capacité à reconnaitre des « structures » dans des entrées a été utilisée récemment par Google DeepMind pour créer une partie de l’automate Alpha Go ayant battu l’humain au jeu de GO.\nDans ce contexte, l\u0026rsquo;utilisation du mot intelligence pose débat, néanmoins les progrès récents d\u0026rsquo;apprentissages profonds commencent à justifier ce terme. En effet, nous le verrons dans le paragraphe 4, certains algorithmes sont dorénavant capables de créer des concepts auxquels les hommes de l\u0026rsquo;art n\u0026rsquo;avaient jamais pensé. Lorsque la machine commence à concevoir des solutions non pensées mais validées par l\u0026rsquo;homme, le mot intelligence commence à prendre sens afin de qualifier cette nouvelle autonomie d\u0026rsquo;analyse allouée aux machines.\nII. L’historique Après une émergence remarquée au milieu du XXe siècle, la technologie a graduellement perdu de sa superbe pour ressortir de l’oubli à la fin du siècle dernier grâce à une méthode d’apprentissage, dit « rétropropagation du gradient », ouvrant le potentiel de ces algorithmes. Cette dernière reste le pilier de la technologie et porte encore de nos jours sa progression.\nL’engouement récent et « généralisé » pour cette technologie semble le fruit de plusieurs facteurs. Tout d’abord d’innovations comme les réseaux multicouches et les réseaux de convolution (convnet) qui servent maintenant de fers de lance au « deep learning » dans l’image. Ensuite, de la puissance de calcul des clusters actuels qui ont permis la réalisation de réseaux de « deep learning » comptant plusieurs milliards de neurones, gourmands en ressources pour leurs apprentissages. Et enfin, des usages du « web 2.0 » qui ont fait émerger de bases à très fort volume permettant des apprentissages exhaustifs (exemple base d’images taguées par des utilisateurs).\nCertaines avancées continuent de renforcer l’avenir de cette classe d’algorithme, notamment une nouvelle génération de réseaux plus proche du fonctionnement des neurones biologiques, les réseaux de neurones impulsionnels. Ces derniers intègrent en plus des réseaux existants la dimension temporelle des stimuli présentés aux neurones, permettant d’enrichir la capacité des réseaux à modéliser les mécanismes de la mémoire des neurones biologiques, mais aussi leur capacité à modéliser des problèmes non-linéaires.\nLES PROJETS DE MACHINE LEARNING DANS UN CONTEXTE INDUSTRIEL Intéressons-nous aux différences de cycles de vie entre un système d’information classique et un système apprenant. Elles sont majoritairement au nombre de trois. La première constitue une contrainte spécifique de gestion de projet, critique lors du développement d’une solution industrielle basée sur des algorithmes apprenant. Les deux autres sont des avantages intrinsèques aux cycles de vie de telles solutions comparées à des solutions classiques, « non apprenantes ».\nLa courbe ci-dessous trace le cycle de vie de la solution, et l’exhaustivité de sa réponse au cahier des charges :\nI. La gestion du processus d’apprentissage au centre de la réussite du projet Nous nous apercevons que les processus projets doivent s\u0026rsquo;adapter au concept d\u0026rsquo;apprentissage par rapport au développement de solutions classiques.\nEn effet un système« apprenant» sous-entend un système qui n\u0026rsquo;est pas opérationnel en pré-production et donc un système qui n\u0026rsquo;arrivera en « production » au sens fonctionnel, à savoir, une réponse exhaustive au cahier des charges, qu\u0026rsquo;après un déploiement permettant de réaliser l\u0026rsquo;exhaustivité de cet apprentissage.\nUne validation du système est donc une validation pré-déploiement d\u0026rsquo;une aptitude à résoudre le problème, cela est légèrement différent de la validation d\u0026rsquo;un système classique ou l\u0026rsquo;exhaustivité du résultat est supposée pré déploiement. Au-delà de la validation technologique, cela impose également une stratégie de déploiement radicalement différente, plus progressive, où la rapidité de captation des occurrences des situations d’apprentissages est capitale pour la pérennisation du déploiement, même si la technologie avait éventuellement convaincu à un stade de preuve de concept.\nIl est souhaitable, dans la mesure du possible, de produire la preuve de concept sur les situations à apprendre les plus complexes pour fournir aux « décideurs » une « garantie » d\u0026rsquo;exhaustivité des résultats avant la complétude des apprentissages qui n\u0026rsquo;arrivera, comme nous venons de le voir, qu\u0026rsquo;après un déploiement complet\nCela constitue en soi un vrai challenge pour les projets de machine Learning devant faire leurs preuves sur des résultats basés essentiellement sur des simulations, le système n\u0026rsquo;étant pas encore déployé. Un catalyste possible est d\u0026rsquo;adjoindre au développement de la solution un environnement de simulation « émulant » un maximum de situations possibles pour réaliser des apprentissages exhaustifs pré-déploiement, cela n’est néanmoins pas possible dans tous les cas d’usage, il faut en effet veiller que les économies de développement accomplies avec des solutions apprenantes ne soient pas absorbées par des coûts de simulations prohibitifs.\nII. Un optimum de solution supérieur Comme vu précédemment, les réseaux de neurones artificiels permettent aux solutions de fournir des résultats beaucoup plus agnostiques au contexte du problème que les solutions classiques. Cette adaptabilité aux conditions opérationnelles permet en général de gagner en robustesse et en exhaustivité face aux exigences du cahier des charges lors de la vie opérationnelle du produit.\nIII. L’obsolescence n’est plus une fatalité Un autre avantage des systèmes apprenants est de rapidement s’adapter à moindre coût à des contextes radicalement différents, sans repenser complètement la solution. Cela étend la durée de vie des solutions et les rend résistantes face à l’obsolescence gagnant généralement les systèmes classiques.\nAu-delà des difficultés de la vie du projet, la validation de cette technologie s\u0026rsquo;accompagne dans certain cas d\u0026rsquo;une nécessite de caractère GAME (globalement au moins équivalent) à un système existant, qu\u0026rsquo;il soit un algorithme existant ou une tâche dévolue à un agent. Il est donc important de comprendre les avantages intrinsèques des systèmes apprenants.\nBILAN COGNITIF DES SYSTÈMES APPRENANT Des univers normés comme SNCF Réseau (Cf. article sur le Blog Cereza), où la sécurité est omniprésente et à la racine des décisions technologiques, constituent un stress-test solide pour cette technologie.\nNous entrons dans une phase où elle doit prouver, dans un premier temps, dans un cadre « non sécuritaire » d\u0026rsquo;aide à la décision, une fiabilité « globalement au moins équivalente » aux algorithmes existants ou à une analyse humaine.\nLa perception \u0026ldquo;boîte noire\u0026rdquo; des algorithmes de machine Learning est souvent à la source d\u0026rsquo;un questionnement sur leur positionnement dans l\u0026rsquo;accomplissement des tâches par rapport à l\u0026rsquo;homme ou aux systèmes que l\u0026rsquo;on qualifiera de \u0026ldquo;classiques\u0026rdquo;. La représentation abstraite suivante permet de définir les différences \u0026ldquo;fonctionnelles\u0026rdquo; d\u0026rsquo;une analyse accomplies par ces différents systèmes.\nL\u0026rsquo;avènement de l\u0026rsquo;informatique, souvent qualifié de révolution industrielle, a bouleversé les processus de gestion et de traitement de l\u0026rsquo;information jusqu\u0026rsquo;à engendrer la société que nous connaissons actuellement. Les possibilités d’interconnections des systèmes leur ont permis de s\u0026rsquo;affranchir des limitations que l\u0026rsquo;homme subit dans sa corporalité. En effet notre physionomie nous empêche de nous combiner, et de former un réseau, ou un ensemble de cerveaux incrémenteraient une capacité de calcul en réseau local. L\u0026rsquo;informatique classique a donc permis, par cette qualité, de capter beaucoup plus de contexte qu\u0026rsquo;un humain pour répondre à un problème. A ceci près que la perception des algorithmes classiques est très dépendante du contexte, un exemple étant qu\u0026rsquo;un algorithme classique de reconnaissance d\u0026rsquo;objet aurait du mal à reconnaître une pomme dans un contexte changeant, ce qui ne pose aucun problème pour l\u0026rsquo;homme qui reconnaitra un concept peu importe le contexte.\nLa réponse à un problème donné dépend de 2 critères fondamentaux, la masse de contexte admissible par un système, où, comme nous l\u0026rsquo;avons vu, l\u0026rsquo;informatique classique possède un avantage sur l\u0026rsquo;homme, mais également la profondeur d\u0026rsquo;analyse, que certains pourraient ramener à une capacité d\u0026rsquo;abstraction et de conceptualisation. C\u0026rsquo;est précisément sur cet aspect que l\u0026rsquo;homme conserve son net avantage et produit des analyses beaucoup plus complexes que des algorithmes.\nLes réseaux de neurones artificiels sont capables de se combiner pour faire émerger des concepts à l\u0026rsquo;instar du cerveau humain, mais également des liens entre ces concepts et ce, sans que l\u0026rsquo;homme n’ait à expliciter quelque règle que ce soit sous forme de code. Ils sont donc capables d\u0026rsquo;abstraitiser des stimuli comme les humains et sont en ce sens capables d\u0026rsquo;analyses beaucoup plus profondes que les algorithmes classiques. Ils sont néanmoins loin d\u0026rsquo;être aussi capables que le cerveau humain dans cette manipulation de concepts pour résoudre des problèmes. Ils bénéficient par contre de l\u0026rsquo;avantage de leurs aïeux classiques d\u0026rsquo;une capacité de capter un contexte étendu par rapport à l\u0026rsquo;humain et produisent ainsi des « raisonnements » inédits, complexes, et inaccessibles au cerveau humain.\nL’exemple marquant est la prolifération dans le domaine médical, notamment par l’université de Stanford, d’études montrant que des algorithmes de « deep learning » produisent des analyses inédites de tissus cancéreux en y détectant des signaux précurseurs dans les tissus environnants , signaux difficilement détectables par les hommes de l’art.\nIl est important de préciser qu’ « intelligence » dans la plus part de cet article réfère à la « capacité à résoudre un problème » nous laisserons pour l\u0026rsquo;instant la « quête d\u0026rsquo;un état de conscience » de côté, l\u0026rsquo;intelligence artificielle générale étant encore hors de portée des algorithmes de « machine Learning » actuels.\nCette combinaison de « digestibilité » de données massives couplée à une certaine capacité d\u0026rsquo;abstraction donne au couple « Machine Learning » + « Big-Data » une valeur économique conséquente, sans pour autant avoir des systèmes « profondément » intelligents. Ces capacités algorithmiques nouvelles sont d\u0026rsquo;autant plus intéressantes qu’elles sont rapides. En effet, leur rapidité leur permet des temps d\u0026rsquo;analyse sur des contextes larges en temps réel alors que les algorithmes classiques digèreraient cette masse de donnée dans un temps relativement plus long..\nLa prochaine étape pour ces algorithmes est d\u0026rsquo;apprendre par « eux-mêmes », de gagner en autonomie pour créer des concepts ex-nihilo sans que l\u0026rsquo;homme n’intervienne comme « guide » ou « professeur », c\u0026rsquo;est ce que l\u0026rsquo;on appelle les algorithmes autoapprenants.\nINTELLIGENCE EMBARQUÉE OU INTELLIGENCE CENTRALISÉE Dans une époque où l\u0026rsquo;augmentation de la capacité de calcul passe par une augmentation de l\u0026rsquo;infrastructure et son interconnectivité, il a souvent été de mise de comparer l\u0026rsquo;efficience énergétique des systèmes informatiques à celle du cerveau humain et de constater le gouffre énergique des clusters de calcul pour des fonctions que l\u0026rsquo;homme accomplit sans effort à très faible coût énergétique. Nous entrons alors dans le monde de la performance par watt, ou devrait-on dire, de l\u0026rsquo;intelligence par watt. Là encore les RNA permettent des analyses complexes à très faible coût énergétique. Il est alors possible d\u0026rsquo;embarquer des traitements très complexes « on chip » sur des processeurs que l\u0026rsquo;on qualifie de « neuromorphiques » avec les avantages d\u0026rsquo;un traitement temps réel de l\u0026rsquo;information et des gains en bande passante du fait de la communication du seul fruit de l\u0026rsquo;analyse sans obligation de communication de la source ou stimuli. Bien évidemment, l\u0026rsquo;intelligence des RNA étant scalable, il fait sens pour certaines applications de centraliser le réseau de neurones sur une infrastructure centralisée de plus grosse taille pour une analyse plus complexe, cela dépend donc du cas d\u0026rsquo;utilisation, mais il faut retenir que le bond en performances par watt des RNA a permis l\u0026rsquo;émergence d\u0026rsquo;une multitude d\u0026rsquo;applications embarquées inaccessibles aux systèmes classiques.\nL\u0026rsquo;IMPACT DU MACHINE LEARNING SUR ÉCOSYSTÈME DES ENTREPRISES DE L\u0026rsquo;INFORMATION Bien évidemment, une telle révolution ne va pas sans restructurer des faces entières de l\u0026rsquo;économie. Il est dès lors tout naturel que la physionomie du marché des ESN (Entreprises de Services Numériques) s\u0026rsquo;adapte à ces nouveaux outils.\nDeux caractéristiques, le concept d\u0026rsquo;apprentissage d\u0026rsquo;une part, et l\u0026rsquo;aspect générique de la technologie d\u0026rsquo;autre part, vont faire évoluer le métier de développement informatique ainsi que le conseil en management des systèmes d\u0026rsquo;informations.\nL\u0026rsquo;apprentissage met donc en scène des experts métiers devant des plateformes pour « apprendre » à la machine directement la fonctionnalité métier recherchée, sans passer par une étape de développement, ce dernier étant implicitement « encodé » par l\u0026rsquo;algorithme d\u0026rsquo;apprentissage. Il est à noter que les RNA actuels nécessitent encore l\u0026rsquo;intervention d\u0026rsquo;un « architecte » RNA pour suggérer la meilleure structure de réseau pour apprendre le cas d\u0026rsquo;usage, mais un réseau générique suffisamment flexible pour accueillir une diversité de cas d\u0026rsquo;usages métier est de plus en plus « réalisable ».\nL\u0026rsquo;intelligence artificielle transfère donc un coût et temps élevé de développement externalisé vers un coût et temps réduit d\u0026rsquo;expertise interne à l\u0026rsquo;entreprise.\nUne partie du marché de développement va donc graduellement disparaître, les ESN peuvent couvrir cette perte par soit, offrir leur propre plateforme « intelligente », soit par de l\u0026rsquo;AMOA spécialisée dans le déploiement de solutions de « machine Learning ».\nVERS L\u0026rsquo;AUTOAPPRENTISSAGE, DES ABSTRAITISATIONS AU DELÀ DE L\u0026rsquo;HUMAIN Les algorithmes autoapprenants, au gré de leurs perfectionnements, vont, dans un premier temps, permettre l\u0026rsquo;automatisation de tâches complexes où l\u0026rsquo;improvisation est sollicitée plus que la répétition. Cela va bien évidement contribuer à remplacer l\u0026rsquo;homme dans de nombreux services.\nMais la recherche est encore longue avant d\u0026rsquo;arriver à une intelligence artificielle générale, et plusieurs facteurs sont peut-être, sans le savoir, en train d\u0026rsquo;en ralentir ces efforts.\nLe premier est que l\u0026rsquo;homme crée pour son propre « confort » et surtout pour ses propres capacités d\u0026rsquo;interprétations des résultats, or, cette part systémiquement \u0026ldquo;anthropomorphique\u0026rdquo; dans nos solutions bornera potentiellement les raisonnements de ces algorithmes, là où la machine pourrait éventuellement nous dépasser. Il faut donc que l\u0026rsquo;homme centre sa recherche fondamentale pour fusionner le meilleur de notre biologie avec celui d\u0026rsquo;une intelligence dématérialisée i.e., s\u0026rsquo;inspirer de l\u0026rsquo;humain sans s\u0026rsquo;y confiner, ce dernier ayant été « construit » sur un ensemble d\u0026rsquo;organes « limités » fournissant les stimuli.\nLes briques de bases que sont les modèles mathématiques des réseaux de neurones artificiels doivent également encore progresser pour arriver au niveau d\u0026rsquo;un cerveau humain, les limitations actuelles étant dans la manière dont se propage l\u0026rsquo;information dans les réseaux de neurones artificiels (feedforwarding only) et la nécessité de forts volumes de données pour l’apprentissage, les réseaux actuels n\u0026rsquo;étant pas encore capables d\u0026rsquo;apprendre sur très peu d\u0026rsquo;exemples.\nEnfin, l\u0026rsquo;industrie sort petit à petit du biais introduit par l\u0026rsquo;informatique actuelle dite booléenne de ne pas tolérer l\u0026rsquo;erreur. Les exemples d\u0026rsquo;introduction d\u0026rsquo;hardware/software bayésien tolérant des taux d\u0026rsquo;erreurs non négligeables se multiplient, allant des efforts d\u0026rsquo;informatiques quantiques à des solutions utilisant les processus de fonderie actuels, comme le PCMOS pour Probabilistic CMOS introduit par l\u0026rsquo;université de Rice où un processeur tolérant 8% d’erreur a doublé ses performances et réduit sa consommation énergétique de 30% [http://www.ece.rice.edu/~al4/visen/2006date.pdf].\nCette aversion à l\u0026rsquo;erreur est encore un réflexe naturel et les résultats exigés des RNA sont également extrêmement élevés avec des niveaux de réponses de l\u0026rsquo;ordre de 99,5%. Ces scores sont effectivement à rechercher pour les domaines où l\u0026rsquo;homme opère également à ces niveaux, comme la reconnaissance visuelle, mais la recherche en cognition et raisonnements artificiels ne doit pas se structurer autour de la quête d\u0026rsquo;un taux d\u0026rsquo;erreur de 0,1%. En effet, l\u0026rsquo;homme vit constamment avec l\u0026rsquo;erreur, si nous devions quantifier le taux d\u0026rsquo;erreur de nos « choix » et « raisonnements », il serait probablement plus autour de 60%, légèrement au-dessus d\u0026rsquo;un résultat « aléatoire » (l\u0026rsquo;évolution nous ayant doté de ces capacités, elles doivent probablement avoir une valeur ajoutée par rapport à l\u0026rsquo;aléa pur) mais avec un taux d\u0026rsquo;erreur relativement important.\nAu-delà de ces quelques limitations, cette intelligence artificielle généralisée, proche de l\u0026rsquo;homme, reste l\u0026rsquo;obsession d\u0026rsquo;une poignée de chercheurs à la convergence des neurosciences (retro-engineering de l\u0026rsquo;activité/structure cérébrale, de la mémoire), des mathématiques (progrès sur le modèle) et de la psychologie/psychanalyse (fonctionnement des concepts humains) et sera sans nul doute parmi les plus intrigantes inventions que le futur nous réserve.\n","date":"Oct 19, 2016","href":"https://blog.talanlabs.com/de-linformatique-classique-vers-linformatique-cognitive/","kind":"page","labs":null,"tags":["Deep Learning","IA","Machine Learning","Réseaux de Neurones Artificiels (RNA)"],"title":"De l’informatique classique vers l’informatique cognitive"},{"category":null,"content":"Les fonctionnalités ne sont pas spécifiées, les testeurs n’ont rien à tester, les développeurs sont débordés. Problèmes courants dans la gestion Agile d’IT, qui sont des indicateurs d’un manque de fluidité dans le processus. Si en tant que manager, vous voulez améliorer vos méthodes, les principes du Lean peuvent vous guider à optimiser votre rapport valeur/coût.\nEn utilisant un outil de suivi de projet, comme le Kanban, vous observez pas à pas l’avancement de vos fonctionnalités. En observant les bouchonnements, vous repérez les obstacles à identifier. C’est en ce sens que des méthodes visant à mettre en place des démarches d’amélioration continue telles que Lean et Kanban permettent de parler d\u0026rsquo;innovation en management de projets IT.\nPlantons le décor Récemment, l\u0026rsquo;IT a réussi à trouver un rythme managérial adapté à l\u0026rsquo;aspect créatif du métier. C\u0026rsquo;est ce qu\u0026rsquo;on appelle aujourd\u0026rsquo;hui les méthodes \u0026ldquo;Agiles\u0026rdquo;. Les qualités de votre produit, adaptabilité, modularité et évolutivité, deviennent celles de votre gestion de projet.\nC\u0026rsquo;est très beau sur le papier\u0026hellip; mais concrètement, ça donne quoi ?\nUn ensemble de bonnes pratiques [1].\nQuand on veut les appliquer, on se voit modifier son cadre de travail. Une méthode de travail typiquement « Agile » est le « Scrum » [2]. Il est assez courant de s\u0026rsquo;inspirer du « Scrum » pour en déduire un cadre de travail spécifique pour votre propre projet.\nApparition des nouveaux acteurs Le « Lean », comme « l\u0026rsquo;Agile », est un ensemble de bonnes pratiques.\nLe Kanban, comme le « Scrum », est une méthodologie d’organisation du travail.\nApparus au Japon au milieu du siècle dernier Lean et Kanban, sont des concepts qui ont été adaptés récemment aux systèmes de l\u0026rsquo;information. Ils ont inspiré les managers « agiles » qui voulaient rendre leur processus plus fluide [3].\nDifférencier l’Agile et le Lean n’est pas toujours chose aisée, car ces philosophies partagent des valeurs communes. Mais ce sont bien deux mouvements différents [4].\n« Lean », le précepte Être « Lean », c\u0026rsquo;est prendre du recul par rapport à la gestion actuelle de son projet. C’est se placer en tant qu\u0026rsquo;observateur du procédé. Le but étant de supprimer les obstacles et améliorer le rapport qualité/prix [5].\nPour parler plus concrètement, imaginez que vous êtes gestionnaire d\u0026rsquo;une source d\u0026rsquo;eau, d\u0026rsquo;un puits par exemple. En termes Lean, l\u0026rsquo;eau est votre « valeur », c’est la richesse que vous produisez. Imaginez qu\u0026rsquo;à chaque fois que vous allez chercher de l\u0026rsquo;eau au puits, votre seau d\u0026rsquo;eau est à moitié vide. Vous avez de l\u0026rsquo;eau, certes, mais un aller-retour du seau vous coûte cher en huile de coude.\nEn tant que manager Lean, votre but serait de réussir à remonter un seau complet. Pour une même qualité d’eau, cela serait bien moins coûteux ! Et comme vous êtes aussi un brillant manager, vous vous penchez sur votre système. Une poulie, un puits et un seau qui monte et qui descend. Les obstacles qui entravent l\u0026rsquo;efficacité de votre récupération d\u0026rsquo;eau peuvent être divers et nombreux. Des branches mortes, une manivelle irrégulière\u0026hellip; Pour éliminer les problèmes les plus significatifs, vous devez d’abord les repérer pour les identifier.\nKanban, une méthode Avant d\u0026rsquo;éliminer un obstacle, il vous faut trouver une méthode pour déterminer la localisation des obstacles. C\u0026rsquo;est pour cela que le Kanban est apparu. Le Kanban est un outil pour faciliter la gestion de projet, rien de bien méchant.\nPour les agilistes de l\u0026rsquo;IT, il ressemble le plus souvent à un tableau de post\u0026rsquo;its. Romain Couturier l\u0026rsquo;a très bien présenté au LeanKanban14 [6] si vous souhaitez en découvrir d\u0026rsquo;avantage.\nLe Kanban sert à repérer les « goulots d\u0026rsquo;étranglement » de votre projet, c’est-à-dire là où il y a perte de valeur. En ce qui concerne votre puits, c’est là où se trouve cette branche qui vous accable et qui perturbe la remontée de votre seau. La méthode consiste à observer en détail le parcours de chaque unité de valeur de votre projet. Pour votre puits, une unité de valeur est un \u0026ldquo;seau d\u0026rsquo;eau\u0026rdquo;.\nLe principe est d\u0026rsquo;observer l\u0026rsquo;avancement pas à pas de vos seaux.\nEn observant ce qu\u0026rsquo;il se passe pour chaque étape, vous saurez où se trouve cette satanée branche bloquante. « Oh saperlipopette ! » vous exclamez-vous (oui, vous êtes très poli quand vous allez au puits). « A chaque montée de seau, à 8m de longueur de corde, je sens que j\u0026rsquo;ai des difficultés à remonter mon seau ! » BINGO, bien joué !\nAinsi, on se force, grâce au Kanban, à analyser l\u0026rsquo;efficacité de son processus.\nPour aller plus loin :\n_[1] Je vous conseille de lire le Manifest Agile http://fr.wikipedia.org/wiki/Manifeste_agile _\n_[2] Plus de détails sur la méthodologie Scrum https://www.scrumalliance.org/why-scrum _\n[3] \u0026ldquo;Kanban, Enclenchez le Moteur d\u0026rsquo;Amélioration continue de votre IT\u0026rdquo; de David J.Anderson\n_[4] \u0026ldquo;Lean \u0026amp; Agile, lequel choisir ?\u0026rdquo; Lucille Achard http://blog.cereza.fr/si-agilite/agilite-vs-lean-lequel-choisir-870 _\n_[5] \u0026ldquo;8 règes simples pour passer au Lean IT\u0026rdquo; par Joe McKendrick http://www.zdnet.com/article/8-simple-rules-for-achieving-lean-it/ _\n[6] Kanban pour tous - Romain Couturier http://fr.slideshare.net/calton13/kanban-pour-tous\n","date":"Oct 12, 2016","href":"https://blog.talanlabs.com/lean-kanban-un-management-innovant-pour-un-developpement-agile/","kind":"page","labs":null,"tags":["développement agile","Lean kanban","management"],"title":"Lean-Kanban : un management innovant pour un développement Agile"},{"category":null,"content":"Episode 4 - Des réunions moins nombreuses (et plus pertinentes) Ceci est le quatrième, et dernier, article d’une série consacrée à l’UX et les bonnes pratiques employées dans la conception d’applications.\nRetrouvez l’#Episode 1 – Avoir une vision globale\nRetrouvez l’#Episode 2 – Connecter les objectifs\nRetrouvez l’#Episode 3 – Faire participer tout le monde\nTL;DR (pour les lecteurs pressés)\nLes réunions ont la mauvaise réputation d\u0026rsquo;être une perte de temps, une interruption au « vrai » travail, ou l\u0026rsquo;opportunité pour quelques managers d\u0026rsquo;entendre le son de leur propre voix. Elles peuvent durer 30 minutes, 2h… Voire même toute la journée (si si!). Un cauchemar pour bien des personnes impliquées. Pourtant, bien menées, elles peuvent être une occasion précieuse pour votre équipe de collaborer et rendre l\u0026rsquo;expérience positive, avec trois conseils simples et efficaces issus des pratiques UX.\nOrdre du jour, do-goals et be-goals Avant toute chose, il est important de définir un ordre du jour de la réunion avec des objectifs clairs pour toutes les personnes conviées.\nLes objectifs se répartissent en deux catégories: les be-goals et les do-goals. Les be-goals sont des objectifs qui définissent une attente de la façon dont l\u0026rsquo;employé va agir. Les objectifs décrivant la façon dont nous voulons que l\u0026rsquo;employé soit sont dits hédoniques.\nExemples :\n Être autonome.  Arriver au travail à 8h chaque matin. Être proche des autres.    Parce que les be-goals décrivent la façon dont vous voulez que la personne se comporte, ils sont en cours et souvent très difficiles à mesurer.\nLes do-goals sont, par contraste, des objectifs pragmatiques. Ils définissent des résultats spécifiques et sont faciles à mesurer. Ces objectifs sont observables et établissent une voie claire pour la personne à suivre.\nExemples :\n Organiser une rétrospective après chaque enquête utilisateur.  Mener des tests de charge pour l\u0026rsquo;application X chaque mois. Présenter un nouveau parcours utilisateur à l\u0026rsquo;équipe pour approbation avant le 15 Août.    Les be-goals sont souvent si généraux qu\u0026rsquo;ils ne mettent pas au défi la personne de terminer quelque chose de tangible. Pour la motiver davantage, il faut donc se concentrer sur la rédaction des do-goals, qui les incitent à produire un résultat concret.\nConsidérez qui doit participer En complément de mon précédent article, voici quelques points supplémentaires à retenir vis-à-vis des participants à la réunion :\n  Incluez les personnes que vous voulez investies dans une discussion collaborative et envoyez des comptes-rendus ultérieurement à ceux souhaitant simplement connaître les décisions finales.\n  Utilisez des activités telles que celles disponibles dans Gamestorming pour impliquer et faire participer tout le monde.\n  Intégrer un ou des utilisateur(s) à la réunion\n  Ce dernier point est particulièrement critique dans le contexte UX d\u0026rsquo;un projet. Trop souvent, on ne se réfère qu\u0026rsquo;aux études analytiques (sondages, interviews, outils d\u0026rsquo;analytics) pour connaître nos utilisateurs. Bien que celles-ci soient importantes, tout comme le sont les résultats de tout test quantitatif, les meilleures entreprises sont bâties sur des relations.\nEn se focalisant sur vos utilisateurs et en apprenant à les connaître en tant qu\u0026rsquo;individus, non seulement vous leur ferez adhérer plus facilement au produit, vous pourrez également améliorer votre produit avec une source constante de détails de persona, testeurs d\u0026rsquo;ergonomie et utilisateurs à interviewer.\n« Je ne suis pas un numéro. Je suis un homme libre. »\nLa Technique Pomodoro Last but not least, une réunion, qu\u0026rsquo;elle soit dans un contexte UX ou autre, peut être rendue encore plus efficace avec la Technique Pomodoro.\nDéveloppée par Francesco Cirillo à la fin des années 1980, la Technique Pomodoro est une méthode de gestion du temps utilisant un minuteur pour décomposer le travail en intervalles, traditionnellement de 25 minutes, séparées par de courtes pauses. Ces intervalles sont nommés pomodoros, de l\u0026rsquo;italien _pomodoro _(tomate), d\u0026rsquo;après les minuteurs de cuisine en forme de tomate que Cirillo utilisait quand il était étudiant. Cette méthode est basée sur l\u0026rsquo;idée que des pauses fréquentes peuvent améliorer l\u0026rsquo;agilité mentale. Elle est notamment adoptée depuis, de même que les concepts de timeboxing et de développement itératif et incrémental, dans des contextes de pair programming.\nVoici en quoi la Technique Pomodoro peut améliorer l\u0026rsquo;efficacité d\u0026rsquo;une réunion :\n Elle permet de travailler avec le temps, non contre lui. Une bonne base de time tracking pour éviter que la réunion de 30 minutes ne se transforme… en réunion de 2h (ça s\u0026rsquo;est vu!). De par ses pauses planifiées et courtes (dans le cas de longues réunions), elle contribue à réduire le sentiment de burnout. Elle aide également à réduire les distractions, qui peuvent être multiples (appel, e-mail, réaliser soudainement que vous devez changer l\u0026rsquo;huile de votre voiture…), et les classer selon leur priorité. Parce que cette méthode permet une meilleure concentration, les participants à la réunion sont à l\u0026rsquo;écoute, réduisant ainsi, pour eux comme pour leurs managers, le nombre de réunions de cadrage futures\u0026hellip; Tout le monde est content :)  Faites donc le test : lors de votre prochaine « grosse » réunion, établissez un ordre du jour, décomposez votre temps en intervalles de 25 minutes suivies de pauses de 5 minutes (et 15 minutes de pause au bout de 5 pomodoros), notez ce que vous avez accompli au bout de chaque pomodoro et analysez les résultats obtenus à l\u0026rsquo;issu de la réunion.\nPour en savoir plus sur la technique Pomodoro : speedevelopment.com/technique-pomodoro.\n","date":"Sep 14, 2016","href":"https://blog.talanlabs.com/les-bonnes-pratiques-ux-episode-4/","kind":"page","labs":null,"tags":["be-goals","do-goals","pomodoro","réunions","ux"],"title":"Les bonnes pratiques UX – Episode 4"},{"category":null,"content":"Episode 3 – Faire participer tout le monde Ceci est le troisième article d’une série consacrée à l’UX et les bonnes pratiques employées dans la conception d’applications.\nRetrouvez l’#Episode 1 – Avoir une vision globale\nRetrouvez l’#Episode 2 – Connecter les objectifs\nTL;DR (pour les lecteurs pressés)\nLes parties prenantes (« stakeholders » en anglais, ceux qui ne travaillent pas directement sur le projet au quotidien) apportent différentes perspectives aux objectifs et besoins d\u0026rsquo;un projet. Elles peuvent être issues d\u0026rsquo;une direction métier, du marketing, de l\u0026rsquo;informatique ou de la maîtrise d\u0026rsquo;ouvrage.\nAfin de maximiser les chances de succès d\u0026rsquo;un projet, il est important d\u0026rsquo;impliquer ces personnes, ainsi que les designers et les développeurs, très tôt dans le cycle de conception de l\u0026rsquo;expérience utilisateur, leur donner aisément la parole et partager rapidement les connaissances.\nLes différents jalons du cycle de conception UX Les projets digitaux se composent, dans la phase de conception de l\u0026rsquo;expérience utilisateur, de 5 jalons en moyenne.\n  1er jalon : la réunion de kickoff.\nLe point de départ du projet, où les objectifs et fonctionnalités sont définies par les parties prenantes. Vos équipes et parties prenantes sont toutes réunies dès cette étape ? Génial ! Tout le monde est à la même page !\n  2ème jalon : le premier round de wireframes.\nUn wireframe (ou « zoning / maquette fil de fer » en français) est un schéma plus ou moins fidèle de page web ou écran d\u0026rsquo;application. Le schéma peut être réalisé sur papier ou sur un logiciel dédié et permet de définir l\u0026rsquo;architecture des pages/écrans et le parcours utilisateur défini préalablement. Si d\u0026rsquo;autres acteurs rejoignent le projet durant cette phase, cela reste très bien. Seuls quelques rapides séances de rattrapages seront de rigueur pour être à niveau.\n  3ème jalon : le test utilisateur.\nL\u0026rsquo;étape consistant à présenter et faire tester les wireframes aux utilisateurs. Cette étape est cruciale pour les parties prenantes du projet car elles peuvent voir maintenant comment les utilisateurs interagissent avec les wireframes et ajouter leur feedback pour influencer les futurs rounds.\n  4ème jalon : le deuxième round de wireframes.\nAie ! Juste au moment où tout le monde pensait être sur la même longueur d\u0026rsquo;onde, un nouvel acteur arrive sur le projet et doit être mis au courant de toutes les décisions prises, du contexte qui a influencé ces décisions et son précieux feedback peut potentiellement signifier quelques semaines de travail en plus.\n  5ème jalon : le lancement.\nOn arrête tout et on recommence ! C\u0026rsquo;est généralement à ce moment-là que la partie prenante, arrivée en toute fin de projet, s\u0026rsquo;écrie, fortement étonnée : « Mais ce n\u0026rsquo;est pas du tout ce que j\u0026rsquo;avais en tête ! » Une expérience peut-être un peu trop familière pour certains d\u0026rsquo;entre nous. ;)\nAu démarrage du projet, il est donc impératif d\u0026rsquo;avoir tous les acteurs à bord – designers, parties prenantes et développeurs – afin que les spécifications et objectifs soient compris de tous, et d\u0026rsquo;éviter le risque de recommencer le projet de zéro.\n  Entendre toutes les voix Voici un problème assez récurrent : beaucoup de réunions sont dominées par une ou deux personnes avec de fortes voix et opinions. Ces voix peuvent provenir de gens du métier, de la technique comme du design.\nIl est alors nécessaire d\u0026rsquo;utiliser des activités de brainstorming afin d\u0026rsquo;entendre toutes les voix durant les discussions ouvertes, et mettre un point d\u0026rsquo;honneur à donner la parole aux personnes plus réservées ou silencieuses pour livrer leurs pensées, sur le moment ou dans un e-mail ultérieur.\nDonnez à tous la chance de s\u0026rsquo;exprimer !\nPartagez très tôt les connaissances Les documents de spécifications de conception (ou « specs ») traditionnelles requièrent beaucoup d\u0026rsquo;effort et ne peuvent fréquemment pas garder le rythme avec les changements de conception et de fonctionnalités. Inclure les développeurs très tôt dans la phase de conception permet une compréhension partagée des enjeux et fonctionnalités du projet.\nComme avec les parties prenantes, les designers doivent travailler avec les développeurs main dans la main.\n« [Les designers] sont faits pour résoudre des problèmes, et [ils] ne résolvent pas des problèmes avec des documents de conception. [Ils] les résolvent avec un logiciel élégant, efficace et sophistiqué. »\n– Jeff Gothelf, Lean UX\nEt vous ? A partir de quelle étape tous les acteurs participent au projet ?\nRetrouvez l’# Episode 4 – \u0026ldquo;Des réunions moins nombreuses (et plus pertinentes)\u0026quot; le 14 septembre\n","date":"Aug 31, 2016","href":"https://blog.talanlabs.com/les-bonnes-pratiques-ux-episode-3-2/","kind":"page","labs":null,"tags":["gestion de projet","participation","ux"],"title":"Les bonnes pratiques UX – Episode 3"},{"category":null,"content":"Episode 2 - Connecter les objectifs Ceci est le deuxième article d\u0026rsquo;une série consacrée à l\u0026rsquo;UX et les bonnes pratiques employées dans la conception d\u0026rsquo;applications.\nRetrouvez l'** #Episode 1 - Avoir une vision globale**\nTL;DR (pour les lecteurs pressés)\nDe nombreux projets commencent sans une idée claire des objectifs finaux. L\u0026rsquo;objectif de base du projet peut être connu, mais, trop souvent, les objectifs commerciaux et les objectifs de l\u0026rsquo;utilisateur ne sont pas articulés ou partagés. En outre, les objectifs d\u0026rsquo;une entreprise ne sont pas toujours alignés avec ceux de l\u0026rsquo;utilisateur. Cela conduit à des designs qui soit frustrent les utilisateurs, soit ne répondent pas au besoin métier.\nLa solution ? Identifier tous les objectifs en amont, et résoudre les conflits dès le début. Mener des interviews auprès des clients et parties prenantes au début d\u0026rsquo;un projet pour connaître leurs attentes. Ensuite, comparer les objectifs des utilisateurs et des entreprises, pour aider à identifier les conflits et proposer des solutions adéquates.\nQuand l\u0026rsquo;expérience utilisateur rencontre les objectifs métier\nL\u0026rsquo;expérience utilisateur est-elle une fin en soi ? D\u0026rsquo;une certaine manière, la majorité des concepteurs dans le monde digital sont venus à croire que la création d\u0026rsquo;une bonne expérience utilisateur est une fin en soi. Cette vision conduit néanmoins à bien des désillusions, de même qu\u0026rsquo;elle rend un mauvais service à leurs clients.\nLa vérité est que les objectifs métier devraient prévaloir sur les besoins des utilisateurs à chaque fois. Générer un retour sur investissement est plus important pour un site Web que de garder les utilisateurs heureux. Une telle vision ne serait-elle pas un peu cynique ?\nLa dure réalité Si une organisation ne croit pas qu\u0026rsquo;elle va générer une certaine forme de retour sur investissement – financier ou autre – alors il ne devrait pas y avoir de site Web. En d\u0026rsquo;autres termes, si le site ne génère pas de bénéfices, alors le contrat n\u0026rsquo;a pas été correctement rempli.\nMalgré ce que l\u0026rsquo;on pourrait penser, notre objectif principal en tant que concepteur/UX designer est de remplir les objectifs commerciaux fixés par nos clients. Rappelez-vous que la création d\u0026rsquo;une expérience utilisateur est un moyen à cette fin. Nous ne créons pas une bonne expérience utilisateur juste pour rendre les utilisateurs heureux. Nous le faisons parce que nous voulons qu\u0026rsquo;ils restent sur le site et enclenchent certaines actions qui permettront de générer les bénéfices attendus par nos clients.\nEst-ce que, pour autant, le monde des affaires est en désaccord avec la créativité ?\nLe cerveau droit (créativité/émotion) et le cerveau gauche (logique/rationalité)\nL\u0026rsquo;expérience utilisateur est importante Non, l\u0026rsquo;expérience utilisateur n\u0026rsquo;est pas sans importance ! En fait, la création d\u0026rsquo;une excellente expérience est le principal moyen d\u0026rsquo;aider un site à remplir ses objectifs. Quand un site est bien conçu, il est facile pour les utilisateurs de compléter les « calls to action » (boutons et liens déclenchant un processus) préalablement créés.\nLes utilisateurs satisfaits fournissent également de nombreux autres avantages. Ils peuvent devenir des ambassadeurs de votre site Web. Un utilisateur heureux est beaucoup plus susceptible de recommander vos services et est plus patient quand les choses vont parfois mal. Les utilisateurs enthousiastes peuvent aussi devenir de précieux bénévoles ; ils ont d\u0026rsquo;innombrables idées sur la façon dont votre site Web et ses produits peuvent être améliorés. Bien plus précieux que n\u0026rsquo;importe quel focus group !\nLes utilisateurs heureux génèrent donc un retour sur investissement, ce qui justifie donc le temps et l\u0026rsquo;effort pour leur donner une bonne expérience.\nLorsque objectifs business et expérience utilisateur entrent en conflit En sachant cela, on peut imaginer que les objectifs métier et l\u0026rsquo;expérience utilisateur vont en fait de pair. Il y a cependant des occasions où les deux s\u0026rsquo;affrontent. Dans ces cas-là, le retour sur investissement doit absolument prévaloir sur l\u0026rsquo;expérience utilisateur.\nUn exemple : les UXer se plaignent souvent lorsque les clients leur demandent d\u0026rsquo;ajouter des champs à leurs formulaires en ligne parce qu\u0026rsquo;ils veulent recueillir certains renseignements démographiques sur leurs utilisateurs. On peut penser, à juste titre, que cela gêne les utilisateurs et endommage l\u0026rsquo;expérience utilisateur. Mais nous devons nous demander si ces champs supplémentaires dissuaderaient les utilisateurs de remplir les formulaires – comme nous le craignons – ou s\u0026rsquo;ils les irriteraient juste légèrement. Si les utilisateurs remplissent finalement le formulaire et que la société est en mesure de recueillir des informations démographiques importantes, alors la légère irritation vaut le coup.\nAvez-vous le bon équilibre ? Le danger pour la communauté du design Web est de devenir aveugle à tout autre chose que l\u0026rsquo;expérience utilisateur. Il faut pourtant passer autant de temps et d\u0026rsquo;efforts sur la compréhension et la réalisation des objectifs métier comme sur la création d\u0026rsquo;une bonne expérience utilisateur.\nJe terminerai par ceci : lors de votre dernier projet, combien de temps avez-vous passé à la création de personas, de tests d\u0026rsquo;utilisation et, plus généralement, à l\u0026rsquo;amélioration de l\u0026rsquo;expérience utilisateur ? Est-ce comparable à la quantité de temps passé à l\u0026rsquo;apprentissage des objectifs du client et la création des bons « calls to action » ?\nDemandez-vous si vous avez le bon équilibre.\nJean Hamant\nRetrouvez l’ **# Episode 3 – Impliquer tout le monde **le 31 août\n","date":"Jul 22, 2016","href":"https://blog.talanlabs.com/les-bonnes-pratiques-ux-episode-2-2/","kind":"page","labs":null,"tags":["bonnes pratiques","utilisateur","ux"],"title":"Les bonnes pratiques UX – Episode 2"},{"category":null,"content":"Après l’article de Lucille Achard publié la semaine dernière sur le thème « UX - 5 éléments pour la construire », nous entamons avec le début de l’été une série d\u0026rsquo;articles consacrés à l\u0026rsquo;UX et les bonnes pratiques employées dans la conception d\u0026rsquo;applications.\nUn feuilleton en 4 épisodes co-écrit par Lucille Achard et Jean Hamant que vous retrouverez jusqu’au 14 septembre et qui abordera plusieurs thématiques : la nécessité d’avoir une vision globale, de pouvoir connecter les objectifs, d’avoir moins de réunions mais des réunions plus pertinentes !\nEpisode 1 - Avoir une vision globale TL;DR (Trop long, pas lu !)\nLes détails d\u0026rsquo;un projet sont clairement importants. Mais se focaliser trop sur les détails peut aboutir à une expérience utilisateur trop décousue et inconsistante au fil du temps.\nAfin de garder le projet sur la bonne voie, il est nécessaire d\u0026rsquo;avoir une vision partagée de l\u0026rsquo;avenir du projet.\nL\u0026rsquo;ancêtre de l\u0026rsquo;Apple Watch\nPensez à une horloge. Son mécanisme millimétré représente les détails de votre produit alors que l\u0026rsquo;objet, dont la fonction est de mesurer le temps, incarne la vision globale. Tous ces engrenages doivent convenir entre eux afin de faire marcher le tic-tac caractéristique de l\u0026rsquo;horloge.\nL’atelier « REMEMBER THE FUTURE » Une des pratiques employées dans les innovation games popularisés en 2006 par Luke Hohmann se nomme « Remember the future ». Durant cet atelier, les clients parties prenantes du projet sont appelées à imaginer ce que l\u0026rsquo;expérience de l\u0026rsquo;utilisation du produit sera dans une date future – une fois l\u0026rsquo;application terminée et adoptée par ses utilisateurs.\n  Distribuez à chacun de vos clients quelques morceaux de papier.\n  Demandez-leur d\u0026rsquo;imaginer qu\u0026rsquo;ils sont dans le futur, à une date plus ou moins éloignée du présent et qu\u0026rsquo;ils ont presque continuellement utilisé votre produit entre aujourd\u0026rsquo;hui et cette date future – ce pourrait être une semaine, ou un mois, voire cinq ans – choisissez un laps de temps approprié pour votre produit.\n  Maintenant, demandez-leur d\u0026rsquo;aller encore plus loin – un jour après, une semaine ou un mois.\n  Enfin, demandez à votre client d\u0026rsquo;écrire, avec autant de détails que possible, exactement ce que votre produit aura fait pour les rendre heureux – ou plus riches, ou plus productifs ou plus en sécurité ou plus intelligents ; choisissez l\u0026rsquo;ensemble des adjectifs qui fonctionnent le mieux pour votre produit.\n  Il est temps de sortir la Delorean !\nNB : La formulation de la question est extrêmement importante. Vous obtiendrez des résultats différents si vous demandez « Qu\u0026rsquo;est-ce que le système doit faire ? » au lieu de « Qu\u0026rsquo;est-ce que le système a fait ». Si vous êtes sceptique, essayez !\nPourquoi cet atelier ?   Des descriptions plus fournies\nCe jeu est basé sur de nombreuses études en psychologie cognitive qui ont examiné la façon dont nous pensons à l\u0026rsquo;avenir. Lorsque nous posons la question « Qu\u0026rsquo;est-ce que notre produit devrait faire ? », on ne nous donne pas un cadre de référence pour la comparaison.\nLorsque nous posons la question « Qu\u0026rsquo;est-ce que nos produits ont fait ? », nous générons des descriptions plus longues, plus fantaisistes, riches en détails, sensibles, car il est plus facile de comprendre et de décrire un événement futur du passé qu\u0026rsquo;un événement futur possible, même si n\u0026rsquo;a pas eu lieu.\n  Une vision globale plus claire\nCette approche a d\u0026rsquo;autres avantages importants. En pensant à un événement futur comme quelque chose qui a déjà eu lieu, il est plus simple d\u0026rsquo;imaginer une suite d\u0026rsquo;événements aboutissant au résultat final – on s\u0026rsquo;est tous imaginés un jour notre speech parfait si l\u0026rsquo;on remportait un oscar !\nSi vous demandez « Qu\u0026rsquo;est-ce que notre produit devrait faire ? », vous vous demandez non seulement ce que le produit va faire, mais comment le produit pourrait éventuellement le faire. Si, au contraire, vous demandez : « Qu\u0026rsquo;est-ce que notre produit a fait ? », non seulement vous avez une idée plus concrète de ce que le produit a fait, mais vous pouvez également commencer à répondre à la question « Comment le produit l\u0026rsquo;a fait ? ».\nPenser à un futur produit comme déjà complété nous permet de prendre des décisions plus efficaces en réduisant l\u0026rsquo;ensemble des résultats possibles qui doivent être considérés avant qu\u0026rsquo;un plan approprié ne soit choisi.\nAyez ce but en tête tout en travaillant sur vos tâches quotidiennes. Documentez les buts à court terme et à long terme en amont du projet, et révisez-les chaque jour ou chaque semaine pour que votre équipe les garde en tête.\n  Je vous laisse avec ce proverbe chinois, à méditer\u0026hellip; :)\n« Un jour en vaut trois pour qui fait chaque chose en son temps. »\nLucille Achard \u0026amp; Jean Hamant\nRetrouvez l' **# Episode 2 - Connecter les objectifs **le 22 juillet\n","date":"Jul 6, 2016","href":"https://blog.talanlabs.com/les-bonnes-pratiques-ux-episode-1/","kind":"page","labs":null,"tags":["bonnes pratiques","User experience","ux"],"title":"Les bonnes pratiques UX - Episode 1"},{"category":null,"content":"Talan Labs était présent à la 5ème édition des FLUPA UX Days, le rendez-vous annuel des professionnels de l’UX, les 16 et 17 juin derniers à la Cité Internationale à Paris. Organisée par la FLUPA, France -Luxembourg User Experience Professionals Association, cette édition 2016 fut riche d’enseignements en matière d’UX : Nous vous en proposons 7.\nBonne lecture !\n1- Stratégie UX = se distinguer de ses concurrents** Comme l’a indiqué Paul BRIAN d’UX Strat, n’importe qui peut recoller des pots cassés. Mais à se concentrer sur les détails, on perd de vue le produit. Pour créer un véritable chef-d’œuvre, il faut avoir un objectif commun qui permet de rassembler les efforts de design.\n2- Adapter ses ateliers à son public = plus d’efficacité** Pour Corinne LEULIER, Directrice de l’expérience utilisateur chez Klee Group, il existe quatre types de profils dans une réunion : le professeur, le rassembleur, le meneur et l’artiste. Le but est d’utiliser les forces de chaque profil.\nPour Marie-Amélie COTILLON, Responsable du studio de création Backelite, il est nécessaire de trouver des illustrations et activités adaptés aux utilisateurs. Par exemple, quand les utilisateurs sont des militaires, l’utilisation d’une image d’une scène de bataille peut vous aider à vous rapprocher de votre public.\n3- Coopération et collaboration : deux concepts différents** Coopérer, c’est travailler l’un pour l’autre. Collaborer, c’est réfléchir ensemble.\nLa difficulté dans la collaboration, c’est l’empathie, car il est toujours difficile de se mettre à la place de l’autre. Pour Céline TONNELIER, UX Designer chez Smart Agence, il est impératif de réinjecter de l’empathie dans l’espace de travail.\n4- Evaluer son UX avec des questionnaires « tout-cuit »** Les chercheurs en cognition, dont Carine LALLEMAND de l’Université de Luxembourg et animatrice du Blog «UX Mind » , proposent des outils de travail pour améliorer l’UX !\nA signaler notamment, une boite à outil d’évaluation de l’UX avec les questionnaires AttrakDiff, les échelles d’évaluation UEQ et meCUE.\n5- L’UX évolue dans le temps** Nous avons tendance à surtout tester l’utilisation immédiate d’un produit, mais Carine LALLEMAND précise que pour un utilisateur, le souvenir reste plus important que l’impression immédiate.\n6- Accompagner l\u0026rsquo;utilisateur dans le multi-écran Le téléphone est notre meilleur ami, notre tablette est notre meilleur domestique, et le PC est notre meilleur outil de travail. D’après les recherches de Hugo LABONDE, UX Designer chez Usabilis, passer d’un support à l’autre est très courant et l’utilisateur ne se rend pas forcément compte qu’il transite d’écran en écran. Le trajet de l’utilisateur doit donc inclure des solutions de design comme « dois-je reprendre la vidéo Youtube entre le smartphone et PC ? ».\n7- Designer pour l’accessibilité : booster d’innovation** Au UCD14 (User Centred Design Conférence), le sujet avait aussi été abordé : Designer pour compenser un handicap cognitif permet de s’ouvrir à l’innovation. Andréa BOISADAN de l’Université Paris Descartes et de Tactile Studio, a confirmé cette idée en montrant que cette contrainte lui a permis de designer une application nouvelle et plus intuitive.\nEt vous, qu’avez-vous retenu du FLUPA UX Days ?\n","date":"Jun 29, 2016","href":"https://blog.talanlabs.com/les-7-lecons-du-/flupa-ux-day-2016/","kind":"page","labs":null,"tags":["FLUPA UX days","ux"],"title":"Les 7 leçons du FLUPA UX Day 2016"},{"category":null,"content":"Dans cet article, je vous propose d\u0026rsquo;étudier le concept d’eXpérience Utilisateur (UX) appliqué à l’environnement d\u0026rsquo;une maison. Pour se faire, nous commencerons par ce que l’on perçoit immédiatement en tant qu\u0026rsquo;utilisateur, avant de décomposer notre UX en plusieurs couches de design.\nJesse James Garrett, grand guru de la Conception Centrée Utilisateur (1) propose de découper l\u0026rsquo;UX en 5 éléments (2) :\n La Surface, the Surface Le Squelette, the Skeleton La Structure, the Structure Le Sujet, the Scope La Stratégie, the Strategy  Une fois cet exercice assimilé, vous ne verrez plus vos produits de la même manière !\nSur ce, enfilons notre casquette d\u0026rsquo;utilisateur.\nvoici votre casquette\nVous êtes prêt ? Parfait ! C\u0026rsquo;est parti pour notre User Story du jour.\nIl était une fois, VOUS dans un salon. Vous avez faim, et décidez d\u0026rsquo;aller vous faire un sandwich thon-concombre-mayonnaise. Pour cela, vous devez quitter la pièce et vous rendre dans la cuisine.\nVous observez les alentours, puis apercevez une porte. Elle est jolie, avec une belle poignée.\nLa Surface\nC’est ce que nous voyons, ce que nous touchons, ce qui rentre en contact avec nos sens. Dans une maison, l\u0026rsquo;élément \u0026ldquo;Surface\u0026rdquo; est composé des portes, des fenêtres, de la décoration intérieure et extérieure. Comprendre l\u0026rsquo;utilisation d\u0026rsquo;une surface dépend de son affordance. L\u0026rsquo;affordance d\u0026rsquo;une poignée de porte, par exemple, est la propriété qui fait comprendre qu\u0026rsquo;il est possible de l\u0026rsquo;actionner. L\u0026rsquo;affordance permet aussi de voir si l\u0026rsquo;on doit la pousser, la tourner, ou la tirer.\nBilan : la surface permet toutes nos interactions directes avec le produit.\nVOUS constatez que cette porte est la seule disponible dans le salon. Vous choisissez donc de vous en approcher. Heureusement, celle-ci est juste en face de vous et facile d\u0026rsquo;accès.\nLe Squelette\nLe squelette oriente et dispose les surfaces. Dans une maison, le squelette équivaut au plan. Le plan est bon s\u0026rsquo;il permet d\u0026rsquo;assurer que les surfaces sont utilisables. Par exemple, avec deux portes côte à côte donnant sur la même chambre, l\u0026rsquo;une d\u0026rsquo;elle perdra son utilité.\nBilan : Le squelette permet d\u0026rsquo;assurer une bonne utilisation des surfaces.\nVOUS allez pour ouvrir la porte. Vous espérez que celle-ci vous permettra d\u0026rsquo;accéder éventuellement à la cuisine. Vous avez faim tout de même !\nLa Structure\nLa structure donne une direction à l\u0026rsquo;utilisation du squelette et de la surface. Dans une maison, c\u0026rsquo;est l\u0026rsquo;ordre des salles qui structure l\u0026rsquo;expérience. Par exemple, si des toilettes sont positionnées à l\u0026rsquo;entrée de la maison, l\u0026rsquo;UX de \u0026ldquo;rentrer à la maison en ayant besoin de faire pipi\u0026rdquo; est optimisée.\nBilan : La structure de la maison donne l’enchaînement des squelettes qui contiennent des surfaces\nVOUS ouvrez la porte, et à votre grand bonheur, trouvez la cuisine (sauvé !). Celle-ci est grande et l\u0026rsquo;ambiance y est agréable. Vous décidez alors que vous pourrez allègrement vous préparer un sandwich bien mérité après tant d\u0026rsquo;émotions. En plus de ça, vous vous offrirez le luxe de savourer votre thon-concombre-mayonnaise assis dans la cuisine, sans avoir à retourner dans le salon.\nLe Sujet\nLe sujet donne un cadre d’utilisations auxquelles la structure doit répondre. Dans une maison, le sujet donne l\u0026rsquo;ensemble des fonctionnalités qu\u0026rsquo;elle peut accomplir. Par exemple, une maison peut offrir la possibilité de dormir, de se laver, de cuisiner (des sandwichs thon-concombre-mayonnaise par exemple), d’accueillir des invités, de loger des enfants, de jardiner, de se baigner dans une piscine (si vous avez de la chance), de se dorer la pilule sur un balcon (si celui-ci est suffisamment grand) ou d\u0026rsquo;accéder directement à la plage aux Maldives (fonctionnalité utopique bien qu’extrêmement désirable).\nBilan : Le sujet cadre le choix des structures.\nVOUS vous sentez bien dans cette maison. Vous voyez qu\u0026rsquo;elle répondait à vos besoins attendus (pouvoir préparer et manger un sandwich). De plus, elle a répondu à un besoin implicite (pouvoir manger dans un espace adapté).\nLa Stratégie\nLa stratégie permet de structurer l\u0026rsquo;offre et le sujet du produit. Dans une maison, la stratégie donne l\u0026rsquo;objectif de construction de la maison.\nPar exemple, une maison construite pour loger une famille n\u0026rsquo;aborde pas les mêmes problématiques qu\u0026rsquo;une maison qui va ouvrir un Bed \u0026amp; Breakfast.\nBilan : La stratégie donne la raison pour laquelle l\u0026rsquo;objet est construit.\nVOUS vous dites alors que, mine de rien \u0026ldquo;c\u0026rsquo;est bien fait l\u0026rsquo;architecture d\u0026rsquo;une maison ! Si seulement les produits digitaux étaient aussi simples et efficaces\u0026rdquo;.\nReprenons maintenant notre casquette de concepteur digital.\nUne casquette bleue cette fois\nTant que ces 5 éléments UX sont conçus, peu importe comment les tâches sont distribuées au sein de votre équipe de concepteurs. Certaines méthodologies, comme la Conception Centrée Utilisateur, permettent d\u0026rsquo;assurer une bonne prise en compte de ces étapes de design. Toujours selon J.J. Garrett, il est important de s\u0026rsquo;assurer que toutes ces dimensions soient en accord avec les besoins et les comportements de l\u0026rsquo;utilisateur.\nIl devient donc apparent qu\u0026rsquo;une définition purement fonctionnelle, c\u0026rsquo;est à dire se cantonnant uniquement au sujet et à la structure du produit, reste insuffisante pour le concepteur d\u0026rsquo;UX.\nMerci pour votre lecture. Vos commentaires sont les bienvenus !\n(1) UCD, User Centered Design en anglais, apparu notamment dans The Design of Everyday Things de Donald Norman (2) The Elements of User Experience (User Centered Design for the web and beyond), Jesse James Garrett\n","date":"Jun 22, 2016","href":"https://blog.talanlabs.com/5-elements-qui-fabriquent-lux/","kind":"page","labs":null,"tags":["CCU","jesse james garett","ux"],"title":"UX - 5 éléments pour la construire"},{"category":null,"content":"Sponsor Platinum de la 5ème édition de Devoxx France, Talan Labs vous propose un retour sur les chiffres qui ont marqué \u0026ldquo;la conférence des développeurs passionnés\u0026rdquo; :\nDevoxx 2016 c\u0026rsquo;est plus de 2 700 participants et 243 heures de conférences en 3 jours ! Sur son stand Talan Labs proposait 2 animations originales qui ont attirées plus d’un millier de personne :\n  1 jeu vidéo « SuperTalan » développé en interne, mélangeant des QCM et un jeu d’arcade. L’objectif ? Se qualifier à l’une des 2 finales pour tenter remporter 1 box NVidia.\n  1 machine à pinces, comme on peut en voir dans les fêtes foraines, permettant aux participants les plus « agiles» d’attraper l’un des 3 goodies TalanLabs : et oui, _chez Talan Labs les goodies se méritent ! _\n  A signaler également le succès de la conférence Tools in Action animé par Talan Labs : « Toast TK : l’autonomisation de tests facile, collaborative et Open Source »\nTalanlabs profite de ce billet pour remercier l’équipe organisatrice Devoxx 2016 pour son implication et sa réactivité ainsi que les équipes Talan Labs présentes sur l’évènement.\nRetrouvez les vidéos de l’évènement :\nVidéo Talan Labs : https://www.youtube.com/watch?v=vsObaa4Nsr0\u0026amp;feature=youtu.be\nVidéo Devoxx 2016 : https://www.youtube.com/watch?v=hYg7eMMUNtk\nNous vous quittons en images et vous donnons rendez-vous dans un **1 **an pour la 6ème édition de la conférence Devoxx !\n","date":"Apr 27, 2016","href":"https://blog.talanlabs.com/retour-sur-devoxx-2016/","kind":"page","labs":null,"tags":["Devoxx2016"],"title":"Retour sur Devoxx 2016"},{"category":null,"content":"Talan Labs, société du Groupe Talan, sera présente à Devoxx 2016, l’événement de référence des développeurs de logiciels en France, du 20 au 22 avril 2016 au Palais des Congrès de Paris.\nSponsor Platinium de la manifestation, Talan Labs présentera sur son stand (Platinium 03) l’ensemble de son offre et de ses savoir-faire : projets, conseil et expertise IT, Centres de Services et assistance technique.\nTalan Labs, animera également une conférence « Tools in Action » le mercredi 20 avril à 18h00 (Salle Neuilly 251) : « Toast TK_ _: l’automatisation de tests facile, collaborative et open source ».\nDans le cadre de cet évènement, Talan Labs organise un grand jeu concours sur twitter du 6 au 11 avril 2016 pour vous permettre de remporter une place pour participer à Devoxx 2016.\nLes règles sont simples :\n Connectez-vous sur twitter et suivez le compte @TalanLabs Retweetez le tweet du jeu concours  Le lundi 11 avril, à l’issu d’un tirage au sort, l’heureux participant sera contacté sur Twitter et se verra remettre une place pour participer à la conférence Devoxx 2016.\nN’attendez plus : A vos claviers, prêts…tweetez!\n","date":"Apr 6, 2016","href":"https://blog.talanlabs.com/jeu/","kind":"page","labs":null,"tags":["Devoxx","jeu concours"],"title":"Jeu concours Devoxx 2016"},{"category":null,"content":"Talan Labs, société du Groupe Talan, sera présente pour la première fois à Devoxx 2016 - l’événement de référence des développeurs logiciels en France, du 20 au 22 avril 2016 au Palais des Congrès de Paris.\nSponsor Platinium de la manifestation, Talan Labs animera également une conférence sur l’automatisation des tests d’acceptance.\nPour en savoir plus, téléchargez ce document.\n","date":"Apr 1, 2016","href":"https://blog.talanlabs.com/premiere-pour-talanlabs-a-devoxx-2016/","kind":"page","labs":null,"tags":["developpeur","Devoxx","logiciels"],"title":"Première pour Talan Labs à Devoxx 2016"},{"category":null,"content":"Depuis 15 ans, Devoxx (précédemment JavaPolis) est le rendez-vous des développeurs passionnés. Née en 2001 en Belgique sous l\u0026rsquo;impulsion du Belgian Java User Group (BeJUG), Devoxx a, depuis, pris ses quartiers dans plusieurs pays d\u0026rsquo;Europe (Royaume-Uni, Pologne et France). Et a même franchi la Méditerranée en 2015, à l\u0026rsquo;occasion du Devoxx Morocco en novembre.\nA l\u0026rsquo;approche de la 5è édition de Devoxx France, qui se tiendra du 20 au 23 Avril 2016 au Palais des Congrès de Paris, l\u0026rsquo;équipe de Talan Labs vous propose de revivre les meilleurs moments de l\u0026rsquo;édition 2015, au travers sa sélection des conférences les plus appréciées des collaborateurs.\n Machine learning et régulation numérique par Guillaume Laforge \u0026amp; Didier Girard Barbus et barbares par François le Droff \u0026amp; Romain PELISSE Le retour en force de GWT par Sami Jaber Quand Java prend de la vitesse, Apache Maven vous garde sur les rails par Arnaud Héritier \u0026amp; Hervé Boutemy Hashons peu mais hashons bien par Olivier Croisier \u0026amp; Olivier Bourgain Les cast codeurs: table ronde si vous ne suivez pas encore le podcast, n’hésitez pas à les suivre  Pour aller plus loin, n\u0026rsquo;hésitez pas à naviguer parmi les différentes conférences ou à vous rendre sur Parleys, qui regroupe les conférences Devoxx des années précédentes et des autres pays.\nVotre conférence favorite n\u0026rsquo;est pas dans cette (trop) courte shortlist ? N\u0026rsquo;hésitez pas à nous l\u0026rsquo;indiquer dans les commentaires !\n","date":"Mar 25, 2016","href":"https://blog.talanlabs.com/best-of-devoxx-2015/","kind":"page","labs":null,"tags":["Devoxx","Java"],"title":"Best of Devoxx 2015"},{"category":null,"content":"Simple. Collaboratif. Open Source. Trois piliers qui font aujourd\u0026rsquo;hui de Toast TK un outil d\u0026rsquo;automatisation des « tests d\u0026rsquo;acceptance » bien dans son époque, collant parfaitement au développement agile, dans une approche « Test First/Test Last » et en mode collaboratif entre les métiers et le développement.\nTests d\u0026rsquo;acceptance » : l\u0026rsquo;enjeu crucial du développement agile En cycle agile, les fonctionnalités sont développées les unes après les autres, de façon itérative. Très efficace sur le plan de l\u0026rsquo;organisation, ce mode de développement impose cependant une très grande rigueur en matière de tests : l\u0026rsquo;intégration de nouvelles fonctionnalités doit s\u0026rsquo;opérer sans régression de celles déjà validées.\nCe qui implique un certain nombre de contraintes. A commencer par le maintien de la documentation fonctionnelle à jour. Tandis que la complexité grandissante des applications oblige, bien souvent, à multiplier les outils de tests, afin d\u0026rsquo;assurer la cohérence entre l\u0026rsquo;IHM, les services et la base de données. Tout en garantissant la maintenabilité des scénarios de tests, afin qu\u0026rsquo;ils s\u0026rsquo;inscrivent dans une approche préventive, les coûts d\u0026rsquo;une démarche corrective étant toujours plus élevés.\nToast TK : un outil issu du « terrain » Ces contraintes, nombreux sont les développeurs et plus globalement les équipes projets agiles à les ressentir tous les jours. Raison pour laquelle les équipes Talan Labs ont décidé d\u0026rsquo;agir et de se créer un outil d\u0026rsquo;automatisation des « tests d\u0026rsquo;acceptance » correspondant parfaitement à leurs besoins et surtout aux réalités terrain, en mode Test First ou Test Last.\nInitié en 2012, Toast TK est un projet de R\u0026amp;D soutenu par un certain nombre de clients grands comptes de Talan Labs, parmi lesquels la SNCF, Toast TK prend la forme d\u0026rsquo;un ToolKit Open Source intégrant 3 applications distinctes : Toast Engine, le framework (parsing, exécution des test et reporting) ; Toast Studio (capture et replay d\u0026rsquo;actions utilisateurs) ; Toast WebApp (maintenance des scénarios).\nPour la conception de l\u0026rsquo;outil, priorité a été donnée, d\u0026rsquo;une part, à la simplicité : de rédaction, de maintenabilité et d’évolutivité des campagnes de tests, à l\u0026rsquo;aide par exemple d\u0026rsquo;un dictionnaire d\u0026rsquo;objets, réutilisables dans tous les scénarios. Mais également, d\u0026rsquo;autre part, à l\u0026rsquo;accessibilité de la solution aux testeurs aussi bien qu\u0026rsquo;aux équipes fonctionnelles, dans une logique d\u0026rsquo;intégration continue, avec des rapports à la fois techniques et non techniques.\nLe collaboratif, au cœur du dispositif de tests Qu\u0026rsquo;ils soient gratuits ou payants, Open Source ou propriétaires, de nombreux moteurs de tests existent sur le marché, avec des approches plus ou moins parcellaires. Toast TK ouvre une nouvelle voie en proposant un toolkit à la fois gratuit et complet, avec des fonctionnalités avancées telles que record/replay, des rapports de campagnes, et même l\u0026rsquo;intégration d\u0026rsquo;autres frameworks de tests, parmi lesquels Selenium ou encore FEST, afin de proposer un écosystème complet aux équipes de tests.\nEn complément, Toast TK a d\u0026rsquo;abord été pensé pour permettre l\u0026rsquo;échange permanent entre développeurs, testeurs et MOA, grâce notamment à la synchronisation des scénarios dès qu\u0026rsquo;un changement est opéré. Sans compter que la rédaction des scénarios en langage naturel permet aux équipes fonctionnelles d\u0026rsquo;être rapidement opérationnelles, tout en servant de documentation fonctionnelle facilement automatisable, dans une approche Test First comme Test Last.\nProposé en Open Source et entièrement gratuit, Toast TK Engine, bien que déjà fonctionnel dans cette première version disponible sur GitHub, n\u0026rsquo;en est qu\u0026rsquo;à ses balbutiements. En se confrontant au marché et à la communauté de développeurs et de passionnés, la solution continuera à évoluer et à se renforcer, en particulier quant aux fonctionnalités proposées par le Studio et la WebApp.\nPour en savoir plus : https://toast.talanlabs.com\nNB : Toast TK donnera lieu à une conférence « Tools in Action », le 20 avril, lors du Devoxx 2016\n","date":"Mar 2, 2016","href":"https://blog.talanlabs.com/toast-tk-lautomatisation-de-tests-facile-collaborative-et-gratuite/","kind":"page","labs":null,"tags":["Automatisation","développement agile","Open Source","tests d’acceptance","Toast TK"],"title":"Toast TK, des tests automatisés faciles, collaboratifs et gratuits"},{"category":null,"content":"Talan Labs est sponsor Platinium de la 5ème édition de Devoxx France, qui se déroulera du 20 au 22 avril 2016 au Palais des Congrès de Paris.\nConférence de référence des développeurs en France, Devoxx France réunit chaque année plus de 2 500 participants.\nRéservez d’ores et déjà la date et venez découvrir nos animations sur notre stand.\nA propos de Devoxx France\nDevoxx France est une conférence en 2 temps forts, sur 3 jours. La première journée (Le 20 avril), appelée \u0026ldquo;University\u0026rdquo;, permet d\u0026rsquo;assister à des présentations et de participer à des ateliers (par exemple sur Docker ou Angular 2). Le jeudi 21 et le vendredi 22 avril sont consacrés à des Keynotes, autour d\u0026rsquo;orateurs prestigieux, puis à un cycle de 8 conférences, toutes les 50 mn.\nDu Web à la Sécurité, en passant par le Cloud Computing, le comité de programme de Devoxx France sélectionne 220 propositions de conférence, sur 776 candidatures cette année.\nDevoxx France fait partie de la famille des conférences Devoxx (Belgique, Angleterre, Pologne, Maroc) qui regroupent plus de 10 000 développeurs à travers le monde.\n","date":"Feb 5, 2016","href":"https://blog.talanlabs.com/talanlabs-sponsor-platinium-devoxx-2016/","kind":"page","labs":["Lab 9"],"tags":["Devoxx"],"title":"Talan Labs sponsor Platinium de Devoxx 2016"},{"category":null,"content":"Cette année à Devoxx au rayon Big Data on pouvait assister à la présentation de Datomic, une base de données NoSQL. Voici ce qu\u0026rsquo;elle recèle.\nConçue depuis 2012 par Rich Hickey, Datomic se veut une base simple, transactionnelle, distribuée, flexible, évolutive, taillée pour les architecture cloud. Cette dernière se présente de la façon suivante :\nDerrière ce schéma un peu barbare, retenons que Datomic se base sur 3 concepts\n Peer  Correspond à une librairie installable coté serveur d\u0026rsquo;application. Il sert de passerelle avec le reste de la base : soumet les transactions et reçoit les changements du Transactor. Il fournit également la couche d\u0026rsquo;accès aux données, un système de cache et un moteur de requêtes (Datalog). Il gère toutes les communications utiles pour la connexion au Transactor et systèmes de stockage ainsi qu\u0026rsquo;avec le Datalog. Dernier point le Peer peut se lancer en mode standalone en utilisant une base montée en mémoire.\n Transactor  est unique (1 seul processus) et est obligatoire dès que le nombre de Peer est supérieur à 1 ou qu\u0026rsquo;il exploite un service de stockage. Il est chargé de recevoir et de coordonner les changements par ordre chronologique. De ce fait Datomic répond aux principes ACID des transactions. Il transmet les changements aux Peers et s\u0026rsquo;occupe de l\u0026rsquo;indexation via un processus d\u0026rsquo;arrière-plan.\n Stockage  Peut être un service interne ou externe. Pour un service externe Datomic est capable de se brancher avec DynamoDB, Riak, Cassandra, Couchbase, Infinispan ou une base SQL comme PostgreSQL.\nAprès l\u0026rsquo;architecture attardons nous sur la philosophie de Datomic. Rich Hickey est parti du constat que les bases de données relationnelles qu\u0026rsquo;on connait se repose sur un modèle sensiblement dépassé. En effet cette représentation des données a vu le jour il y a quelques dizaines d\u0026rsquo;années où les contraintes en terme de stockage étaient fortes, ce qui n\u0026rsquo;est plus le cas aujourd\u0026rsquo;hui.\nL\u0026rsquo;idée était d\u0026rsquo;abandonner la méthode de Place-oriented Programming (trouve puis écrase la valeur) historique pour mettre en place une gestion des faits. Datomic se base sur des faits (qui a dit Memento ?)\nMais qu\u0026rsquo;est-ce qu\u0026rsquo;un fait ? Un fait est :\n Immuable Atomique (concerne un seul élément) Temporel  Avec Datomic un fait se nomme un \u0026ldquo;Datom\u0026rdquo;, un Datom correspond à\n Entité Attribut Valeur Transaction (correspond à un temps base) Opération  A travers ses Datoms, Datomic est construit sur un unique schéma de table (se rapprochant des modèles schemaless): la notion de base de données est donc l\u0026rsquo;ensemble des datoms enregistrés. Dans la dernière version le nombre maximum de datoms est de 10 milliards par base.\nRevenons sur un détail : un fait est immuable. Vous vous dites comment une donnée peut être immuable, est-il impossible de changer les faits ? Dans ce cas oui puisque que les faits sont associés à une temporalité : Tim Cook est devenu le président d\u0026rsquo;Apple, ça ne veut pas dire pour autant que Steve Jobs n\u0026rsquo;a jamais été président d\u0026rsquo;Apple. Ainsi une modification est réalisée par l\u0026rsquo;ajout d\u0026rsquo;un nouveau fait (ici par exemple pour répondre à la question \u0026ldquo;qui est le président d\u0026rsquo;Apple ? \u0026ldquo;). En ce qui concerne la suppression, celle-ci se fait de manière logique par le biais du booléen Opération du modèle de données.\nNico, aime, pizza, 1001, true\nNico, aime, biere, 1001, true\nNico, aime, vodka-martini, 1001, true\nNico, aime, ginto, 1002, true\nNico, aime, biere, 1002, false\nOn peut voir l\u0026rsquo;ajout de faits dans Datomic comme la croissance d\u0026rsquo;un arbre, chaque transaction est un anneau de l\u0026rsquo;arbre. Ce qui amène à l\u0026rsquo;une des spécificités de Datomic, il est possible de revenir à l\u0026rsquo;état exact de la base de données (à une transaction donnée) : Datomic permet le voyage dans le temps.\nDatomic propose un système de covering index c\u0026rsquo;est-à-dire que le contenu des 5 colonnes est indexé. Le Transactor se charge d\u0026rsquo;actualiser les index et les données indexées sont poussées aux Peers pour exploitation dans leur cache et l’exécution des requêtes.\nQuelques spécificités supplémentaires :\n Datomic supporte Datalog comme langage de requêtage La possibilité de faire des transactions spéculatives c\u0026rsquo;est-à-dire de ne pas les envoyer au Transactor. Les modifications apportées sont visibles uniquement en local Grâce à son architecture, les lectures sont dissociées des écritures.  Datomic propose une gestion simple de données temporelles avec une forte évolutivité de capacité de lectures tout en garantissant une gestion transactionnelle des données. Il limite également toute une partie du traffic réseau avec sa gestion locale de cache et de requêtes. Ce dernier point amène aussi un inconvénient majeur puisque en raison de ses caractéristiques ACID la montée en charge en terme d\u0026rsquo;écriture risque d\u0026rsquo;être sensible et complexe comparé à d\u0026rsquo;autres solutions NoSQL.\nDatamic trouvera donc un intérêt pour toute application de gestion avec un fort besoin de rapidité de réponse. Le meilleur cas reste quand même le besoin d\u0026rsquo;historisation des données puisque l\u0026rsquo;architecture Datomic est idéalement taillée pour.\n","date":"Jun 26, 2015","href":"https://blog.talanlabs.com/datomic-base-de-donnees-noublie-rien/","kind":"page","labs":null,"tags":["Datomic"],"title":"Datomic la base de données qui n'oublie rien"},{"category":null,"content":"UX signifie eXpérience Utilisateur, et elle va au-delà du besoin utilisateur.\nImaginez que vous ayez besoin de nouvelles chaussures\u0026hellip; Vous entrez dans un magasin et voyez une centaine de modèles différents. Toutes les chaussures ont la fonctionnalité \u0026ldquo;protège la plante des pieds\u0026rdquo;. Et pourtant, vous n\u0026rsquo;allez pas sélectionner la première paire sous votre nez.\nDes chaussures Crocs sont très fonctionnelles, mais ce n\u0026rsquo;est pas le premier choix de tout le monde\nMais pourquoi ne pas se contenter d\u0026rsquo;une paire de chaussures extrêmement fonctionnelles ?\nParce que les chaussures doivent correspondre à VOTRE CONTEXTE. Votre contexte, c\u0026rsquo;est votre style de vie, votre besoin, votre budget et votre image de vous.\nCe qu\u0026rsquo;il s\u0026rsquo;est passé, c\u0026rsquo;est que vous choisissez vos chaussures selon l\u0026rsquo;eXpérience Utilisateur qu\u0026rsquo;elles vont vous procurer.\nL\u0026rsquo;UX en trois points La réussite d\u0026rsquo;un design d\u0026rsquo;eXprérience Utilisateur est mesurée par l\u0026rsquo;ingénieur en utilisabilité (Usability Engineering). Les indicateurs clefs d\u0026rsquo;une UX sont :\n fonctionnalité utilisabilité, plaisir à l\u0026rsquo;utilisation.  Prenons pour exemple un moyen de locomotion : le cheval. La fonctionnalité souhaitée est de pouvoir avancer. L\u0026rsquo;utilisation se fait en dirigeant le cheval. Le plaisir d\u0026rsquo;utilisation comprend de ne pas avoir mal aux fesses pendant le trajet. Au final, voici comment s\u0026rsquo;illustre l\u0026rsquo;UX dans l\u0026rsquo;équitation :\n le cheval répond aux besoins de fonctionnalité les rennes répondent aux besoins d\u0026rsquo;utilisabilité la selle permet un plaisir à l\u0026rsquo;utilisation  L\u0026rsquo;UX, c\u0026rsquo;est vieux comme le monde Ces dernières années, avec la venue de Google et d\u0026rsquo;Apple, il y a eu une véritable prise de conscience de l\u0026rsquo;importance de l\u0026rsquo;UX. C\u0026rsquo;est clairement un avantage compétitif que de se préoccuper du bien-être de ses utilisateurs. Et une bonne UX est le résultat d\u0026rsquo;un design réfléchi et empathique.\nUn projet n\u0026rsquo;a pas pour seul but de fournir un produit. La finalité d\u0026rsquo;un projet, c\u0026rsquo;est l\u0026rsquo;utilisation du produit.\nEn B2B nos clients veulent une eXprérience Utilisateur qui les transforme en super-compétiteurs.\nB2C nos utilisateurs veulent une eXpérience Utilisateur qui les transforme en maîtres incontestés de leur métier.\nPourquoi du UX en informatique ? Dans un prochain article, nous aborderons comment l\u0026rsquo;UX s\u0026rsquo;intègre dans les technologies de l\u0026rsquo;information\u0026hellip;. les logiciels, les sites webs et même les objets connectés.\nMerci d\u0026rsquo;avoir lu, et à bientôt !\n","date":"Jun 26, 2015","href":"https://blog.talanlabs.com/ux-quest-cest/","kind":"page","labs":null,"tags":["design","experience","utilisateur","ux"],"title":"UX - Qu'est-ce-que c'est ?"},{"category":null,"content":"Un constat On assiste ces dernières années à un changement des habitudes des utilisateurs notamment sur l\u0026rsquo;Internet mondial avec la démocratisation des offres haut débits. Dans ce contexte certaines personnes se sont rendues compte qu\u0026rsquo;un modèle relationnel des données atteignait ses limites : le NoSQL allait faire son entrée dans le monde de la représentation des données.\nMême si initialement le NoSQL est une réponse à la croissance toujours plus importante sur Internet, il trouve également sa place dans le monde de l\u0026rsquo;entreprise sur des échelles moindres. L\u0026rsquo;objet de cet article est de confronter les deux approches : le modèle relationnel versus le NoSQL.\nLes bases de données relationnelles Avantages :\n La technologie est mature (création il y a plusieurs dizaines d\u0026rsquo;années) ce qui fait qu\u0026rsquo;aujourd\u0026rsquo;hui le SQL est un langage standard et normalisé On a une garantie que les transactions sont atomiques, cohérentes, isolées et durables \u0026ndash; principe ACID (Atomic, Consistent, Independant, Durable) La possibilité de mettre en œuvre des requêtes complexes (croisement multiple des données) Du fait du nombres d\u0026rsquo;années d\u0026rsquo;existence, un large support est disponible et il existe également de fortes communautés.  Inconvénients :\n La modification du modèle établi peut être couteuse L’évolutivité des performances est privilégiée de manière verticale (augmentation des ressources du serveur) bien qu\u0026rsquo;une évolutivité horizontale soit possible, cette dernière reste plus coûteuse (environnement type cluster) Sur un très grand volume de données (centaines-milliers de Teraoctets) le modèle peut atteindre des limites en terme de performance Pour certains éditeurs, le prix de licence est élevé.  Quelques exemples de solutions :\n Oracle Database PostgreSQL Microsoft SQL Sever Oracle MySQL  Les bases de données NoSQL Avantages :\n L\u0026rsquo;évolutivité se fait de manière horizontale (pour augmenter les performances on ajoute des nouvelles machines) Les données sont distribuées sur plusieurs machines (sharding) de ce fait on évite les goulets d\u0026rsquo;étranglements lors de la récupération des données (fortes performances de lecture) La représentation des données est notable par l\u0026rsquo;absence de schéma (schemaless) La majorité des solutions est Open Source, néanmoins il existe des Support Pro pour répondre aux besoins des entreprises.  Inconvénients :\n Il n\u0026rsquo;existe pas de langage d\u0026rsquo;interrogation standardisé : chaque éditeur a mis en place le sien La mise en œuvre d\u0026rsquo;un environnement fortement transactionnel (fort besoin d\u0026rsquo;écriture) où le séquencement des écritures est primordial, reste complexe puisque l\u0026rsquo;architecture est distribuée compliquant l\u0026rsquo;atomicité et la cohérence des transactions L\u0026rsquo;écriture de requêtes complexes est difficile à mettre en œuvre L\u0026rsquo;offre NoSQL est segmentée en plusieurs familles où chacune répond à un besoin précis.  Quelques exemples de solutions (divisés en 4 familles) :\n clé-valeur : Riak, Redis Orientée colonne : Cassandra, HBase Orientée document : MongoDB, CouchDB, ElasticSearch Orientée graphe : Neo4j, HypergraphDB  Famille clé-valeur Cette catégorie de bases fonctionne comme une table associative clé/valeur ce qui en fait une base simple à mettre en place et permet un accès rapide aux informations (système de cache). Néanmoins il n\u0026rsquo;est pas possible d\u0026rsquo;indexer le contenu d\u0026rsquo;une telle base ce qui entraine un développement plus important pour garantir les performances.\nCe système est adapté dans le cadre de communication temps réel comme une messagerie en ligne.\nFamille orientée colonne La représentation se fait par colonnes et non par lignes comme pour un système classique, il s\u0026rsquo;agit peut-être du modèle le plus difficile à se représenter.\nModèle d\u0026rsquo;une base de données relationnelle\nModèle d\u0026rsquo;une base de données NoSQL famille colonne\nCette modélisation apporte une simplification d\u0026rsquo;ajout de colonne et est capable de gérer des millions de colonnes. De plus la valeur \u0026ldquo;null\u0026rdquo; a désormais un coût de stockage de zéro. Comme les données d\u0026rsquo;une même colonne sont similaires, la compression des données en est plus efficace et les recherches \u0026ldquo;verticales\u0026rdquo; plus performantes.\nUn inconvénient notable est qu\u0026rsquo;une mise à jour peut nécessiter la modification de toutes les valeurs d\u0026rsquo;une colonne pour tous les enregistrements et donc devenir très coûteux.\nCette base peut être mise en place pour gérer les commentaires d\u0026rsquo;un article de blog.\nFamille orientée document Cette famille est une extension de la famille clé/valeur en associant une clé à un document hiérarchique comme le XML, le JSON.\nIci on va pouvoir stocker toute forme de structure non plane simplement, pouvoir profiter de la puissance de l\u0026rsquo;indexation en ciblant les balises du document et ainsi bénéficier d\u0026rsquo;une interrogation simplifiée (absence de jointure).\nEn évoquant la mise en œuvre de requêtes une limite peut être atteinte dans la mise en place de requêtes analytiques complexes.\nDans le web, ce modèle se prête bien à la sauvegarde de profils utilisateur.\nFamille orientée graphe Le principe de cette famille repose sur le fait d\u0026rsquo;utiliser des objets en s\u0026rsquo;appuyant sur la théorie des graphes (représentation de nœuds et d\u0026rsquo;arcs) : chaque élément connait son(ses) voisin(s).\nUn tel principe permet de mettre facilement des algorithmes de parcours de graphe (recherche du plus court chemin, nœud(s) ayant une position centrale, \u0026hellip;) et via une indexation d\u0026rsquo;augmenter sensiblement les performances (création de listes chainées).\nCependant ce principe reste un domaine bien spécifique compliqué à utiliser concrètement et sera moins efficace dans l\u0026rsquo;exploitation des relations.\nL\u0026rsquo;exemple le plus parlant est la mise en place d\u0026rsquo;un arbre généalogique : avec ce principe il est simple de retrouver les degrés de parenté entre les personnes.\nUn petit tableau récapitulatif des différentes familles\nPour conclure Aujourd\u0026rsquo;hui le NoSQL apporte une nouvelle façon d\u0026rsquo;appréhender la modélisation des données. Le NoSQL n\u0026rsquo;est pas là pour remplacer les bases de données relationnelles, il répond à des besoins différents mais les deux approches peuvent cohabiter. Le choix de l\u0026rsquo;une ou de l\u0026rsquo;autre sera donc fortement dépendante du contexte et du besoin.\n","date":"Jun 23, 2015","href":"https://blog.talanlabs.com/etude-comparative-bdd-relationnelle-versus-nosql/","kind":"page","labs":["Lab 9"],"tags":["NoSQL"],"title":"Etude comparative bdd relationnelle versus NoSQL"},{"category":null,"content":"Qui n\u0026rsquo;a jamais pesté contre le terminal windows (cmd), son copier/coller non ergonomique, le noir et blanc ou encore l\u0026rsquo;agrandissement limité ? Pour corriger tout ça voici un petit programme fort sympathique : Cmder\nEn plus celui-ci a la bonne idée d\u0026rsquo;intégrer msygit : à vous les commandes linux.\nInstallation  Le télécharger sur le site officiel : http://gooseberrycreative.com/cmder/  Décompresser l\u0026rsquo;archive dans un répertoire Exécuter Cmder.exe    Si vous avez un message d\u0026rsquo;erreur sur le fait qu\u0026rsquo;il manque une dll\nIl faut installer ces packages :\nPour Windows 64-bit\n http://download.microsoft.com/download/0/4/1/041224F6-A7DC-486B-BD66-BCAAF74B6919/vc_redist.x64.exe\n Pour Windows 32-bit\n http://download.microsoft.com/download/0/4/1/041224F6-A7DC-486B-BD66-BCAAF74B6919/vc_redist.x86.exe\n Intégration de Cmder au clic droit Pour l\u0026rsquo;intégrer dans explorateur de fichiers (clique droit sur un dossier et ouvrir une ligne de commande), il faut exécuter la commande suivante sous une console avec les droits Administrateur.\ncmder.exe /REGISTER ALL \n","date":"Jun 10, 2015","href":"https://blog.talanlabs.com/outils-developpeur-cmder/","kind":"page","labs":["Lab 9"],"tags":["cmder","windows"],"title":"Outils du développeur - Cmder"},{"category":null,"content":"L\u0026rsquo;intégrateur est une application qui se place au niveau du back-office d\u0026rsquo;une solution SI. C\u0026rsquo;est le composant qui permet d\u0026rsquo;échanger avec les autres SI (partenaires, historiques, \u0026hellip;) via l\u0026rsquo;intermédiaire de flux que ce soit sous forme JMS (MQSeries), mails, file system, SMS \u0026hellip; Ces flux peuvent être reçus ou envoyés et une supervision adéquate est de rigueur.\nPréambule\n Contraintes Cahier des charges Mise en place Ecrans Statistiques Réception Emission Observations Exploitation Cahier de tests Import de cas de tests Sauvegarde du résultat Cas de tests : groupes Variables Limites techniques  Préambule Dans un tel SI, l\u0026rsquo;intégration de ces flux est soumise à des contraintes de performance directement liées à la forte volumétrie et la taille des échanges et doit se faire en asynchrone afin de tirer meilleure partie des ressources matérielles. Cette contrainte sur la volumétrie est renforcée par le type d\u0026rsquo;activité de l\u0026rsquo;entreprise puisqu\u0026rsquo;il est fort probable que la réception et l\u0026rsquo;émission de ces flux se fasse sur un créneau horaire assez spécifique.\n On pourrait concevoir que l\u0026rsquo;essentialité des flux serait reçu en semaine. La répartition sur une journée est elle-même hétérogène et se calque sur les horaires d\u0026rsquo;ouverture (8h-20h par exemple)\n Alors que le client est connecté au serveur d\u0026rsquo;application, l\u0026rsquo;intégrateur fonctionne en mode stand-alone et peut avoir plusieurs instances lancées simultanément (à condition de bien compartimenter les traitements).\n Pour compartimenter, on pourrait par exemple mettre sur une instance tous les traitements pour l\u0026rsquo;envoi de flux, dont les batch d\u0026rsquo;envoi de mail et sur une autre instance ceux de l\u0026rsquo;intégration et donc le recyclage. On pourrait également traiter certains types de flux sur une instance et le reste sur une autre. L\u0026rsquo;intégrateur développé par Synaptix permet de faire dialoguer les différentes instances entre elles.\n Contraintes Contraintes techniques Les contraintes techniques sont principalement dues à la volumétrie des échanges mais également à des contraintes d\u0026rsquo;unicité qui peuvent intervenir.\nDeux traitements en parallèle peuvent ainsi se bloquer mutuellement s\u0026rsquo;ils modifient les mêmes objets dans un ordre différent, travailler sur des versions obsolètes des objets, créer en doublon des objets censés être uniques \u0026hellip;\nEtant donné la volumétrie, il faut pouvoir paramétrer le nombre de traitements en parallèle suivant le type de flux et ce afin de protéger l\u0026rsquo;intégrateur contre toute surcharge. Il ne s\u0026rsquo;agirait pas de noyer l\u0026rsquo;application avec plusieurs milliers de traitements en parallèle. Pouvoir intervenir sur ces paramètres à chaud, c\u0026rsquo;est-à-dire même une fois l\u0026rsquo;application lancée, est un avantage non négligeable.\nContraintes fonctionnelles Avec toute solution d\u0026rsquo;intégration de flux se pose le problème du recyclage notamment dans le cas où ces flux sont hiérarchisés et dépendent d\u0026rsquo;un autre.\n Dans le cas quasi systématique où un premier flux crée un objet qui doit être modifié par le second, l\u0026rsquo;intégration du second ne peut se faire que si le premier a correctement été intégré. Cela vaut également dans le cas où les deux arrivent à quelques millisecondes d\u0026rsquo;écart, il est fort à parier que le premier n\u0026rsquo;aura pas fini d\u0026rsquo;être intégré étant donné le caractère asynchrone des traitements.\n De plus, le besoin de supervision apparaît rapidement afin de pouvoir contrôler les traitements et ce afin de permettre des actions utilisateur pour débloquer la situation.\n Avec des données référentielles, il arrive qu\u0026rsquo;il manque des données. Ces données doivent alors être rajoutées ou mises à jour via une action utilisateur. Une fois corrigé, l\u0026rsquo;utilisateur doit relancer les traitements bloqués.\n Parfois, l\u0026rsquo;intégration d\u0026rsquo;un flux ne doit pas se faire. C\u0026rsquo;est le cas lorsqu\u0026rsquo;une contrainte d\u0026rsquo;unicité sur un objet doit être respectée et qu\u0026rsquo;un nouveau flux demande à nouveau sa création.\n Le flux \u0026ldquo;doublon\u0026rdquo; doit être archivé ou supprimé sans qu\u0026rsquo;il ait été intégré.\n Cahier des charges Suite aux contraintes détectées, il faut proposer une solution qui permet de caractériser des types d\u0026rsquo;erreurs et de les rendre paramétrable afin de modifier le statut du flux en conséquence.\nCes différentes contraintes nous imposent de proposer un système de recyclage et de supervision, dans lequel les erreurs peuvent bénéficier d\u0026rsquo;un type de recyclage afin de savoir que faire du flux.\nLe principe proposé retenu est le suivant :\n  Chaque traitement se fait dans un thread à part\n  Ces traitements peuvent remonter des erreurs\n  Si les flux sont des XML, ils peuvent être validés en utilisant un XSD\n  Les types d\u0026rsquo;erreurs sont paramétrables et entraînent les actions suivantes : alerte, recyclage automatique (délai paramétrable), recyclage manuel, rejeté\n  Un écran permet de visualiser les messages reçus, de les recycler\n  Un écran permet de choisir un type de recyclage pour chaque type d\u0026rsquo;erreur\n  Un écran permet de visualiser des statistiques sur les flux, par jour\n  Un écran permet de visualiser des statistiques sur les erreurs, par jour\n  Mise en place d\u0026rsquo;une servlet technique permettant de paramétrer le nombre de traitements en parallèle\n  Les flux doivent être stockés\n  Il faut pouvoir intégrer des flux pour effectuer des tests ou des rattrapages\n  Mise en place L\u0026rsquo;intégrateur mis en place au sein du programme TOSCA à GEFCO sur le projet Plate-forme Service Client intègre un système complet de recyclage et de supervision des différents flux reçus. La solution retenue pour le stockage des flux a été de mettre à contribution la base de données. Cette solution n\u0026rsquo;est pas optimale mais a pour mérite de faciliter le recyclage et la supervision. Le principal problème est le stockage des flux pour lesquels une compression a été mise en place tardivement.\nLes flux ont un état qui permet d\u0026rsquo;indiquer où ils en sont dans leur vie :\n Messages en import Messages en export  A intégrer A émettre  En cours de traitement En cours de traitement  A recycler manuellement A recycler manuellement  A recycler automatiquement A recycler automatiquement  Intégré Envoyé  Rejeté Rejeté  Annulé Annulé    Deux états supplémentaires ont été rajoutés pour les flux en mode **_test_** :  Test réussi  Test échoué    Lors de l\u0026rsquo;intégration d\u0026rsquo;un flux XML pour lequel le type de flux indique qu\u0026rsquo;il faut effectuer une validation en utilisant un XSD, la première étape est de vérifier sa syntaxe. En cas d\u0026rsquo;erreur, le flux remonte une erreur technique de type XSD_ERROR mais le traitement continue car elle peut-être caractérisée en tant qu'Alerte, donc non bloquante pour l\u0026rsquo;intégration.\nEcrans Ce premier écran présente les différents flux qui n\u0026rsquo;ont pas encore été intégrés : ceux qui viennent d\u0026rsquo;être reçu et ceux qui sont en recyclage.\nUne fois intégrés, ils seront transféré dans l\u0026rsquo;écran des messages archivés, quasi identique à celui présenté.\nCet écran présente des statistiques sur le nombre de messages, ici ceux qui ont été archivés.\nCet écran présente des statistiques sur les erreurs remontées par des traitements d\u0026rsquo;intégration et le mode de recyclage associé (automatique, manuel, rejeté, alerte).\nCe dernier écran présente le système de cahier de test avec le cas d\u0026rsquo;un cahier utilisé pour les tests de non régression.\nL\u0026rsquo;édition du contenu d\u0026rsquo;un flux se fait dans une interface conviviale où il est facile de variabiliser des références et de voir la valeur actuelle.\nStatistiques Les statistiques suivantes ont été réalisées sur une semaine complète.\n Sens  Nombre de flux  Proportion  Réception 3 828 408 79%  Emission 1 016 041 21%    Réception Malheureusement sur les quatre millions de flux reçus en une semaine, ils ne sont pas répartis équitablement par jour. Nous observons que 98% d\u0026rsquo;entre eux sont reçus du lundi au vendredi. De plus, en semaine, 70% des flux sont reçus de 8h à 20h (50% de la journée).\nSur ce créneau, nous recevons donc environ 550 000 flux, soit 45 800 par heure, 763 par minute, 12 par seconde.\nRecyclage Ces flux ne s\u0026rsquo;intègrent pas tous dès le premier coup, une bonne partie tombe en recyclage. Sur un échantillon donné, voici une approximation des proportions observées\n Intégration  Nombre de flux  Proportion  Intégré du premier coup 1 413 025 67%  Avec recyclage 681 646 33%    Ces 681 646 flux ont été joués au total plus de 8 000 000 de fois. C'est le paramétrage des types d'erreurs qui va pouvoir directement influer sur le nombre de recyclages. Les recyclages étant répartis sur la semaine entière, nous pouvons en déduire que nous en traitons 47 500 par heure, soit 793 par minute, c'est à dire environ 13 par seconde. Du moins en théorie ! Dans les faits, le recyclage s'effectue par des batchs qui ont pour effet de concentrer le recyclage sur une période très réduite. Emission 99.7% des flux à émettre le sont en semaine et la plage horaire 8h à 20h représente plus de 80% des envois. Sur le million de flux à envoyer, 810 000 le seront sur les heures ouvrées, totalisant au total 162 078 par jour, soit 13 506 par heure, 225 par minute, c\u0026rsquo;est-à-dire près de 4 par seconde.\nLe recyclage n\u0026rsquo;est que peu utilisé dans les flux à l\u0026rsquo;export, il s\u0026rsquo;agirait principalement d\u0026rsquo;erreurs techniques relevées lors de la confection du flux et nous ayant contraint à ne pas pouvoir l\u0026rsquo;envoyer.\nObservations Nos calculs nous amènent à penser que nous traitons à l\u0026rsquo;import 25 flux par seconde en semaine de 8h à 20h, recyclage compris, et 4 à l\u0026rsquo;export. Le reste du temps, nous en traitons moins d\u0026rsquo;une quizaine. En réalité, il est fréquent d\u0026rsquo;observer que pendant une seconde nous traitons plus de 60 flux avec des maximum relevés allant jusqu\u0026rsquo;à 75 flux, ce qui semble être notre capacité maximale.\nExploitation Il est très intéressant de pouvoir suivre l\u0026rsquo;état de l\u0026rsquo;intégrateur du point de vue technique. A ces fins, une servlet a été mise en place et permet de consulter le nombre de traitements en cours et en attente. Le format de sortie est en JSON.\n{ \u0026#34;name\u0026#34;: \u0026#34;TRIP_FILE\u0026#34;, \u0026#34;available\u0026#34;: true, \u0026#34;busy\u0026#34;: true, \u0026#34;overloaded\u0026#34;: false, \u0026#34;meaning\u0026#34;: \u0026#34;Processing Channel 40/80, fr.gefco.tli.psc.integrator.agent.in.TripFileAgent, 24 running, 0 pending\u0026#34;, \u0026#34;nbWorking\u0026#34;: 24, \u0026#34;nbWaiting\u0026#34;: 0 }  name est le nom de la file (qui est liée à un agent) available indique si l\u0026rsquo;agent est actif busy indique si au moins un thread est lancé sur l\u0026rsquo;agent overloaded indique si la file est surchargée meaning est une description nbWorking indique le maximum de threads en parallèle autorisé nbWaiting indique le maximum de messages en attente (utile s\u0026rsquo;il y a plusieurs instances de l\u0026rsquo;intégrateur)\n A tout moment il est possible de modifier le nombre de traitements en parallèle en utilisant une servlet.\n Cette opération doit être sécurisée, c\u0026rsquo;est pourquoi cette servlet n\u0026rsquo;est accessible qu\u0026rsquo;en interne et a besoin d\u0026rsquo;un token passé en paramètre pour être prise en compte\n Cahier de tests Un système de tests a été développé et est accessible depuis un écran permettant d\u0026rsquo;ajouter, modifier, effacer et jouer des flux. Permettant de jouer des cas de tests complets, il met également en avant les tests de non régression en proposant de sauvegarder le résultat de l\u0026rsquo;intégration (l\u0026rsquo;état du message et les erreurs remontées) afin de pouvoir effectuer un delta lorsqu\u0026rsquo;il sera rejoué.\nLes cas de tests sont placés dans un cahier, créé par un utilisateur.\n Afin de proposer une référence permettant ce delta, il est nécessaire de jouer au moins une fois le flux et d\u0026rsquo;indiquer que le résultat obtenu est la référence.\n Une erreur dispose de trois propriétés\n Propriété  Exemple 1  Exemple 2  Code _LIEU_INCONNU_ _MARCHANDISE_INEXISTANTE_  Attribut _VILLE_ _NO_  Valeur _Pariss_ _123456_    Le delta effectué est fait en utilisant toutes les propriétés. Si une erreur était attendue et n'a pas été remontée ou si une erreur inattendue a été remontée, le cas de test est invalide. Import de cas de tests Il existe plusieurs façons de créer des flux, allant du simple bouton Ajouter au bouton Ajouter dans un cahier de tests depuis un flux reçu en passant par le bouton Charger les dépendances, ce dernier permettant de chercher en base tous les flux qui portent sur le même objet ou qui sont nécessaires afin de pouvoir jouer le flux sélectionné.\n Il est également possible d\u0026rsquo;exporter un ou plusieurs flux d\u0026rsquo;un cahier dans un .zip permettant de les importer dans un autre cahier, sur un autre environnement par exemple.\n Ces méthodes permettent de reproduire très rapidement un cas observé en PROD sur un autre environnement.\nLors de l\u0026rsquo;ajout d\u0026rsquo;un flux ou lors de son édition, et si le type de flux le permet, la validation du XML en utilisant un XSD est faite à la volée, ce qui minimise les risques de se tromper.\nSauvegarde du résultat Pour chaque flux, il est possible d\u0026rsquo;indiquer s\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;un test à blanc ou s\u0026rsquo;il faut sauvegarder le résultat en base. Dans le premier cas, l\u0026rsquo;intégralité des modifications effectuées en base par l\u0026rsquo;intégration du flux est rollback. Pour le second cas, il est nécessaire de bénéficier d\u0026rsquo;une autorisation et ce afin de protéger contre toute erreur de manipulation en particulier sur l\u0026rsquo;environnement de production.\nCas de tests : groupes Un cas de test complet comporte des flux hiérarchisés. Ainsi, un ensemble de flux qui doit se jouer consécutivement sera placé dans un même groupe. A la fin de l\u0026rsquo;intégration d\u0026rsquo;un flux, le système jouera le suivant du même groupe et ainsi de suite. Si l\u0026rsquo;un des flux est rejeté les suivants ne sont pas joués, le test de non régression (si référence présente) est alors invalide.\n Plusieurs groupes peuvent être joués en même temps. Des flux de deux groupes différents ne doivent pas dépendre l\u0026rsquo;un de l\u0026rsquo;autre.\n Variables Lors de l\u0026rsquo;édition du flux, il est possible de faire recours à des variables qui sont paramétrables, et ce par cahier. On variabilisera alors les identifiants, codes, \u0026hellip; qui sont utilisés par différents flux du groupe.\n\u0026lt;!-- premier flux --\u0026gt; #{tripFileNoCas1} \u0026lt;!-- second flux --\u0026gt; #{tripFileNoCas1} Lors du test, la variable sera remplacée par sa valeur dans chacun des flux.\n Afin de faciliter les tests, il est possible de variabiliser une chaîne de caractères directement dans l\u0026rsquo;interface d\u0026rsquo;édition d\u0026rsquo;un flux en sélectionnant la chaîne de caractères à remplacer puis via clic-droit, variabiliser ou alt-shift-L. Cette variabilisation se fait dans tous les flux du groupe, c\u0026rsquo;est pourquoi il faut avoir commencé par le regroupement des flux du même cas de test.\n Incrémentation Pour les tests de non régression, où les objets sont habituellement sauvegardés en base, il est indispensable de repartir sur de nouveaux jeux de données, d\u0026rsquo;où la possibilité qui est donnée à l\u0026rsquo;utilisateur d\u0026rsquo;incrémenter automatiquement une ou plusieurs variables lors du test. C\u0026rsquo;est directement dans l\u0026rsquo;édition du flux que sela s\u0026rsquo;opère, en utilisant une syntaxe qui indique à quel moment se fait l\u0026rsquo;incrémentation.\nPremière solution, on part sur un nouveau jeu de données\n\u0026lt;!-- premier flux --\u0026gt; #{++tripFileNoCas1} \u0026lt;!-- l\u0026#39;incrémentation se fait avant la lecture de la valeur --\u0026gt; \u0026lt;!-- second flux --\u0026gt; #{tripFileNoCas1} Deuxième solution, on prépare le jeu de données suivant\n\u0026lt;!-- premier flux --\u0026gt; #{tripFileNoCas1} \u0026lt;!-- second flux --\u0026gt; #{tripFileNoCas1++} \u0026lt;!-- l\u0026#39;incrémentation se fait après la lecture de la valeur --\u0026gt; Nous préférerons la première solution puisqu\u0026rsquo;elle permet, en cas d\u0026rsquo;erreur d\u0026rsquo;intégration sur l\u0026rsquo;un des flux, de pouvoir recommencer sur un nouveau jeu de données.\n N.B. Les dates sont elles-aussi incrémentables en utilisant une syntaxe un peu plus complexe. Exemple : ${++(1w1d;yyyy-MM-dd'T'HH:mm:ss'Z')variable} indiquant qu\u0026rsquo;on souhaite afficher la date sous tel format et que l\u0026rsquo;incrémentation se fait d\u0026rsquo;une semaine et un jour.\n Limites techniques A GEFCO, nous avons rencontré plusieurs limites techniques.\nSur un environnement UNIX, le nombre de traitements qui peuvent être lancés simultanément sur une machine est limité par un paramètre (cf ulimit -a, max user processes). Par défaut configurée à 1024, il ne faut pas oublier de l\u0026rsquo;augmenter lorsque l\u0026rsquo;intégrateur monte en charge.\nUne autre limite rencontrée a été au niveau de l\u0026rsquo;espace mémoire HEAP Space, actuellement fixée à 4 096Mo. Lors de tout développement, il faut penser à garder le moins longtemps possible les gros objets sinon lors d\u0026rsquo;une surcharge ou d\u0026rsquo;un retard accumulé, l\u0026rsquo;espace mémoire pourra saturer.\nUn certain nombre de traitements ont besoin d\u0026rsquo;être synchronisés. Synchroniser un traitement destiné à être asynchrone pour des raisons de performances est à proscrire, on risque de figer tout le traitement. Dans ce genre de cas, il ne faut pas hésiter à retarder un traitement en le remettant dans la file d\u0026rsquo;attente et donc en jouer un autre plutôt que le faire attendre avec un verrou et par la même occasion accumuler le retard sur les autres traitements.\nLors de la sauvegarde des objets en base, une vérification de leur version est effectuée. Si la version a été changée, une erreur est remontée et le traitement doit recommencer après un certain délai (recyclage automatique).\nAfin d\u0026rsquo;éviter les blocages en base de données, les objets ne sont sauvegardés qu\u0026rsquo;à la fin des traitements pour être le plus proche possible de leur commit en base. Attention, cette décision aura pour effet d\u0026rsquo;augmenter le nombre d\u0026rsquo;objets pour lesquels la version est obsolète (cf point précédent).\nDes inter-blocages entre la partie Java et la partie BDD sont apparus lorsqu\u0026rsquo;un verrou Java et un verrou BDD étaient sollicités en même temps. Avant de rentrer dans un bloc synchronisé, la partie Java avait mis à jour une ligne en BDD sans commiter, et attend de pouvoir dans le bloc, sauf que le verrou Java est déjà utilisé par un traitement qui a besoin de mettre à jour la même ligne. Côté bonnes pratiques, il vaut mieux éviter l\u0026rsquo;utilisation de verrous Java en simultané avec Oracle Database et laisser à Oracle la gestion des verrous.\nLe stockage des flux sur la base de données se fait dans un BLOB, difficilement optimisé pour des traitements par Oracle Database. La compression des flux a permis d’accélérer la récupération et le stockage de ces flux dans la base.\n","date":"Jun 8, 2015","href":"https://blog.talanlabs.com/gefco-integrateur-mise-place-dun-integrateur-de-flux/","kind":"page","labs":null,"tags":["Performance"],"title":"Gefco : intégrateur - mise en place d'un intégrateur de flux"},{"category":null,"content":"Introduction JavaScript fut, depuis sa création par Brendan Eich en 1995 jusqu\u0026rsquo;à l’arrivée du moteur JavaScript V8, considéré comme un langage de programmation limité au \u0026ldquo;scripting\u0026rdquo; de fonctionnalités pour navigateur. L\u0026rsquo;évolution des habitudes de consommation des technologies web et du cycle de développement d’applications en entreprise, apporte désormais de nouveaux enjeux et ouvre la voie à des perspectives en terme de solutions techniques. Nous sommes passés d’écosystèmes favorisant la mise en place de \u0026ldquo;Client lourd\u0026rdquo; au sein de réseaux d\u0026rsquo;entreprise fermés à un paysage interconnecté, décentralisé et répliqué. Dans cet article, nous introduirons comment Javascript devient une alternative crédible pour accompagner cette transformation digitale autant en B2C qu’en B2B.\nBesoin d\u0026rsquo;Agilité Historiquement, les entreprises privilégient l\u0026rsquo;utilisation de J2EE ou .Net quand il s’agit de réaliser des solutions web.\nEn effet, au niveau corporatif, la technique doit composer avec les contraintes en terme de budget, coût, délais et politique d\u0026rsquo;organisation de plateau de projet. Ces contraintes font que jusqu’à présent, les équipes de développement se reposaient principalement sur des solutions de type J2EE ou .Net afin de garantir une réponse mature aux besoins de maintenabilité, évolutivité et robustesse.\nPar ailleurs, JavaScript permet de répondre à des problématiques grandissantes telles que la « scalabilité » et l’agilité. Sa nature légère, fonctionnelle est en adéquation avec ces besoins et influence de plus en plus la prise de décision en matière d’architecture et de paradigmes de programmation. Même si Java et .Net restent au cœur de l’implémentation d’architectures applicatives, la flexibilité du JavaScript en font une alternative intéressante.\nJavascript: une alternative qui gagne en maturité Depuis la démocratisation du moteur V8 avec Node JS, JavaScript est devenu une alternative pour la réalisation de développements autant côté navigateur que serveur. Aujourd\u0026rsquo;hui, on peut réaliser des solutions web en suivant une approche monolingue et cohérente. En utilisant JavaScript et JSON comme format de donnée, on arrive à couvrir:\nla récolte et gestion d’actions utilisateurs sur le navigateur (Angular, Knockout…)\n la persistance de données (MongoDB) en utilisant un format unique (JSON, BSON) la réalisation d’une une couche applicative en JavaScript (Express, Meteor…).  Comme l’illustre le schéma ci-dessus, Les outils et Frameworks JavaScript sont aujourd’hui en plein essor et apportent des solutions sur toutes les couches nécessaires à la réalisation d’applications web d’envergure.\nFocus sur AngularJS AngularJS est un framework conçu pour le développement d\u0026rsquo;application web de type SPA. Il se démarque largement des autres outils sur le marché pour la réalisation de grands projets. Supporté et développé par Google, il englobe les caractéristiques suivantes :\n  Testabilité \u0026amp; Robustesse: Il vient accompagné de socles de validation (AngularJS scenario, Karma, Protractor) pour faciliter la réalisation de tests autant unitaires que bout en bout et ceci en temps réel. Par exemple, La modification de code javascript déclenche automatiquement une série de tests unitaires sur Karma. On obtient ainsi de manière systématique un rapport d’exécution de test nous indiquant si le code modifié a provoqué des régressions sur les règles métier.\n  Flexibilité \u0026amp; Modularité: L’injection de dépendance et l’aspect modulaire d’AngularJS permet de remplacer avec fluidité une fonction voir toute un partie du comportement de l’application. Cette modularité permet à de larges équipes de s\u0026rsquo;organiser et travailler en parallèle sur différents aspects/modules applicatifs en minimisant les interdépendances.\n  Séparation d’aspects: AngularJS apporte des concepts qui aident à structurer le code côté Javascript. Il permet de clairement faire la séparation entre différents aspects:\n Manipulation de DOM par l’utilisation de « directives » Implémentation de logique métier grâce aux « controllers » Gestion d’accès aux données par l\u0026rsquo;utilisation de « services » Manipulation de l’arborescence de données au travers du** « scope »**    Extensibilité: Ce Framework s’interface facilement avec d’autres outils du marché. Il contient nativement une implémentation légère de jQuery qui peut être remplacée suivant le besoin par l’implémentation complète.\n  En conclusion Javascript offre désormais un cadre de développement de plus en plus pertinent et attractif pour le monde de l\u0026rsquo;entreprise, notamment avec des Framework tels qu\u0026rsquo;AngularJS. Après avoir entendu parler pendant plusieurs années du J2EE et du .Net comme étant des solutions naturelles en entreprise, nous percevons dès à présent les prémices de ce que l’on appellera probablement un jour, le Js2EE (JavaScript To Entreprise Edition). Nous accompagnons nos clients dans cet esprit de transformation agile et digitale en utilisant les techniques avec pragmatisme. AngularJS a été éprouvé sur différents projets et apporte des résultats très satisfaisants d\u0026rsquo;un point de vue technique tout en répondant au contexte organisationnel et métier de nos clients. Néanmoins, il est important de rester pragmatique, que ce soit du JavaScript du Java ou du C#, le contexte d\u0026rsquo;un projet peut rendre l’un ou l’autre pertinent pour l’élaboration d\u0026rsquo;une solution technique à forte valeur ajoutée.\nL’image qui suit inspire à illustrer cette pensée\u0026hellip;\n","date":"Jun 5, 2015","href":"https://blog.talanlabs.com/javascript-service-de-lentreprise/","kind":"page","labs":null,"tags":["Agile","Angular","Javascript","Scrum"],"title":"JavaScript au service de l’entreprise"},{"category":null,"content":"Gefco a fait appel à Cereza et Talan Labs pour le développement de leur plateforme service client et la création de tableau de bord client temps réel.\nUn de leurs besoins est de donner aux utilisateurs internes et aux clients des statistiques sur les données des commandes de transports en temps réel sur différent indicateurs : Nombre de commande, nombre de colis, taux de service, délai de livraison, présence de document, lieux de départ, lieu d\u0026rsquo;arrivé, etc.\nL\u0026rsquo;outil a été découpé en deux applications distinct, une version de bureau pour les utilisateurs Gefco et l\u0026rsquo;autre une version web pour les clients. La version de bureau apporte plus de fonctionnalités que la version web.\nLes tableaux de bord sont créés depuis l\u0026rsquo;application de bureau et visualisable en temps réel.\nQuelques chiffres :\n16 000 000 commandes de transport manipulées\n 3000 clients 30 filtres possibles 25 types d\u0026rsquo;indicateurs 20 combinaisons par indicateur 13 mois d\u0026rsquo;historique 5 modes d\u0026rsquo;affichage : Tableau, Secteur, Histogramme total/vertical/horizontal 5 secondes pour le calcul d\u0026rsquo;un indicateur  Technologies :\nJava\n GWT Oracle  Présentation de l\u0026rsquo;outil en images Gestion Ecran gestion des tableaux de bord : ajouter, dupliquer, éditer et supprimer  Partage et Multilingue Editeur Création d’un tableau de bord Système multi-page, choix de la taille et de l’orientation pour l’export PDF  Bibliothèque d’indicateurs. Plus de 20 indicateurs différents. Glisser-déposé d’un indicateur  Options spécifiques à chaque type d’indicateur. 300 combinaisons de paramétrages  Différents modes de visualisation : tableau, camembert, histogramme vertical/horizontal  Mise en page, taille des indicateurs, prévisualisation  Ajout de filtres fixes (plus de 20)  Chaque filtre a son propre paramétrage Visualisation temps réel Version pour les utilisateurs internes Version web pour les clients  Filtres dynamiques : plages de date, lieux de départ/arrivée\u0026hellip; Boutons plus/moins Version pour les utilisateurs internes Version web pour les clients  Indicateur géographique zoomable : pays, département et ville  Visualisation du détail de chaque cellule Version pour les utilisateurs internes Version web pour les clients Export Export au format PDF  Export au format Excel ","date":"Jun 3, 2015","href":"https://blog.talanlabs.com/gefco-tableau-de-bord/","kind":"page","labs":["Lab 9"],"tags":["oracle"],"title":"Gefco : Tableau de bord"}]